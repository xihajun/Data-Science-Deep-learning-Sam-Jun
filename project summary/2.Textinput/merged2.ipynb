{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function data_genelization in module myfun:\n",
      "\n",
      "data_genelization(sample_size=2, loops=1000, size=26, key=3, x_as_vector=False, y_as_vector=False)\n",
      "    data_genelization(sample_size = 2,loops = 1000, size = 26, key = 3, x_as_vector = False, y_as_vector = False):\n",
      "    return x_train, label with equual size, labels with 1 dimension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data_genelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x_train, y_train, y_train_small = data_genelization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 0s 465us/step - loss: 0.6700 - acc: 0.6623\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.5888 - acc: 0.8702\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.4076 - acc: 0.9510\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.2146 - acc: 0.9615\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1666 - acc: 0.9615\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1595 - acc: 0.9615\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1579 - acc: 0.9615\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1563 - acc: 0.9615\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1548 - acc: 0.9615\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1530 - acc: 0.9615\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1512 - acc: 0.9615\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1491 - acc: 0.9615\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1468 - acc: 0.9615\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1443 - acc: 0.9615\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1415 - acc: 0.9615\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1386 - acc: 0.9615\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1355 - acc: 0.9615\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1321 - acc: 0.9615\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1284 - acc: 0.9615\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1245 - acc: 0.9616\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1205 - acc: 0.9617\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1164 - acc: 0.9618\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1123 - acc: 0.9620\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1081 - acc: 0.9624\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1041 - acc: 0.9628\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1001 - acc: 0.9637\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0962 - acc: 0.9641\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0924 - acc: 0.9648\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0888 - acc: 0.9657\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0853 - acc: 0.9669\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0817 - acc: 0.9678\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0784 - acc: 0.9688\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0750 - acc: 0.9706\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0717 - acc: 0.9721\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0686 - acc: 0.9733\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0655 - acc: 0.9757\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0626 - acc: 0.9774\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0597 - acc: 0.9786\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0569 - acc: 0.9797\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0543 - acc: 0.9806\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0518 - acc: 0.9819\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0494 - acc: 0.9831\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0471 - acc: 0.9844\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0450 - acc: 0.9857\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0429 - acc: 0.9862\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0410 - acc: 0.9868\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0391 - acc: 0.9876\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0373 - acc: 0.9883\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0357 - acc: 0.9885\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0341 - acc: 0.9893\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0326 - acc: 0.9896\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0311 - acc: 0.9903\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0297 - acc: 0.9912\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0284 - acc: 0.9916\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0271 - acc: 0.9922\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0260 - acc: 0.9927\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0248 - acc: 0.9931\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0237 - acc: 0.9934\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0227 - acc: 0.9939\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0217 - acc: 0.9943\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0208 - acc: 0.9945\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0199 - acc: 0.9949\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0191 - acc: 0.9950\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0182 - acc: 0.9956\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0175 - acc: 0.9958\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0168 - acc: 0.9960\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0161 - acc: 0.9962\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0155 - acc: 0.9963\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0148 - acc: 0.9966\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0142 - acc: 0.9970\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0137 - acc: 0.9970\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0131 - acc: 0.9972\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0126 - acc: 0.9973\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0121 - acc: 0.9976\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0116 - acc: 0.9976\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0108 - acc: 0.9978\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0104 - acc: 0.9979\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0100 - acc: 0.9982\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0096 - acc: 0.9983\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0093 - acc: 0.9984\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0089 - acc: 0.9985\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0086 - acc: 0.9985\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0083 - acc: 0.9986\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0080 - acc: 0.9987\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0077 - acc: 0.9986\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0074 - acc: 0.9988\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0072 - acc: 0.9988\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0070 - acc: 0.9989\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0067 - acc: 0.9988\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0065 - acc: 0.9990\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0062 - acc: 0.9991\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0060 - acc: 0.9991\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0058 - acc: 0.9992\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0056 - acc: 0.9992\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0054 - acc: 0.9993\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0052 - acc: 0.9992\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0049 - acc: 0.9994\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0047 - acc: 0.9994\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0046 - acc: 0.9994\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0044 - acc: 0.9995\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0043 - acc: 0.9995\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0041 - acc: 0.9995\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0037 - acc: 0.9996\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0035 - acc: 0.9997\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0032 - acc: 0.9997\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 9.7236e-04 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 9.4125e-04 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 9.1808e-04 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 8.9147e-04 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 8.6359e-04 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 8.4166e-04 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 8.1558e-04 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 7.9493e-04 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 7.8806e-04 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.5468e-04 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.3276e-04 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.1028e-04 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.8649e-04 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.6916e-04 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.5236e-04 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.3536e-04 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 6.1699e-04 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.0138e-04 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.8287e-04 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.7002e-04 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.5388e-04 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.4054e-04 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.2514e-04 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.1190e-04 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.9908e-04 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.8500e-04 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.7333e-04 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.6115e-04 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.4938e-04 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.3618e-04 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.2750e-04 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.1621e-04 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.0465e-04 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.9477e-04 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.8559e-04 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 3.7617e-04 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.6605e-04 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 3.5802e-04 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.4792e-04 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.4036e-04 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 3.3260e-04 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.2386e-04 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.1661e-04 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.0922e-04 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.0159e-04 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.9404e-04 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.8667e-04 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.8076e-04 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.7379e-04 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.6671e-04 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.6116e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2776f898>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2]])\n",
    "\n",
    "\n",
    "def predict_results_only_2(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2))\n",
    "    label_list = list(map(num2str, label1, label2))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2):   \n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        return(\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 100us/step\n",
      "\n",
      "acc: 99.62%\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, y_test_small = data_genelization()\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "prediction_list, label_list = predict_results_only_2(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN UN\n",
      "YG YG\n",
      "LW LW\n",
      "KG KG\n",
      "IC IC\n",
      "DV DV\n",
      "IM IM\n",
      "CF CF\n",
      "SV SV\n",
      "VR VR\n",
      "EK EK\n",
      "DY DY\n",
      "PJ PJ\n",
      "NM NM\n",
      "FD FD\n",
      "GR GR\n",
      "CK CK\n",
      "FG FG\n",
      "VK VX\n",
      "RF RF\n",
      "FG FG\n",
      "WM QC\n",
      "IC IC\n",
      "WR WR\n",
      "AM AM\n",
      "VP VP\n",
      "LY LY\n",
      "UB UB\n",
      "IM IM\n",
      "WG WG\n",
      "GM GM\n",
      "JI JI\n",
      "BS BS\n",
      "EI EI\n",
      "AJ AJ\n",
      "MY MY\n",
      "JW JW\n",
      "LQ LQ\n",
      "LR LR\n",
      "AB AB\n",
      "WS WS\n",
      "KR KR\n",
      "JM JM\n",
      "MV MV\n",
      "SQ XQ\n",
      "LW LW\n",
      "GL GL\n",
      "VS VS\n",
      "AF AF\n",
      "HC HC\n",
      "CC CC\n",
      "FK FK\n",
      "RW RW\n",
      "ZU ZU\n",
      "FI FI\n",
      "KL KL\n",
      "VE VE\n",
      "DO DO\n",
      "GG GG\n",
      "JV JV\n",
      "SY SY\n",
      "SP SP\n",
      "NB NB\n",
      "KX KX\n",
      "CP CP\n",
      "TN TN\n",
      "LJ LJ\n",
      "TQ TQ\n",
      "GI GI\n",
      "NT NT\n",
      "SQ SQ\n",
      "YY YY\n",
      "LG LG\n",
      "NY NY\n",
      "FC FC\n",
      "EY EY\n",
      "OT OT\n",
      "WS WS\n",
      "BG BG\n",
      "OU OU\n",
      "MZ MZ\n",
      "KF KF\n",
      "IP IP\n",
      "FB FB\n",
      "YN YN\n",
      "FA FA\n",
      "RE RE\n",
      "XA XA\n",
      "XI XI\n",
      "EI EI\n",
      "RF RF\n",
      "AT AT\n",
      "CG CG\n",
      "GD GD\n",
      "WX WX\n",
      "ZX ZX\n",
      "QE QE\n",
      "XC XC\n",
      "YN YN\n",
      "XM XM\n",
      "CF CF\n",
      "NL NL\n",
      "HS HS\n",
      "NX NX\n",
      "EO EO\n",
      "IP IP\n",
      "SP SP\n",
      "LH LH\n",
      "MM MM\n",
      "CP CP\n",
      "LX LX\n",
      "ZU ZU\n",
      "XS XS\n",
      "GV GV\n",
      "TN TN\n",
      "NM NM\n",
      "VC VC\n",
      "FD FD\n",
      "CX CX\n",
      "PY PY\n",
      "XR XR\n",
      "AS AS\n",
      "ZK ZK\n",
      "FF FF\n",
      "ZP ZP\n",
      "UF UF\n",
      "MO MO\n",
      "HM HM\n",
      "FE EZ\n",
      "ZD ZD\n",
      "OQ OQ\n",
      "IP IP\n",
      "RU RU\n",
      "JY JY\n",
      "RP RP\n",
      "RI RI\n",
      "UE UE\n",
      "UK UK\n",
      "WK WK\n",
      "JQ JQ\n",
      "BS BS\n",
      "ES ES\n",
      "QS QS\n",
      "RT RT\n",
      "EA EA\n",
      "QJ QJ\n",
      "MO MO\n",
      "UO RX\n",
      "CK CK\n",
      "VD VD\n",
      "SQ SQ\n",
      "VB VB\n",
      "OU OL\n",
      "RL RL\n",
      "DD DD\n",
      "EV EV\n",
      "BW BW\n",
      "ZW ZW\n",
      "UN UN\n",
      "HC HC\n",
      "QH QH\n",
      "AE AE\n",
      "MU MU\n",
      "VP VP\n",
      "BO BO\n",
      "RM RM\n",
      "KC KC\n",
      "WU WU\n",
      "GM GM\n",
      "UO RX\n",
      "BG BG\n",
      "DL DL\n",
      "HK HK\n",
      "WI WI\n",
      "CW CW\n",
      "XB XB\n",
      "WW WW\n",
      "FN FN\n",
      "DH DH\n",
      "WQ WQ\n",
      "LR LR\n",
      "AP AP\n",
      "PH PH\n",
      "IT IT\n",
      "MW MW\n",
      "IP IP\n",
      "CC CC\n",
      "VK VX\n",
      "QL QL\n",
      "FE FE\n",
      "TG ZC\n",
      "AF AF\n",
      "MP MP\n",
      "VJ VJ\n",
      "RO RO\n",
      "IP IP\n",
      "RZ RZ\n",
      "VJ VJ\n",
      "KU KU\n",
      "NX NX\n",
      "ZI ZI\n",
      "BU BU\n",
      "LF LF\n",
      "LJ LJ\n",
      "BQ BQ\n",
      "PQ PQ\n",
      "ID ID\n",
      "VN VN\n",
      "YH YH\n",
      "NV NV\n",
      "GQ GQ\n",
      "HL HL\n",
      "BX BX\n",
      "NR NR\n",
      "CJ CJ\n",
      "OF OF\n",
      "EM EM\n",
      "BM BM\n",
      "AH AH\n",
      "XR XR\n",
      "LI LI\n",
      "TG TG\n",
      "OY OY\n",
      "MH MH\n",
      "NV NV\n",
      "TX TX\n",
      "SM SM\n",
      "RL RL\n",
      "CA CA\n",
      "FM FM\n",
      "CZ CZ\n",
      "PV PV\n",
      "EW EW\n",
      "QG QG\n",
      "YK YK\n",
      "PQ PQ\n",
      "LH LH\n",
      "VY VY\n",
      "YY YY\n",
      "JT JT\n",
      "DD DD\n",
      "BE BE\n",
      "IT IT\n",
      "ZA ZA\n",
      "OU OU\n",
      "WO WO\n",
      "PP PP\n",
      "EK EK\n",
      "XN XN\n",
      "EO EO\n",
      "FL FL\n",
      "BY BY\n",
      "KK KK\n",
      "UJ UJ\n",
      "QK QK\n",
      "LE LE\n",
      "TB TB\n",
      "NP NP\n",
      "VU VU\n",
      "TY TY\n",
      "LG LG\n",
      "PO PO\n",
      "OW OW\n",
      "VT YT\n",
      "AT AT\n",
      "UL UL\n",
      "QV QV\n",
      "CU CU\n",
      "PX PX\n",
      "CS CS\n",
      "IL IL\n",
      "YE YE\n",
      "UK UK\n",
      "AU AU\n",
      "VG VG\n",
      "FY FY\n",
      "NI NI\n",
      "TI TI\n",
      "GC GX\n",
      "HJ HJ\n",
      "SO ZO\n",
      "SC SC\n",
      "ZI ZI\n",
      "XJ XJ\n",
      "NX NX\n",
      "UN UN\n",
      "VZ VZ\n",
      "SW SW\n",
      "ON ON\n",
      "BV BV\n",
      "XB XB\n",
      "TX TX\n",
      "FA FA\n",
      "FZ FZ\n",
      "YU YU\n",
      "YE YE\n",
      "KZ KZ\n",
      "TP TP\n",
      "OD OD\n",
      "XH XH\n",
      "CB CB\n",
      "EF EF\n",
      "PE PE\n",
      "SY SY\n",
      "JY JY\n",
      "XZ XZ\n",
      "FM FM\n",
      "QQ QQ\n",
      "XS XS\n",
      "DU DU\n",
      "KW KW\n",
      "UG UG\n",
      "SU SU\n",
      "VA VA\n",
      "HR HR\n",
      "DB DB\n",
      "OI OI\n",
      "UG UG\n",
      "RR RR\n",
      "HQ HQ\n",
      "SJ SJ\n",
      "BB BB\n",
      "TL PK\n",
      "ZE ZE\n",
      "XD XD\n",
      "IX IX\n",
      "KQ KQ\n",
      "HZ HZ\n",
      "LO LO\n",
      "GH GH\n",
      "IZ IZ\n",
      "CD CD\n",
      "EO EO\n",
      "FX FX\n",
      "AM AM\n",
      "IP IP\n",
      "UH UH\n",
      "XI XI\n",
      "WM WM\n",
      "UK UK\n",
      "XV XV\n",
      "RE RE\n",
      "YV YV\n",
      "ZR ZR\n",
      "ZS ZS\n",
      "QQ QQ\n",
      "RJ RJ\n",
      "DG DG\n",
      "ME ME\n",
      "YM YM\n",
      "HY HY\n",
      "YG YG\n",
      "KZ KZ\n",
      "YQ YQ\n",
      "JI JI\n",
      "FT FT\n",
      "IL IL\n",
      "UW UW\n",
      "OB OB\n",
      "BV BV\n",
      "ZP ZP\n",
      "SX SX\n",
      "RP RP\n",
      "CT CT\n",
      "JQ JQ\n",
      "TJ TJ\n",
      "GY GY\n",
      "MM MM\n",
      "RJ RJ\n",
      "CJ CJ\n",
      "ZB ZB\n",
      "BQ BQ\n",
      "ID ID\n",
      "KI KI\n",
      "TK TK\n",
      "AQ AQ\n",
      "KX KX\n",
      "WW WW\n",
      "RV RV\n",
      "BL BL\n",
      "NG NG\n",
      "FJ FJ\n",
      "AG AG\n",
      "PF PF\n",
      "ZV ZV\n",
      "GE GE\n",
      "TW TW\n",
      "IN IN\n",
      "RE RE\n",
      "AW AW\n",
      "IF IF\n",
      "GC GX\n",
      "NK NK\n",
      "RH RH\n",
      "SR SR\n",
      "SA SA\n",
      "UT UT\n",
      "FZ FZ\n",
      "RW RW\n",
      "MG MG\n",
      "WN WN\n",
      "SW SW\n",
      "NE NE\n",
      "NG NG\n",
      "ZT ZT\n",
      "EO EO\n",
      "VI VI\n",
      "UH UH\n",
      "PA PA\n",
      "SJ SJ\n",
      "VO VO\n",
      "ZT ZT\n",
      "TS TS\n",
      "OF OF\n",
      "PO PO\n",
      "YJ YJ\n",
      "UV UV\n",
      "MQ MQ\n",
      "IF IF\n",
      "SY SY\n",
      "EY EY\n",
      "II II\n",
      "GD GD\n",
      "LL SL\n",
      "AR AR\n",
      "TZ TZ\n",
      "VK VX\n",
      "MA MA\n",
      "PP PP\n",
      "QB QB\n",
      "XV XV\n",
      "SV SV\n",
      "GW GW\n",
      "MF MF\n",
      "JB JB\n",
      "BE BE\n",
      "HE HE\n",
      "SX SX\n",
      "HZ HZ\n",
      "YB YB\n",
      "FP FP\n",
      "QG QG\n",
      "ZA ZA\n",
      "NU NU\n",
      "WO WO\n",
      "JF JF\n",
      "TA TA\n",
      "XF XF\n",
      "UD UD\n",
      "TQ TQ\n",
      "DB DB\n",
      "CC CC\n",
      "DX DX\n",
      "DD DD\n",
      "HL HU\n",
      "LQ LQ\n",
      "HV HV\n",
      "UO UO\n",
      "RF RF\n",
      "PU PU\n",
      "ZQ ZQ\n",
      "YR YR\n",
      "ST ST\n",
      "DK DK\n",
      "LT LT\n",
      "IM IM\n",
      "GR GR\n",
      "KO KO\n",
      "PE PE\n",
      "OZ OZ\n",
      "YF YF\n",
      "OG OS\n",
      "UE UE\n",
      "ID ID\n",
      "WL WL\n",
      "YP YP\n",
      "HL HU\n",
      "DN DN\n",
      "DI DI\n",
      "JH JH\n",
      "UP UP\n",
      "RS RS\n",
      "YM YM\n",
      "SN SN\n",
      "HC DS\n",
      "AN AN\n",
      "TV TV\n",
      "NX NX\n",
      "OI OI\n",
      "OW OW\n",
      "HG HG\n",
      "VM VM\n",
      "GB GB\n",
      "GD GD\n",
      "EF EF\n",
      "NR NR\n",
      "RO RO\n",
      "LN LN\n",
      "TM TM\n",
      "KI KI\n",
      "IT IT\n",
      "QU QU\n",
      "FV FV\n",
      "PR PR\n",
      "IV IV\n",
      "JD JD\n",
      "BS BS\n",
      "DK DK\n",
      "PL PL\n",
      "FJ FJ\n",
      "BT BT\n",
      "UP UP\n",
      "TY TY\n",
      "WF WF\n",
      "OT OT\n",
      "SF SF\n",
      "UG UG\n",
      "CE CE\n",
      "PC PC\n",
      "DZ DZ\n",
      "VZ VZ\n",
      "GF GF\n",
      "HD HD\n",
      "IK IK\n",
      "CH CH\n",
      "UE CQ\n",
      "EU EU\n",
      "UF UF\n",
      "QN QN\n",
      "QX QX\n",
      "JL JL\n",
      "GG GG\n",
      "YG YG\n",
      "UA UA\n",
      "IP IP\n",
      "BW BW\n",
      "UP UP\n",
      "YA YA\n",
      "US US\n",
      "XH XH\n",
      "TR TR\n",
      "JW JW\n",
      "CK CK\n",
      "HR HR\n",
      "HL HU\n",
      "LL LL\n",
      "GT GT\n",
      "RR RR\n",
      "LJ LJ\n",
      "BB BB\n",
      "NS NS\n",
      "RZ RZ\n",
      "HF HF\n",
      "TQ TQ\n",
      "NN NN\n",
      "RW RW\n",
      "IE IE\n",
      "JX JX\n",
      "FX FX\n",
      "OG OG\n",
      "DK DK\n",
      "KD KD\n",
      "GS GS\n",
      "UX UX\n",
      "YC YC\n",
      "OQ OQ\n",
      "UQ UQ\n",
      "BM BM\n",
      "FR FR\n",
      "HB HB\n",
      "WP WP\n",
      "MB MB\n",
      "BI BI\n",
      "FT FT\n",
      "HW HW\n",
      "JB JB\n",
      "EF EF\n",
      "UQ UQ\n",
      "JC JC\n",
      "PS PS\n",
      "IX IX\n",
      "VM VM\n",
      "CB CB\n",
      "BN BN\n",
      "WU WU\n",
      "UC UC\n",
      "NT NT\n",
      "VS VS\n",
      "UX UX\n",
      "DN DN\n",
      "UK UK\n",
      "VY VY\n",
      "VN VN\n",
      "YK YK\n",
      "OZ OZ\n",
      "GO GO\n",
      "WJ WJ\n",
      "XU XU\n",
      "HZ HZ\n",
      "XF XF\n",
      "EO EO\n",
      "UV UV\n",
      "AK AK\n",
      "SV SV\n",
      "PH PH\n",
      "QM QM\n",
      "GK GK\n",
      "TJ TJ\n",
      "ND ND\n",
      "HS HS\n",
      "ZA ZA\n",
      "EK EK\n",
      "EU EU\n",
      "QT QT\n",
      "HD WD\n",
      "LH LH\n",
      "AO AO\n",
      "TT TT\n",
      "GW GW\n",
      "NJ NJ\n",
      "XU XU\n",
      "LR LR\n",
      "RV RV\n",
      "UB UB\n",
      "OP OP\n",
      "KL KL\n",
      "QE QE\n",
      "HI HI\n",
      "HN HN\n",
      "FE EZ\n",
      "HI HI\n",
      "MM MM\n",
      "UJ UJ\n",
      "PM PM\n",
      "FS FS\n",
      "JD JD\n",
      "DT DT\n",
      "TZ TZ\n",
      "ZK ZK\n",
      "CS CS\n",
      "JZ JZ\n",
      "LH LH\n",
      "CJ CJ\n",
      "WA WA\n",
      "LM LM\n",
      "NB NB\n",
      "IQ IQ\n",
      "DT DT\n",
      "HH HH\n",
      "UC UC\n",
      "LU LU\n",
      "FD FD\n",
      "NE NE\n",
      "LC LC\n",
      "LI LI\n",
      "YX YX\n",
      "EF EF\n",
      "XA XA\n",
      "QK QK\n",
      "YM YM\n",
      "TP TP\n",
      "GF GF\n",
      "NG NG\n",
      "LS LS\n",
      "VP VP\n",
      "QS QS\n",
      "HH HH\n",
      "FC FC\n",
      "BF BF\n",
      "WT WT\n",
      "DE DE\n",
      "ZW ZW\n",
      "GG GG\n",
      "JL JL\n",
      "RW RW\n",
      "KL KL\n",
      "ZW ZW\n",
      "HL HL\n",
      "CG CG\n",
      "WF WF\n",
      "EH EH\n",
      "IF IF\n",
      "TE TE\n",
      "XO XO\n",
      "SA SH\n",
      "PX PX\n",
      "NU NU\n",
      "ZB ZB\n",
      "JD JD\n",
      "LN LN\n",
      "KJ KJ\n",
      "SJ SJ\n",
      "VA VA\n",
      "HD WD\n",
      "SY SY\n",
      "MW MW\n",
      "JC JC\n",
      "FY FY\n",
      "LU LU\n",
      "VK VK\n",
      "BV BV\n",
      "ZK ZK\n",
      "IJ IJ\n",
      "PB PB\n",
      "ST ST\n",
      "UD UD\n",
      "JQ JQ\n",
      "XK XK\n",
      "XC XC\n",
      "WK WK\n",
      "FL FL\n",
      "CU CU\n",
      "XL XL\n",
      "MQ MQ\n",
      "JM JM\n",
      "FW FW\n",
      "MK MK\n",
      "NE NE\n",
      "NQ NQ\n",
      "XU XU\n",
      "CO CO\n",
      "PU PU\n",
      "QL QL\n",
      "IA IA\n",
      "YP YP\n",
      "GL GL\n",
      "DK DK\n",
      "LL SL\n",
      "TO TO\n",
      "EP EP\n",
      "FY FY\n",
      "CD CD\n",
      "HT HT\n",
      "HK HK\n",
      "BO BO\n",
      "EB EB\n",
      "JH JH\n",
      "SD SD\n",
      "HQ HQ\n",
      "TK TK\n",
      "IF IF\n",
      "YE YE\n",
      "SW SW\n",
      "RJ RJ\n",
      "MR MR\n",
      "KU KU\n",
      "KO KO\n",
      "HA HA\n",
      "SD SD\n",
      "GC GX\n",
      "PZ PZ\n",
      "BL BL\n",
      "VU VU\n",
      "DW DW\n",
      "XL XL\n",
      "XZ XZ\n",
      "JD JD\n",
      "SP SP\n",
      "GK GK\n",
      "PL PL\n",
      "FG FG\n",
      "XW XW\n",
      "LY LY\n",
      "LF LF\n",
      "GW GW\n",
      "CA CA\n",
      "UE UE\n",
      "LM LM\n",
      "LQ LQ\n",
      "US US\n",
      "DR DR\n",
      "WM QC\n",
      "UP UP\n",
      "FV FV\n",
      "MJ MJ\n",
      "VB VB\n",
      "AX AX\n",
      "YE YE\n",
      "BV BV\n",
      "SQ SQ\n",
      "LN LN\n",
      "MK MK\n",
      "IO IO\n",
      "GR GR\n",
      "LT LT\n",
      "TI TI\n",
      "EU EU\n",
      "TJ TJ\n",
      "RR RR\n",
      "VF VF\n",
      "AJ AJ\n",
      "ZB ZB\n",
      "HE HE\n",
      "LI LI\n",
      "MP MP\n",
      "BS BS\n",
      "BO BO\n",
      "EM EM\n",
      "IG IG\n",
      "HM HM\n",
      "NO NO\n",
      "TQ TQ\n",
      "GM GM\n",
      "QS QS\n",
      "VZ VZ\n",
      "HI HI\n",
      "FD FD\n",
      "UO RX\n",
      "HT HT\n",
      "AV AV\n",
      "TT TT\n",
      "VK VK\n",
      "XK XK\n",
      "TF TF\n",
      "YR YR\n",
      "SZ SZ\n",
      "RS RS\n",
      "CF CF\n",
      "QH QH\n",
      "UN UN\n",
      "EE EE\n",
      "AH AH\n",
      "WO WO\n",
      "AK AK\n",
      "VE VE\n",
      "LM LM\n",
      "GO GO\n",
      "JF JF\n",
      "YQ YQ\n",
      "RB RB\n",
      "OU OL\n",
      "LS LS\n",
      "EF EF\n",
      "GC GX\n",
      "ZZ ZZ\n",
      "VY VY\n",
      "NU NU\n",
      "UG UG\n",
      "MT MT\n",
      "AY AY\n",
      "GG GG\n",
      "IG IG\n",
      "QJ QJ\n",
      "WL WL\n",
      "WJ WJ\n",
      "NV NV\n",
      "EH EH\n",
      "XM XM\n",
      "XF XF\n",
      "RK RK\n",
      "RW RW\n",
      "RI RI\n",
      "GC GX\n",
      "BT BT\n",
      "RL RL\n",
      "TJ TJ\n",
      "RH RH\n",
      "WL WL\n",
      "QN QN\n",
      "LV LV\n",
      "QY QY\n",
      "HT HT\n",
      "CF CF\n",
      "XI XI\n",
      "JB JB\n",
      "DH DH\n",
      "QY QY\n",
      "WN WN\n",
      "QQ QQ\n",
      "ZU ZU\n",
      "ZR ZR\n",
      "VM VM\n",
      "AD AD\n",
      "TT TT\n",
      "YZ YZ\n",
      "NN NN\n",
      "VC VC\n",
      "LS LS\n",
      "VE VE\n",
      "ET ET\n",
      "IP IP\n",
      "RQ RQ\n",
      "NC NC\n",
      "XU XU\n",
      "UV UV\n",
      "ZM ZM\n",
      "RL RL\n",
      "JG JG\n",
      "SY SY\n",
      "GV GV\n",
      "TZ TZ\n",
      "JC JC\n",
      "DO DO\n",
      "FW FW\n",
      "ST ST\n",
      "UJ UJ\n",
      "SU SU\n",
      "XV XV\n",
      "AB AB\n",
      "SF SF\n",
      "GV GV\n",
      "BH BH\n",
      "QH QH\n",
      "VZ VZ\n",
      "GP GP\n",
      "IY IY\n",
      "PD PD\n",
      "TX TX\n",
      "OY OY\n",
      "OZ OZ\n",
      "WU WU\n",
      "OO OO\n",
      "YU YU\n",
      "NY NY\n",
      "BL BL\n",
      "ZW ZW\n",
      "FD FD\n",
      "YV YV\n",
      "MC MC\n",
      "OU OL\n",
      "NP NP\n",
      "BQ BQ\n",
      "AE AE\n",
      "BS BS\n",
      "LE LE\n",
      "TK TK\n",
      "AL AL\n",
      "DK DK\n",
      "SU SU\n",
      "XO XO\n",
      "MV MV\n",
      "TU TU\n",
      "DN DN\n",
      "YN YN\n",
      "WG WG\n",
      "NP NP\n",
      "FU FU\n",
      "MQ MQ\n",
      "TV TV\n",
      "FI FI\n",
      "WZ WZ\n",
      "NO NO\n",
      "RW RW\n",
      "YL YL\n",
      "OI OI\n",
      "CH CH\n",
      "LH LH\n",
      "PH PH\n",
      "NZ NZ\n",
      "FB FB\n",
      "MT MT\n",
      "VL VL\n",
      "PX PX\n",
      "NK NK\n",
      "IK IK\n",
      "UO UO\n",
      "KC KC\n",
      "GO GO\n",
      "KV KV\n",
      "BK BK\n",
      "AO AO\n",
      "WY WY\n",
      "VC VC\n",
      "HZ HZ\n",
      "QT QT\n",
      "ZP ZP\n",
      "EL EL\n",
      "HW HW\n",
      "UK UK\n",
      "LJ LJ\n",
      "VI VI\n",
      "IN IN\n",
      "XS XS\n",
      "CD CD\n",
      "FN FN\n",
      "UE UE\n",
      "MR MR\n",
      "OU OU\n",
      "JO JO\n",
      "RC RC\n",
      "OH OH\n",
      "YC YC\n",
      "LF LF\n",
      "IL IL\n",
      "RE RE\n",
      "EN EN\n",
      "ES ES\n",
      "KR KR\n",
      "PZ PZ\n",
      "OD OD\n",
      "FL FL\n",
      "XV XV\n",
      "ET ET\n",
      "QZ QZ\n",
      "GO GO\n",
      "KA KA\n",
      "BE BE\n",
      "PX PX\n",
      "OT OT\n",
      "EY EY\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prediction_list)):\n",
    "    print(\"\".join(prediction_list[i]), \"\".join(label_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " ('VK', 'VX'),\n",
       " ('WM', 'QC'),\n",
       " ('SQ', 'XQ'),\n",
       " ('FE', 'EZ'),\n",
       " ('UO', 'RX'),\n",
       " ('OU', 'OL'),\n",
       " ('TG', 'ZC'),\n",
       " ('VT', 'YT'),\n",
       " ('GC', 'GX'),\n",
       " ('SO', 'ZO'),\n",
       " ('TL', 'PK'),\n",
       " ('LL', 'SL'),\n",
       " ('HL', 'HU'),\n",
       " ('OG', 'OS'),\n",
       " ('HC', 'DS'),\n",
       " ('UE', 'CQ'),\n",
       " ('HD', 'WD'),\n",
       " ('SA', 'SH')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n",
    "output = []\n",
    "for x in diff:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an error see if the model can predict it well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [\"SA\"]\n",
    "for i in range(1000):\n",
    "    temp.append(\"SA\")\n",
    "x_test, y_test, y_test_small = data_test(temp)\n",
    "prediction_list, label_list = predict_results_only_2(model, x_test, y_test)\n",
    "diff = list(map(find_diff, prediction_list, label_list))\n",
    "output = []\n",
    "for x in diff:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 3 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As 3 letters have 17576 different combinations, we set the for loop times to 20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x_train, y_train, y_train_small = data_genelization(sample_size = 3, loops = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.2133 - acc: 0.9441\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.1243 - acc: 0.9621\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1s 28us/step - loss: 0.0866 - acc: 0.9695\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0629 - acc: 0.9783\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.0479 - acc: 0.9841\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.0368 - acc: 0.9882\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0202 - acc: 0.9941\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0140 - acc: 0.9963\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0090 - acc: 0.9979\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0055 - acc: 0.9990\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 7.2790e-04 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 4.6762e-04 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 3.0626e-04 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 2.0392e-04 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 1.3773e-04 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 9.3850e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb26751be0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, index3, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2], I2L[index3]])\n",
    "\n",
    "\n",
    "def predict_results_only_3(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    index3 = np.argmax(predictions[:, 52:-1], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "    label3 = np.argmax(y_train[:, 52:-1], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2, index3))\n",
    "    label_list = list(map(num2str, label1, label2, label3))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2):   \n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        return(\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 81us/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, y_test_small = data_genelization(sample_size = 3)\n",
    "prediction_list, label_list = predict_results_only_3(model, x_test, y_test)\n",
    "#prediction_list, label_list = predict_results_only_3(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 predictions:\n",
      "WTG WTG\n",
      "WHL WHL\n",
      "QUE QUE\n",
      "PHU PHU\n",
      "IJY IJY\n",
      "XTE XTE\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 6 predictions:\")\n",
    "for i in range(6):\n",
    "    print(\"\".join(prediction_list[i]), \"\".join(label_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some errors:\n",
      "('MQD', 'MQA')\n",
      "('KSO', 'KSA')\n",
      "('NSO', 'NSA')\n",
      "('DGC', 'DGA')\n",
      "('ZMC', 'ZMA')\n",
      "('TUO', 'TUA')\n",
      "The num of total errors: 36\n",
      "The accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n",
    "output = []\n",
    "for x in diff:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "print(\"Show some errors:\")\n",
    "output.remove(True)\n",
    "for i in range(6):\n",
    "    print(output[i])\n",
    "print(\"The num of total errors:\", len(output))\n",
    "print(\"The accuracy:\", 1-len(output)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 4 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = data_genelization(sample_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 0s 486us/step - loss: 0.6649 - acc: 0.7153\n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.5365 - acc: 0.9304\n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.2811 - acc: 0.9611\n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1722 - acc: 0.9615\n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1655 - acc: 0.9615\n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1636 - acc: 0.9615\n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1626 - acc: 0.9615\n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1612 - acc: 0.9615\n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1607 - acc: 0.9615\n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1601 - acc: 0.9615\n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1596 - acc: 0.9615\n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1590 - acc: 0.9615\n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1583 - acc: 0.9615\n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.1576 - acc: 0.9615\n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1568 - acc: 0.9615\n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1559 - acc: 0.9615\n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1549 - acc: 0.9615\n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1538 - acc: 0.9615\n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.1526 - acc: 0.9615\n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1512 - acc: 0.9615\n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1497 - acc: 0.9615\n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1481 - acc: 0.9615\n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1463 - acc: 0.9615\n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1445 - acc: 0.9615\n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1425 - acc: 0.9615\n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1405 - acc: 0.9615\n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1385 - acc: 0.9615\n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1363 - acc: 0.9615\n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1342 - acc: 0.9615\n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1320 - acc: 0.9615\n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1299 - acc: 0.9616\n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1278 - acc: 0.9616\n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1256 - acc: 0.9617\n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1235 - acc: 0.9618\n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1214 - acc: 0.9619\n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1194 - acc: 0.9621\n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1173 - acc: 0.9624\n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1153 - acc: 0.9627\n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.1133 - acc: 0.9631\n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.1113 - acc: 0.9636\n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.1094 - acc: 0.9638\n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1075 - acc: 0.9643\n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1057 - acc: 0.9646\n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.1039 - acc: 0.9650\n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1022 - acc: 0.9655\n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1005 - acc: 0.9659\n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0988 - acc: 0.9663\n",
      "Epoch 49/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0973 - acc: 0.9669\n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0958 - acc: 0.9673\n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0942 - acc: 0.9678\n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0929 - acc: 0.9682\n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0915 - acc: 0.9686\n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0901 - acc: 0.9690\n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0888 - acc: 0.9694\n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0875 - acc: 0.9697\n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0862 - acc: 0.9701\n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0850 - acc: 0.9705\n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0838 - acc: 0.9708\n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0826 - acc: 0.9711\n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0815 - acc: 0.9716\n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0804 - acc: 0.9720\n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0793 - acc: 0.9725\n",
      "Epoch 64/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0783 - acc: 0.9728\n",
      "Epoch 65/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0772 - acc: 0.9732\n",
      "Epoch 66/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0761 - acc: 0.9735\n",
      "Epoch 67/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0751 - acc: 0.9740\n",
      "Epoch 68/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0741 - acc: 0.9742\n",
      "Epoch 69/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0731 - acc: 0.9747\n",
      "Epoch 70/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0721 - acc: 0.9750\n",
      "Epoch 71/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0713 - acc: 0.9754\n",
      "Epoch 72/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0704 - acc: 0.9758\n",
      "Epoch 73/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0695 - acc: 0.9763\n",
      "Epoch 74/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0686 - acc: 0.9765\n",
      "Epoch 75/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0678 - acc: 0.9768\n",
      "Epoch 76/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0670 - acc: 0.9771\n",
      "Epoch 77/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0662 - acc: 0.9774\n",
      "Epoch 78/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0654 - acc: 0.9779\n",
      "Epoch 79/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0646 - acc: 0.9780\n",
      "Epoch 80/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0639 - acc: 0.9783\n",
      "Epoch 81/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0631 - acc: 0.9787\n",
      "Epoch 82/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0625 - acc: 0.9788\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0618 - acc: 0.9792\n",
      "Epoch 84/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0611 - acc: 0.9794\n",
      "Epoch 85/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0603 - acc: 0.9796\n",
      "Epoch 86/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0597 - acc: 0.9800\n",
      "Epoch 87/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0589 - acc: 0.9802\n",
      "Epoch 88/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0582 - acc: 0.9805\n",
      "Epoch 89/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0576 - acc: 0.9805\n",
      "Epoch 90/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0570 - acc: 0.9809\n",
      "Epoch 91/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0563 - acc: 0.9812\n",
      "Epoch 92/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0557 - acc: 0.9814\n",
      "Epoch 93/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0551 - acc: 0.9815\n",
      "Epoch 94/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0544 - acc: 0.9819\n",
      "Epoch 95/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0538 - acc: 0.9820\n",
      "Epoch 96/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0532 - acc: 0.9821\n",
      "Epoch 97/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0527 - acc: 0.9823\n",
      "Epoch 98/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0521 - acc: 0.9827\n",
      "Epoch 99/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0516 - acc: 0.9829\n",
      "Epoch 100/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0510 - acc: 0.9827\n",
      "Epoch 101/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0504 - acc: 0.9832\n",
      "Epoch 102/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0499 - acc: 0.9834\n",
      "Epoch 103/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0494 - acc: 0.9835\n",
      "Epoch 104/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0489 - acc: 0.9837\n",
      "Epoch 105/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0484 - acc: 0.9841\n",
      "Epoch 106/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0479 - acc: 0.9842\n",
      "Epoch 107/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0474 - acc: 0.9843\n",
      "Epoch 108/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0470 - acc: 0.9845\n",
      "Epoch 109/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0464 - acc: 0.9846\n",
      "Epoch 110/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0461 - acc: 0.9849\n",
      "Epoch 111/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0456 - acc: 0.9851\n",
      "Epoch 112/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0451 - acc: 0.9851\n",
      "Epoch 113/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0447 - acc: 0.9854\n",
      "Epoch 114/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0442 - acc: 0.9856\n",
      "Epoch 115/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0438 - acc: 0.9857\n",
      "Epoch 116/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0434 - acc: 0.9859\n",
      "Epoch 117/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0430 - acc: 0.9860\n",
      "Epoch 118/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0427 - acc: 0.9863\n",
      "Epoch 119/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0421 - acc: 0.9864\n",
      "Epoch 120/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0417 - acc: 0.9865\n",
      "Epoch 121/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0413 - acc: 0.9867\n",
      "Epoch 122/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0409 - acc: 0.9869\n",
      "Epoch 123/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0406 - acc: 0.9869\n",
      "Epoch 124/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0401 - acc: 0.9872\n",
      "Epoch 125/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0399 - acc: 0.9873\n",
      "Epoch 126/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0395 - acc: 0.9873\n",
      "Epoch 127/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0390 - acc: 0.9875\n",
      "Epoch 128/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0387 - acc: 0.9875\n",
      "Epoch 129/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0383 - acc: 0.9877\n",
      "Epoch 130/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0379 - acc: 0.9881\n",
      "Epoch 131/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0376 - acc: 0.9881\n",
      "Epoch 132/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0373 - acc: 0.9879\n",
      "Epoch 133/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0370 - acc: 0.9882\n",
      "Epoch 134/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0365 - acc: 0.9883\n",
      "Epoch 135/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0363 - acc: 0.9886\n",
      "Epoch 136/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0359 - acc: 0.9885\n",
      "Epoch 137/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0356 - acc: 0.9886\n",
      "Epoch 138/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0353 - acc: 0.9888\n",
      "Epoch 139/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0350 - acc: 0.9890\n",
      "Epoch 140/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0346 - acc: 0.9891\n",
      "Epoch 141/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0343 - acc: 0.9891\n",
      "Epoch 142/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0340 - acc: 0.9893\n",
      "Epoch 143/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0337 - acc: 0.9895\n",
      "Epoch 144/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0333 - acc: 0.9895\n",
      "Epoch 145/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0331 - acc: 0.9897\n",
      "Epoch 146/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0328 - acc: 0.9898\n",
      "Epoch 147/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0325 - acc: 0.9899\n",
      "Epoch 148/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0322 - acc: 0.9900\n",
      "Epoch 149/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0319 - acc: 0.9901\n",
      "Epoch 150/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0315 - acc: 0.9901\n",
      "Epoch 151/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0314 - acc: 0.9903\n",
      "Epoch 152/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0310 - acc: 0.9903\n",
      "Epoch 153/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 154/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0305 - acc: 0.9903\n",
      "Epoch 155/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0302 - acc: 0.9909\n",
      "Epoch 156/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 157/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0298 - acc: 0.9906\n",
      "Epoch 158/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0295 - acc: 0.9909\n",
      "Epoch 159/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0293 - acc: 0.9908\n",
      "Epoch 160/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0289 - acc: 0.9911\n",
      "Epoch 161/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0288 - acc: 0.9909\n",
      "Epoch 162/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0285 - acc: 0.9911\n",
      "Epoch 163/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0283 - acc: 0.9913\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0280 - acc: 0.9913\n",
      "Epoch 165/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0278 - acc: 0.9914\n",
      "Epoch 166/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0275 - acc: 0.9915\n",
      "Epoch 167/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0274 - acc: 0.9913\n",
      "Epoch 168/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0272 - acc: 0.9916\n",
      "Epoch 169/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0270 - acc: 0.9917\n",
      "Epoch 170/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0267 - acc: 0.9918\n",
      "Epoch 171/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0265 - acc: 0.9920\n",
      "Epoch 172/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0262 - acc: 0.9919\n",
      "Epoch 173/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0261 - acc: 0.9922\n",
      "Epoch 174/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 175/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0257 - acc: 0.9923\n",
      "Epoch 176/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0254 - acc: 0.9925\n",
      "Epoch 177/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0252 - acc: 0.9924\n",
      "Epoch 178/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0250 - acc: 0.9924\n",
      "Epoch 179/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0249 - acc: 0.9925\n",
      "Epoch 180/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0246 - acc: 0.9924\n",
      "Epoch 181/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0244 - acc: 0.9927\n",
      "Epoch 182/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0242 - acc: 0.9928\n",
      "Epoch 183/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0240 - acc: 0.9928\n",
      "Epoch 184/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0239 - acc: 0.9929\n",
      "Epoch 185/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0236 - acc: 0.9930\n",
      "Epoch 186/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0235 - acc: 0.9931\n",
      "Epoch 187/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0233 - acc: 0.9931\n",
      "Epoch 188/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0231 - acc: 0.9934\n",
      "Epoch 189/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0229 - acc: 0.9935\n",
      "Epoch 190/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0227 - acc: 0.9934\n",
      "Epoch 191/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0225 - acc: 0.9935\n",
      "Epoch 192/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0224 - acc: 0.9935\n",
      "Epoch 193/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0223 - acc: 0.9935\n",
      "Epoch 194/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0220 - acc: 0.9935\n",
      "Epoch 195/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0219 - acc: 0.9938\n",
      "Epoch 196/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0216 - acc: 0.9937\n",
      "Epoch 197/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0216 - acc: 0.9939\n",
      "Epoch 198/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0214 - acc: 0.9940\n",
      "Epoch 199/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0212 - acc: 0.9940\n",
      "Epoch 200/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0210 - acc: 0.9941\n",
      "Epoch 201/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0209 - acc: 0.9941\n",
      "Epoch 202/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0206 - acc: 0.9942\n",
      "Epoch 203/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0205 - acc: 0.9943\n",
      "Epoch 204/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0203 - acc: 0.9941\n",
      "Epoch 205/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0202 - acc: 0.9943\n",
      "Epoch 206/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0201 - acc: 0.9944\n",
      "Epoch 207/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0199 - acc: 0.9945\n",
      "Epoch 208/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0197 - acc: 0.9946\n",
      "Epoch 209/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0196 - acc: 0.9947\n",
      "Epoch 210/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0195 - acc: 0.9945\n",
      "Epoch 211/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0193 - acc: 0.9947\n",
      "Epoch 212/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0192 - acc: 0.9947\n",
      "Epoch 213/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0191 - acc: 0.9948\n",
      "Epoch 214/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0189 - acc: 0.9947\n",
      "Epoch 215/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0187 - acc: 0.9950\n",
      "Epoch 216/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0186 - acc: 0.9950\n",
      "Epoch 217/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0185 - acc: 0.9949\n",
      "Epoch 218/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0184 - acc: 0.9951\n",
      "Epoch 219/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0182 - acc: 0.9949\n",
      "Epoch 220/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0181 - acc: 0.9951\n",
      "Epoch 221/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0180 - acc: 0.9953\n",
      "Epoch 222/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0179 - acc: 0.9952\n",
      "Epoch 223/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0177 - acc: 0.9952\n",
      "Epoch 224/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0176 - acc: 0.9953\n",
      "Epoch 225/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0175 - acc: 0.9952\n",
      "Epoch 226/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0173 - acc: 0.9956\n",
      "Epoch 227/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0172 - acc: 0.9953\n",
      "Epoch 228/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0171 - acc: 0.9954\n",
      "Epoch 229/300\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0170 - acc: 0.9955\n",
      "Epoch 230/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0168 - acc: 0.9956\n",
      "Epoch 231/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0167 - acc: 0.9957\n",
      "Epoch 232/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0167 - acc: 0.9956\n",
      "Epoch 233/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0165 - acc: 0.9956\n",
      "Epoch 234/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0164 - acc: 0.9956\n",
      "Epoch 235/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0163 - acc: 0.9955\n",
      "Epoch 236/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0161 - acc: 0.9959\n",
      "Epoch 237/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0160 - acc: 0.9959\n",
      "Epoch 238/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0160 - acc: 0.9958\n",
      "Epoch 239/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0158 - acc: 0.9960\n",
      "Epoch 240/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0157 - acc: 0.9959\n",
      "Epoch 241/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0156 - acc: 0.9961\n",
      "Epoch 242/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0156 - acc: 0.9960\n",
      "Epoch 243/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0154 - acc: 0.9960\n",
      "Epoch 244/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0153 - acc: 0.9962\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 246/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0151 - acc: 0.9962\n",
      "Epoch 247/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0150 - acc: 0.9961\n",
      "Epoch 248/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0148 - acc: 0.9963\n",
      "Epoch 249/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0147 - acc: 0.9964\n",
      "Epoch 250/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0147 - acc: 0.9963\n",
      "Epoch 251/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0146 - acc: 0.9964\n",
      "Epoch 252/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0144 - acc: 0.9967\n",
      "Epoch 253/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0144 - acc: 0.9964\n",
      "Epoch 254/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0143 - acc: 0.9965\n",
      "Epoch 255/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0142 - acc: 0.9965\n",
      "Epoch 256/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0141 - acc: 0.9966\n",
      "Epoch 257/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0140 - acc: 0.9967\n",
      "Epoch 258/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0139 - acc: 0.9967\n",
      "Epoch 259/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0139 - acc: 0.9966\n",
      "Epoch 260/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0137 - acc: 0.9967\n",
      "Epoch 261/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0136 - acc: 0.9967\n",
      "Epoch 262/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0135 - acc: 0.9969\n",
      "Epoch 263/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0135 - acc: 0.9969\n",
      "Epoch 264/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0134 - acc: 0.9969\n",
      "Epoch 265/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0133 - acc: 0.9971\n",
      "Epoch 266/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0132 - acc: 0.9970\n",
      "Epoch 267/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0131 - acc: 0.9970\n",
      "Epoch 268/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0130 - acc: 0.9970\n",
      "Epoch 269/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0130 - acc: 0.9970\n",
      "Epoch 270/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0128 - acc: 0.9971\n",
      "Epoch 271/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0128 - acc: 0.9971\n",
      "Epoch 272/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0128 - acc: 0.9970\n",
      "Epoch 273/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0127 - acc: 0.9970\n",
      "Epoch 274/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 275/300\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 276/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0123 - acc: 0.9973\n",
      "Epoch 277/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0123 - acc: 0.9971\n",
      "Epoch 278/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0122 - acc: 0.9973\n",
      "Epoch 279/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 280/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 281/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0120 - acc: 0.9972\n",
      "Epoch 282/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0119 - acc: 0.9976\n",
      "Epoch 283/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0119 - acc: 0.9975\n",
      "Epoch 284/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0117 - acc: 0.9974\n",
      "Epoch 285/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0117 - acc: 0.9973\n",
      "Epoch 286/300\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0116 - acc: 0.9976\n",
      "Epoch 287/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0115 - acc: 0.9976\n",
      "Epoch 288/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0114 - acc: 0.9976\n",
      "Epoch 289/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0114 - acc: 0.9977\n",
      "Epoch 290/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0113 - acc: 0.9976\n",
      "Epoch 291/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 292/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0112 - acc: 0.9976\n",
      "Epoch 293/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 294/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0110 - acc: 0.9977\n",
      "Epoch 295/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0110 - acc: 0.9977\n",
      "Epoch 296/300\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0108 - acc: 0.9979\n",
      "Epoch 297/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0108 - acc: 0.9978\n",
      "Epoch 298/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0108 - acc: 0.9977\n",
      "Epoch 299/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0107 - acc: 0.9979\n",
      "Epoch 300/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0106 - acc: 0.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb276fff98>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we should not trust the accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let strat with the simpliest model, say our alphabet has there letter ABC and the \"caeser\" function is to shift one place, ie. shift ABC to BCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = data_genelization(sample_size = 2,loops = 1000, size = 3, key = 1, x_as_vector = False, y_as_vector = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=x_train.shape[1], activation='relu', name = \"input\"))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid', name = \"layer1\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1000/1000 [==============================] - 0s 404us/step - loss: 0.6844 - acc: 0.5315\n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6607 - acc: 0.6298\n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6399 - acc: 0.6413\n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6211 - acc: 0.6878\n",
      "Epoch 5/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6035 - acc: 0.7145\n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.5869 - acc: 0.7213\n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.5693 - acc: 0.7372\n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.5501 - acc: 0.7745\n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.5325 - acc: 0.7772\n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.5151 - acc: 0.7772\n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.4953 - acc: 0.7778\n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4762 - acc: 0.8033\n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.4584 - acc: 0.8290\n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4419 - acc: 0.8453\n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4261 - acc: 0.8580\n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.4107 - acc: 0.8520\n",
      "Epoch 17/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3957 - acc: 0.8827\n",
      "Epoch 18/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3807 - acc: 0.8807\n",
      "Epoch 19/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3658 - acc: 0.8938\n",
      "Epoch 20/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3510 - acc: 0.8987\n",
      "Epoch 21/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3362 - acc: 0.9167\n",
      "Epoch 22/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3215 - acc: 0.9277\n",
      "Epoch 23/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.9518\n",
      "Epoch 24/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2928 - acc: 0.9583\n",
      "Epoch 25/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2787 - acc: 0.9583\n",
      "Epoch 26/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2646 - acc: 0.9783\n",
      "Epoch 27/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2507 - acc: 0.9833\n",
      "Epoch 28/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2369 - acc: 1.0000\n",
      "Epoch 29/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2234 - acc: 1.0000\n",
      "Epoch 30/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2103 - acc: 1.0000\n",
      "Epoch 31/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1970 - acc: 1.0000\n",
      "Epoch 32/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.1841 - acc: 1.0000\n",
      "Epoch 33/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1722 - acc: 1.0000\n",
      "Epoch 34/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1612 - acc: 1.0000\n",
      "Epoch 35/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1512 - acc: 1.0000\n",
      "Epoch 36/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1421 - acc: 1.0000\n",
      "Epoch 37/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1340 - acc: 1.0000\n",
      "Epoch 38/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1267 - acc: 1.0000\n",
      "Epoch 39/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1202 - acc: 1.0000\n",
      "Epoch 40/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1144 - acc: 1.0000\n",
      "Epoch 41/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1091 - acc: 1.0000\n",
      "Epoch 42/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1043 - acc: 1.0000\n",
      "Epoch 43/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1000 - acc: 1.0000\n",
      "Epoch 44/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0959 - acc: 1.0000\n",
      "Epoch 45/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0922 - acc: 1.0000\n",
      "Epoch 46/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0888 - acc: 1.0000\n",
      "Epoch 47/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0856 - acc: 1.0000\n",
      "Epoch 48/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0827 - acc: 1.0000\n",
      "Epoch 49/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0799 - acc: 1.0000\n",
      "Epoch 50/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0773 - acc: 1.0000\n",
      "Epoch 51/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0749 - acc: 1.0000\n",
      "Epoch 52/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0726 - acc: 1.0000\n",
      "Epoch 53/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 54/150\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0684 - acc: 1.0000\n",
      "Epoch 55/150\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0665 - acc: 1.0000\n",
      "Epoch 56/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0647 - acc: 1.0000\n",
      "Epoch 57/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0629 - acc: 1.0000\n",
      "Epoch 58/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0613 - acc: 1.0000\n",
      "Epoch 59/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0597 - acc: 1.0000\n",
      "Epoch 60/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0582 - acc: 1.0000\n",
      "Epoch 61/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0568 - acc: 1.0000\n",
      "Epoch 62/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0554 - acc: 1.0000\n",
      "Epoch 63/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0541 - acc: 1.0000\n",
      "Epoch 64/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0529 - acc: 1.0000\n",
      "Epoch 65/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0517 - acc: 1.0000\n",
      "Epoch 66/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 67/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0494 - acc: 1.0000\n",
      "Epoch 68/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0483 - acc: 1.0000\n",
      "Epoch 69/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0473 - acc: 1.0000\n",
      "Epoch 70/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0463 - acc: 1.0000\n",
      "Epoch 71/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0453 - acc: 1.0000\n",
      "Epoch 72/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0444 - acc: 1.0000\n",
      "Epoch 73/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0435 - acc: 1.0000\n",
      "Epoch 74/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 75/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0417 - acc: 1.0000\n",
      "Epoch 76/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 77/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0401 - acc: 1.0000\n",
      "Epoch 78/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0394 - acc: 1.0000\n",
      "Epoch 79/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0386 - acc: 1.0000\n",
      "Epoch 80/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0379 - acc: 1.0000\n",
      "Epoch 81/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0372 - acc: 1.0000\n",
      "Epoch 82/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0365 - acc: 1.0000\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0358 - acc: 1.0000\n",
      "Epoch 84/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 85/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0345 - acc: 1.0000\n",
      "Epoch 86/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 87/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 88/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0327 - acc: 1.0000\n",
      "Epoch 89/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0321 - acc: 1.0000\n",
      "Epoch 90/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 91/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0311 - acc: 1.0000\n",
      "Epoch 92/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 93/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 94/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 95/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 96/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 97/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 98/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 99/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0271 - acc: 1.0000\n",
      "Epoch 100/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0267 - acc: 1.0000\n",
      "Epoch 101/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 102/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0258 - acc: 1.0000\n",
      "Epoch 103/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 104/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0250 - acc: 1.0000\n",
      "Epoch 105/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 106/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 107/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0238 - acc: 1.0000\n",
      "Epoch 108/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0234 - acc: 1.0000\n",
      "Epoch 109/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 110/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0227 - acc: 1.0000\n",
      "Epoch 111/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0223 - acc: 1.0000\n",
      "Epoch 112/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 113/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 114/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 115/150\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.0210 - acc: 1.0000\n",
      "Epoch 116/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 117/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0203 - acc: 1.0000\n",
      "Epoch 118/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 119/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0197 - acc: 1.0000\n",
      "Epoch 120/150\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 121/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 122/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 123/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 124/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 125/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 126/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 127/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 128/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0172 - acc: 1.0000\n",
      "Epoch 129/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 130/150\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0167 - acc: 1.0000\n",
      "Epoch 131/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 132/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 133/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 134/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 135/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 136/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 137/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 138/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 139/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 140/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 141/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 142/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 143/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 144/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 145/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 146/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 147/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 148/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 149/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 150/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0124 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2cb62940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did the model learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 5)                 35        \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 71\n",
      "Trainable params: 71\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that we have 71 parameters. In the first layer, we have 30 weights and 5 biases. In the second layer, we have 30 weights and 6 biases which show as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.5230295 , -2.3722856 , -0.91483843,  0.9904725 , -2.0497599 ],\n",
       "        [ 0.5239528 , -2.0380375 ,  2.315268  ,  0.71013767,  2.8065956 ],\n",
       "        [ 0.49922538,  1.9143298 , -0.91098964,  0.9905842 ,  0.5758578 ],\n",
       "        [-1.188174  ,  1.0506359 , -0.2596239 , -2.4396398 ,  0.8528697 ],\n",
       "        [-1.1856221 ,  0.7440861 , -0.53387815,  1.9751233 ,  2.1526575 ],\n",
       "        [ 2.0391617 ,  1.6623969 ,  2.4390411 ,  0.5798911 , -0.12870102]],\n",
       "       dtype=float32),\n",
       " array([0.6622595, 1.3193262, 1.170595 , 1.4491769, 1.1961169],\n",
       "       dtype=float32),\n",
       " array([[-0.1914282 ,  0.06342135,  0.9865053 , -1.7767843 ,  2.4452999 ,\n",
       "         -0.830269  ],\n",
       "        [-2.1160614 ,  2.7547426 , -2.2330601 , -1.1951566 ,  0.20668276,\n",
       "          0.33688462],\n",
       "        [ 1.3490559 , -1.2889181 , -1.0776172 , -1.7162324 ,  1.3834927 ,\n",
       "         -0.3855466 ],\n",
       "        [-1.6822165 , -0.6808207 ,  1.5137229 ,  2.424756  , -0.26901057,\n",
       "         -2.9711552 ],\n",
       "        [ 1.9215815 , -0.06922591, -2.0719502 ,  0.56272125, -1.7760575 ,\n",
       "          0.83846855]], dtype=float32),\n",
       " array([-2.2662177, -2.2439988,  2.5550244, -2.4377017, -2.3810773,\n",
       "         2.371451 ], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "biases = model.layers[0].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# Set one layer as output we can get every layers' information given the input\n",
    "\n",
    "dense1_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer('input').output)\n",
    "\n",
    "dense1_output = dense1_layer_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 4.2842922e+00, 0.0000000e+00, 1.2123585e-04,\n",
       "       2.6248446e+00], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2_layer_model = Model(inputs=model.input, outputs=model.get_layer('layer1').output)\n",
    "\n",
    "dense2_output = dense2_layer_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8543303e-03, 9.9991524e-01, 3.9637089e-06, 2.2812188e-03,\n",
       "       2.1130741e-03, 9.9756479e-01], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense2_output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after calculating by two layers weights, the training data reduce the 1 value in the origin place and increase the value corresponding to the label place with value 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad activation function and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we choose a bad activation function what would happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 1.3923 - acc: 0.5190\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 1.3921 - acc: 0.5730\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3920 - acc: 0.5770\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3918 - acc: 0.6490\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3917 - acc: 0.5380\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3915 - acc: 0.5770\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3914 - acc: 0.5210\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3912 - acc: 0.5180\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3911 - acc: 0.5170\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3910 - acc: 0.5410\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3908 - acc: 0.5460\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3907 - acc: 0.5910\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3906 - acc: 0.5480\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3905 - acc: 0.4830\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3904 - acc: 0.5360\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3903 - acc: 0.5970\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3901 - acc: 0.4690\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3900 - acc: 0.4650\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3900 - acc: 0.5570\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3899 - acc: 0.5250\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3898 - acc: 0.5220\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3897 - acc: 0.5380\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3896 - acc: 0.4790\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 1.3895 - acc: 0.4760\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 1.3894 - acc: 0.4440\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 1.3894 - acc: 0.4900\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 1.3893 - acc: 0.5160\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3892 - acc: 0.5580\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3891 - acc: 0.5450\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3891 - acc: 0.5070\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3890 - acc: 0.5340\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3889 - acc: 0.5390\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3889 - acc: 0.5210\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3888 - acc: 0.5180\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3888 - acc: 0.4880\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3887 - acc: 0.4890\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3886 - acc: 0.5210\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3886 - acc: 0.5010\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3885 - acc: 0.5300\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3885 - acc: 0.5290\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3884 - acc: 0.4870\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3884 - acc: 0.5300\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3883 - acc: 0.5440\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3883 - acc: 0.4310\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3882 - acc: 0.5140\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3882 - acc: 0.4920\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3882 - acc: 0.4780\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3881 - acc: 0.4940\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3881 - acc: 0.4520\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3880 - acc: 0.4830\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3880 - acc: 0.4680\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3880 - acc: 0.4890\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3879 - acc: 0.5150\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3879 - acc: 0.4190\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3879 - acc: 0.5080\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3878 - acc: 0.3890\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3878 - acc: 0.4620\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3878 - acc: 0.5020\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3877 - acc: 0.4380\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3877 - acc: 0.4660\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3877 - acc: 0.4190\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3876 - acc: 0.5080\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3876 - acc: 0.4940\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3876 - acc: 0.4630\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3876 - acc: 0.4730\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3875 - acc: 0.5370\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3875 - acc: 0.4600\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3875 - acc: 0.5110\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3875 - acc: 0.4180\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3874 - acc: 0.5110\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3874 - acc: 0.4470\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3874 - acc: 0.4640\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3874 - acc: 0.4890\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.5020\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.5310\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3873 - acc: 0.4530\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.4820\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.5280\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3872 - acc: 0.5160\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3872 - acc: 0.4530\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3872 - acc: 0.4610\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3872 - acc: 0.4610\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3872 - acc: 0.5130\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3871 - acc: 0.5250\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.5140\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.5170\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.4790\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.4990\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3871 - acc: 0.4380\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3870 - acc: 0.5080\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3870 - acc: 0.4770\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3870 - acc: 0.4990\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3870 - acc: 0.4960\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3870 - acc: 0.4700\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3870 - acc: 0.4960\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3870 - acc: 0.4990\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3869 - acc: 0.4740\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3869 - acc: 0.5200\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3869 - acc: 0.4620\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3869 - acc: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2de30400>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy cannot converge to 1 because the softmax and categorical cross entropy treat the output as a distribution, however, it is not a classfication problem. This time our output has two value should be 1, so it is not suitable to use softmax and categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.17837621e-05, 4.99764025e-01, 5.15444517e-05, 1.03189086e-04,\n",
       "       1.24050755e-04, 4.99925375e-01], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999999968251359"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.predict(x_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
