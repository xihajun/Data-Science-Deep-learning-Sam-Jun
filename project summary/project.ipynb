{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning structure 1: One input vs one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for one two one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to train it in our model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xihajun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(26, input_dim=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(26, input_dim=1, init='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Activation('linear')) #relu seems better\n",
    "#model.add(Dense(2, input_dim=1,activation='relu'))\n",
    "# change sigomiod to softmax made acc increasing 100%\n",
    "#model.add(Dense(1, activation='softmax'))\n",
    "# change loss from possion to binary it works well :P\n",
    "#tensorboard = TensorBoard(log_dir = \"graphs/1to1\")\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])\n",
    "#model.compile(loss='mse', optimizer='sgd', metrics=['mse', 'mae', 'mape', 'cosine', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, x_train, y_train = caeserde(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",x_as_vector = False, y_as_vector = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 2.9679 - acc: 0.7692\n",
      "Epoch 2/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 3.1282 - acc: 0.8077\n",
      "Epoch 3/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 4.5070 - acc: 0.7308\n",
      "Epoch 4/250\n",
      "26/26 [==============================] - 0s 225us/step - loss: 4.6440 - acc: 0.8077\n",
      "Epoch 5/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 7.5776 - acc: 0.6154\n",
      "Epoch 6/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 3.6494 - acc: 0.7692\n",
      "Epoch 7/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 4.8814 - acc: 0.6923\n",
      "Epoch 8/250\n",
      "26/26 [==============================] - 0s 235us/step - loss: 4.0078 - acc: 0.5769\n",
      "Epoch 9/250\n",
      "26/26 [==============================] - 0s 235us/step - loss: 5.8256 - acc: 0.6154\n",
      "Epoch 10/250\n",
      "26/26 [==============================] - 0s 243us/step - loss: 4.0200 - acc: 0.4615\n",
      "Epoch 11/250\n",
      "26/26 [==============================] - 0s 245us/step - loss: 5.6262 - acc: 0.6154\n",
      "Epoch 12/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 3.8924 - acc: 0.5385\n",
      "Epoch 13/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 5.9010 - acc: 0.6154\n",
      "Epoch 14/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 3.6859 - acc: 0.6154\n",
      "Epoch 15/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 5.1842 - acc: 0.6923\n",
      "Epoch 16/250\n",
      "26/26 [==============================] - 0s 252us/step - loss: 3.5772 - acc: 0.6538\n",
      "Epoch 17/250\n",
      "26/26 [==============================] - 0s 236us/step - loss: 5.4202 - acc: 0.7308\n",
      "Epoch 18/250\n",
      "26/26 [==============================] - 0s 207us/step - loss: 3.5336 - acc: 0.6154\n",
      "Epoch 19/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 5.0872 - acc: 0.6923\n",
      "Epoch 20/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 3.5759 - acc: 0.6923\n",
      "Epoch 21/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 5.1374 - acc: 0.7308\n",
      "Epoch 22/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 3.6133 - acc: 0.5769\n",
      "Epoch 23/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 6.4205 - acc: 0.1538\n",
      "Epoch 24/250\n",
      "26/26 [==============================] - 0s 233us/step - loss: 9.4935 - acc: 0.1538\n",
      "Epoch 25/250\n",
      "26/26 [==============================] - 0s 229us/step - loss: 9.4997 - acc: 0.0000e+00\n",
      "Epoch 26/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 5.4257 - acc: 0.2692\n",
      "Epoch 27/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 4.1806 - acc: 0.5385\n",
      "Epoch 28/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 4.0172 - acc: 0.6154\n",
      "Epoch 29/250\n",
      "26/26 [==============================] - 0s 184us/step - loss: 4.6005 - acc: 0.5769\n",
      "Epoch 30/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 4.3425 - acc: 0.5769\n",
      "Epoch 31/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 6.5611 - acc: 0.4615\n",
      "Epoch 32/250\n",
      "26/26 [==============================] - 0s 266us/step - loss: 3.8246 - acc: 0.4231\n",
      "Epoch 33/250\n",
      "26/26 [==============================] - 0s 230us/step - loss: 4.5220 - acc: 0.4231\n",
      "Epoch 34/250\n",
      "26/26 [==============================] - 0s 248us/step - loss: 3.9125 - acc: 0.4615\n",
      "Epoch 35/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 5.4761 - acc: 0.5000\n",
      "Epoch 36/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 3.4718 - acc: 0.4615\n",
      "Epoch 37/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 4.6087 - acc: 0.6154\n",
      "Epoch 38/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 3.3276 - acc: 0.6538\n",
      "Epoch 39/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 4.6557 - acc: 0.6923\n",
      "Epoch 40/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 3.2706 - acc: 0.8077\n",
      "Epoch 41/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 4.7209 - acc: 0.7692\n",
      "Epoch 42/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 3.2200 - acc: 0.8462\n",
      "Epoch 43/250\n",
      "26/26 [==============================] - 0s 183us/step - loss: 4.2698 - acc: 0.7692\n",
      "Epoch 44/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 3.1764 - acc: 0.8462\n",
      "Epoch 45/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 4.2268 - acc: 0.7692\n",
      "Epoch 46/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 3.3380 - acc: 0.8462\n",
      "Epoch 47/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 4.3950 - acc: 0.7692\n",
      "Epoch 48/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 3.1799 - acc: 0.8462\n",
      "Epoch 49/250\n",
      "26/26 [==============================] - 0s 217us/step - loss: 3.8134 - acc: 0.7692\n",
      "Epoch 50/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 3.0399 - acc: 0.8462\n",
      "Epoch 51/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 3.7816 - acc: 0.8077\n",
      "Epoch 52/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 3.2057 - acc: 0.8462\n",
      "Epoch 53/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 3.8867 - acc: 0.8077\n",
      "Epoch 54/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 3.1957 - acc: 0.8077\n",
      "Epoch 55/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 2.6432 - acc: 0.8462\n",
      "Epoch 56/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 2.7360 - acc: 0.8077\n",
      "Epoch 57/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 1.8689 - acc: 0.7692\n",
      "Epoch 58/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 2.5893 - acc: 0.7692\n",
      "Epoch 59/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 4.7122 - acc: 0.6538\n",
      "Epoch 60/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 5.6233 - acc: 0.8077\n",
      "Epoch 61/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 5.7170 - acc: 0.8462\n",
      "Epoch 62/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 2.7051 - acc: 0.8077\n",
      "Epoch 63/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 1.3896 - acc: 0.7692\n",
      "Epoch 64/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 1.4674 - acc: 0.8462\n",
      "Epoch 65/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 1.1732 - acc: 0.8462\n",
      "Epoch 66/250\n",
      "26/26 [==============================] - 0s 231us/step - loss: 1.3639 - acc: 0.8077\n",
      "Epoch 67/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 2.2771 - acc: 0.8462\n",
      "Epoch 68/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 3.5320 - acc: 0.7308\n",
      "Epoch 69/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 4.9933 - acc: 0.5385\n",
      "Epoch 70/250\n",
      "26/26 [==============================] - 0s 230us/step - loss: 4.6315 - acc: 0.6923\n",
      "Epoch 71/250\n",
      "26/26 [==============================] - 0s 268us/step - loss: 3.7577 - acc: 0.3846\n",
      "Epoch 72/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 2.0347 - acc: 0.6923\n",
      "Epoch 73/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 2.1122 - acc: 0.5000\n",
      "Epoch 74/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 1.5542 - acc: 0.5769\n",
      "Epoch 75/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 1.6224 - acc: 0.6154\n",
      "Epoch 76/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 1.3527 - acc: 0.6538\n",
      "Epoch 77/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 1.3323 - acc: 0.5769\n",
      "Epoch 78/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 1.9121 - acc: 0.4615\n",
      "Epoch 79/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 4.9303 - acc: 0.1923\n",
      "Epoch 80/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 2.5259 - acc: 0.2692\n",
      "Epoch 81/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 1.3473 - acc: 0.1154\n",
      "Epoch 82/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 1.8817 - acc: 0.1538\n",
      "Epoch 83/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 1.3775 - acc: 0.2308\n",
      "Epoch 84/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 1.7190 - acc: 0.2692\n",
      "Epoch 85/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 2.0272 - acc: 0.6154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 3.0012 - acc: 0.5769\n",
      "Epoch 87/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 2.8847 - acc: 0.6923\n",
      "Epoch 88/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 3.0617 - acc: 0.7692\n",
      "Epoch 89/250\n",
      "26/26 [==============================] - 0s 207us/step - loss: 2.1063 - acc: 0.7308\n",
      "Epoch 90/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 1.7043 - acc: 0.7308\n",
      "Epoch 91/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 1.0985 - acc: 0.6538\n",
      "Epoch 92/250\n",
      "26/26 [==============================] - 0s 207us/step - loss: 0.8734 - acc: 0.7692\n",
      "Epoch 93/250\n",
      "26/26 [==============================] - 0s 230us/step - loss: 0.4780 - acc: 0.8462\n",
      "Epoch 94/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 0.1695 - acc: 0.8462\n",
      "Epoch 95/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 0.1351 - acc: 0.8077\n",
      "Epoch 96/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 0.1764 - acc: 0.8462\n",
      "Epoch 97/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 0.2975 - acc: 0.7308\n",
      "Epoch 98/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 0.5630 - acc: 0.6154\n",
      "Epoch 99/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 0.7982 - acc: 0.3462\n",
      "Epoch 100/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 1.0754 - acc: 0.2308\n",
      "Epoch 101/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 0.8730 - acc: 0.2692\n",
      "Epoch 102/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 0.5603 - acc: 0.5769\n",
      "Epoch 103/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 0.2265 - acc: 0.7308\n",
      "Epoch 104/250\n",
      "26/26 [==============================] - 0s 241us/step - loss: 0.1433 - acc: 0.8846\n",
      "Epoch 105/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 0.1029 - acc: 0.9231\n",
      "Epoch 106/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 0.0705 - acc: 0.9615\n",
      "Epoch 107/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 0.0476 - acc: 1.0000\n",
      "Epoch 108/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 0.0343 - acc: 1.0000\n",
      "Epoch 109/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 0.0270 - acc: 1.0000\n",
      "Epoch 110/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 0.0229 - acc: 1.0000\n",
      "Epoch 111/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 112/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 0.0170 - acc: 1.0000\n",
      "Epoch 113/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 114/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 115/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 116/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 0.0111 - acc: 1.0000\n",
      "Epoch 117/250\n",
      "26/26 [==============================] - 0s 233us/step - loss: 0.0102 - acc: 1.0000\n",
      "Epoch 118/250\n",
      "26/26 [==============================] - 0s 214us/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 119/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 120/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 0.0079 - acc: 1.0000\n",
      "Epoch 121/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 122/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 123/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 124/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 125/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 0.0060 - acc: 1.0000\n",
      "Epoch 126/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 127/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 128/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 129/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 130/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 131/250\n",
      "26/26 [==============================] - 0s 217us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 132/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 0.0045 - acc: 1.0000\n",
      "Epoch 133/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 134/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 135/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 136/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 137/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 138/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 139/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 140/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 141/250\n",
      "26/26 [==============================] - 0s 207us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 142/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 143/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 144/250\n",
      "26/26 [==============================] - 0s 242us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 145/250\n",
      "26/26 [==============================] - 0s 264us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 146/250\n",
      "26/26 [==============================] - 0s 249us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 147/250\n",
      "26/26 [==============================] - 0s 235us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 148/250\n",
      "26/26 [==============================] - 0s 240us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 149/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 150/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 151/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 152/250\n",
      "26/26 [==============================] - 0s 227us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 153/250\n",
      "26/26 [==============================] - 0s 231us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 154/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 155/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 156/250\n",
      "26/26 [==============================] - 0s 227us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 157/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 158/250\n",
      "26/26 [==============================] - 0s 235us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 159/250\n",
      "26/26 [==============================] - 0s 229us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 160/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 161/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 162/250\n",
      "26/26 [==============================] - 0s 230us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 163/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 164/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 165/250\n",
      "26/26 [==============================] - 0s 250us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 166/250\n",
      "26/26 [==============================] - 0s 229us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 167/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 168/250\n",
      "26/26 [==============================] - 0s 244us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 169/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 0.0024 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 171/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 172/250\n",
      "26/26 [==============================] - 0s 228us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 173/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 174/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 175/250\n",
      "26/26 [==============================] - 0s 214us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 176/250\n",
      "26/26 [==============================] - 0s 238us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 177/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 178/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 179/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 180/250\n",
      "26/26 [==============================] - 0s 192us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 181/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 182/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 183/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 9.8867e-04 - acc: 1.0000\n",
      "Epoch 184/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 9.4380e-04 - acc: 1.0000\n",
      "Epoch 185/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 8.8918e-04 - acc: 1.0000\n",
      "Epoch 186/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 8.4012e-04 - acc: 1.0000\n",
      "Epoch 187/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 8.0341e-04 - acc: 1.0000\n",
      "Epoch 188/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 7.6005e-04 - acc: 1.0000\n",
      "Epoch 189/250\n",
      "26/26 [==============================] - 0s 186us/step - loss: 7.2253e-04 - acc: 1.0000\n",
      "Epoch 190/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 6.9586e-04 - acc: 1.0000\n",
      "Epoch 191/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 6.6747e-04 - acc: 1.0000\n",
      "Epoch 192/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 6.3400e-04 - acc: 1.0000\n",
      "Epoch 193/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 6.0313e-04 - acc: 1.0000\n",
      "Epoch 194/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 5.7437e-04 - acc: 1.0000\n",
      "Epoch 195/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 5.4798e-04 - acc: 1.0000\n",
      "Epoch 196/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 5.3046e-04 - acc: 1.0000\n",
      "Epoch 197/250\n",
      "26/26 [==============================] - 0s 183us/step - loss: 5.1050e-04 - acc: 1.0000\n",
      "Epoch 198/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 4.8867e-04 - acc: 1.0000\n",
      "Epoch 199/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 4.7415e-04 - acc: 1.0000\n",
      "Epoch 200/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 4.5799e-04 - acc: 1.0000\n",
      "Epoch 201/250\n",
      "26/26 [==============================] - 0s 240us/step - loss: 4.3915e-04 - acc: 1.0000\n",
      "Epoch 202/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 4.2211e-04 - acc: 1.0000\n",
      "Epoch 203/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 4.0900e-04 - acc: 1.0000\n",
      "Epoch 204/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 3.9573e-04 - acc: 1.0000\n",
      "Epoch 205/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 3.8017e-04 - acc: 1.0000\n",
      "Epoch 206/250\n",
      "26/26 [==============================] - 0s 224us/step - loss: 3.6676e-04 - acc: 1.0000\n",
      "Epoch 207/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 3.5644e-04 - acc: 1.0000\n",
      "Epoch 208/250\n",
      "26/26 [==============================] - 0s 228us/step - loss: 3.4548e-04 - acc: 1.0000\n",
      "Epoch 209/250\n",
      "26/26 [==============================] - 0s 244us/step - loss: 3.3279e-04 - acc: 1.0000\n",
      "Epoch 210/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 3.2243e-04 - acc: 1.0000\n",
      "Epoch 211/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 3.1331e-04 - acc: 1.0000\n",
      "Epoch 212/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 3.0473e-04 - acc: 1.0000\n",
      "Epoch 213/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 2.9666e-04 - acc: 1.0000\n",
      "Epoch 214/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 2.8802e-04 - acc: 1.0000\n",
      "Epoch 215/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 2.7844e-04 - acc: 1.0000\n",
      "Epoch 216/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 2.7043e-04 - acc: 1.0000\n",
      "Epoch 217/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 2.6327e-04 - acc: 1.0000\n",
      "Epoch 218/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 2.5643e-04 - acc: 1.0000\n",
      "Epoch 219/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 2.5028e-04 - acc: 1.0000\n",
      "Epoch 220/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 2.4340e-04 - acc: 1.0000\n",
      "Epoch 221/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 2.3586e-04 - acc: 1.0000\n",
      "Epoch 222/250\n",
      "26/26 [==============================] - 0s 223us/step - loss: 2.3070e-04 - acc: 1.0000\n",
      "Epoch 223/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 2.2442e-04 - acc: 1.0000\n",
      "Epoch 224/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 2.1899e-04 - acc: 1.0000\n",
      "Epoch 225/250\n",
      "26/26 [==============================] - 0s 219us/step - loss: 2.1371e-04 - acc: 1.0000\n",
      "Epoch 226/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 2.0806e-04 - acc: 1.0000\n",
      "Epoch 227/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 2.0352e-04 - acc: 1.0000\n",
      "Epoch 228/250\n",
      "26/26 [==============================] - 0s 229us/step - loss: 1.9809e-04 - acc: 1.0000\n",
      "Epoch 229/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 1.9228e-04 - acc: 1.0000\n",
      "Epoch 230/250\n",
      "26/26 [==============================] - 0s 214us/step - loss: 1.8731e-04 - acc: 1.0000\n",
      "Epoch 231/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 1.8302e-04 - acc: 1.0000\n",
      "Epoch 232/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 1.7848e-04 - acc: 1.0000\n",
      "Epoch 233/250\n",
      "26/26 [==============================] - 0s 230us/step - loss: 1.7478e-04 - acc: 1.0000\n",
      "Epoch 234/250\n",
      "26/26 [==============================] - 0s 230us/step - loss: 1.7030e-04 - acc: 1.0000\n",
      "Epoch 235/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 1.6557e-04 - acc: 1.0000\n",
      "Epoch 236/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 1.6163e-04 - acc: 1.0000\n",
      "Epoch 237/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 1.5801e-04 - acc: 1.0000\n",
      "Epoch 238/250\n",
      "26/26 [==============================] - 0s 238us/step - loss: 1.5431e-04 - acc: 1.0000\n",
      "Epoch 239/250\n",
      "26/26 [==============================] - 0s 238us/step - loss: 1.5118e-04 - acc: 1.0000\n",
      "Epoch 240/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 1.4745e-04 - acc: 1.0000\n",
      "Epoch 241/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 1.4356e-04 - acc: 1.0000\n",
      "Epoch 242/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 1.4077e-04 - acc: 1.0000\n",
      "Epoch 243/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 1.3751e-04 - acc: 1.0000\n",
      "Epoch 244/250\n",
      "26/26 [==============================] - 0s 227us/step - loss: 1.3454e-04 - acc: 1.0000\n",
      "Epoch 245/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 1.3171e-04 - acc: 1.0000\n",
      "Epoch 246/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 1.2865e-04 - acc: 1.0000\n",
      "Epoch 247/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 1.2618e-04 - acc: 1.0000\n",
      "Epoch 248/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 1.2317e-04 - acc: 1.0000\n",
      "Epoch 249/250\n",
      "26/26 [==============================] - 0s 220us/step - loss: 1.2010e-04 - acc: 1.0000\n",
      "Epoch 250/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 1.1755e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c65465198>"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(x_train)/25, np.array(y_train), epochs = 250)#, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the result seems weird that for the same data, the training result shows that we got 100% accuracy but the prediction is different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try to find out why it happens like that? How to calculate the accuracy inside the keras?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.       ],\n",
       "       [ 1.1770233],\n",
       "       [ 1.8817011],\n",
       "       [ 2.6497588],\n",
       "       [ 3.8261323],\n",
       "       [ 4.9017053],\n",
       "       [ 5.5752444],\n",
       "       [ 6.406455 ],\n",
       "       [ 7.2090154],\n",
       "       [ 8.315391 ],\n",
       "       [ 9.716503 ],\n",
       "       [10.22046  ],\n",
       "       [10.9867115],\n",
       "       [11.990413 ],\n",
       "       [12.971404 ],\n",
       "       [13.825737 ],\n",
       "       [14.695664 ],\n",
       "       [16.024948 ],\n",
       "       [16.969534 ],\n",
       "       [17.772316 ],\n",
       "       [18.562874 ],\n",
       "       [19.441145 ],\n",
       "       [20.308346 ],\n",
       "       [19.132788 ],\n",
       "       [19.681318 ],\n",
       "       [18.805635 ]], dtype=float32)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array(x_train)/25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "26/26 [==============================] - 2s 74ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.5215601921081543, 0.3076923191547394]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(np.array(x_train)/25, np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "text, x_train, y_train = caeserde(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",x_as_vector = False, y_as_vector = False)\n",
    "#print(confusion_matrix(y_train, np.rint(model.predict(np.array(x_train)/25))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1c654baf28>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGfCAYAAACNytIiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJztnXucXGV9/z+fRC4bTKKoFblIvEAVTA24qEAbldQajcraiCa6odboGiO1SImVmB9B0sbUhqgVNa5URaPgPdKKaAti1YJmNWlCABGRS4CKgGsQLQLz/f0xszjZmTPX5znne5583r7Oy53ZOe/zOTMk8+S5fWlmEEIIIYQoiilFBxBCCCHE3o0aI0IIIYQoFDVGhBBCCFEoaowIIYQQolDUGBFCCCFEoagxIoQQQohCUWNECCGEEB1D8hMk7yJ5TcbvSfJfSN5IcjvJY9s51RgRQgghRDd8CsD8Fr9/KYAjascIgI+2E6oxIoQQQoiOMbP/AnBvi5ecDODTVuVqAI8h+aRWzkeFDNiMB+++qe8tXgcO/rMQUYQQQohceOj3tzPP64X4rp1g3yc87S2o9mhMMGpmo10oDgFwW93jXbXn7sw6IXpjRAghhBDlodbw6KbxMZlmDbGWjSU1RoQQQoiyU3m46AT17AJwWN3jQwHc0eoEzRkRQgghREguAXBqbVXN8wH82swyh2iAghsjq9ZuwNwFizA0vKxnx0v+4oXYec1/4fprv4d3rnhboR45wjs8ZZHDb5aUHJ6yyBHPExyrhDvaQPIiAFcB+GOSu0guJbmM5MSX+aUAbgJwI4CPA1je1mnW3ZwXkicCeJ2ZdfQptJpUM7ZtB6YNDGDlmvXYvGljpiNrAuuUKVNw3c7vYv7LFmPXrjtx9VWXYnjJclx33U87iRbUI0d4h6cscvjNkpLDUxY5+vPkPoH1zuuCTWDd50nPzDU70GHPCMk5JN9H8mYA/wDg+hAXH5wzGzNnTO/5/Ocedwx+9rOb8fOf34oHH3wQX/jC1/DKV7ykEI8c4R2essjhN0tKDk9Z5IjnEY1kNkZIHknybJLXATgf1WU6NLMXmdmHckvYgoMPOQi37frDnJhdt9+Jgw8+qBCPHOEdnrLI4TdLSg5PWeSI54mBWSXYUQStekauBzAPwCvM7E9rDZCOpuuSHCE5RnLsgk9fFCJn1nUanut22CmUR47wDk9Z5PCbJSWHpyxyxPNEoVIJdxRAq6W9CwEsAvBtkpcBuBjN1w43UL9GOeRGLJO5fdedOOzQgx95fOghT8Kdd/6iEI8c4R2essjhN0tKDk9Z5IjnEY1k9oyY2VfN7LUAngHgSgDvAPBEkh8l+Rc55WvJlrFtePrTn4JZsw7DPvvsg9e85mT8279/qxCPHOEdnrLI4TdLSg5PWeSI54lCjqtpYtB20zMzux/AZwF8luSBAE4B8C4AfX8CK1avw5at2zE+vhvzhoaxfOkSLOxiMtDDDz+Mvz19FS79+ucwdcoUfOrCz+Paa2/oOkcIjxzhHZ6yyOE3S0oOT1nkiOeJgq9Nz7qm66W93aLaNEIIIfY28l7a+/tbfhyuNs3hx+a+tFfbwQshhBBlp6DhlVBEb4yE6NX43R3fdZFDCCGEcElBq2BCodo0QgghhCgUDdMIIYQQJaeozcpCocaIEEIIUXY0TNM7Iaofhqj8GyqLHOEdnrLI4TdLSg5PWeSI5xF7En1p76P2PaTpBbqpfthqAmu/lX+7zSJHfg5PWeTwmyUlh6cscvTnyXtp7wM3fC/Yl/l+R/6pz6q9MQhV/bDfyr+hssgR3uEpixx+s6Tk8JRFjnieKFQeDncUQNeNEZKPZ7NqQV3iqfqhl6qQcvjNIoffLCk5PGWRI55HNNKyMULy+SSvJPkVkseQvAbANQB+QXJ+i/Meqdpbqdyf9ZqG54qqfuilKqQcfrPI4TdLSg5PWeSI54lC4rVpzgewEsBMAFcAeKmZXU3yGQAuAnBZs5Pqq/ZmzRnxVP3QS1VIOfxmkcNvlpQcnrLIEc8ThcRX0zzKzL5lZl8E8L9mdjUAmNn1/V7YU/VDL1Uh5fCbRQ6/WVJyeMoiRzyPaKRdz0h9U+t3k37XV99UqOqH/Vb+DZVFjvAOT1nk8JslJYenLHLE80Sh5JuetVzaS/JhAPcDIIABAL+d+BWA/c1sn3YXyBqm6QbVphFCCFEmcl/au/2b4Zb2/slLfFXtNbOpeQURQgghxN6JtoMXQgghSo5ZMfuDhKIUjZEQQywa6hFCCJEsJZ8zUmhtGiGEEEKIUvSMCCGEEKIFJd9nRI0RIYQQouxomKZ3vJSFXrV2A+YuWISh4WU9nR8yixx+s8jhN0tKDk9Z5IjnCU7JC+W13GckBFn7jORdFrrVBNaxbTswbWAAK9esx+ZNGzNf12oCq5cy1yk5PGWRw2+WlByessjRnyfvfUb+b8uXg32Z73/cwtz3GWlXKO/pJE9s8vyfkXxaPxf2VBZ6cM5szJwxvevzQmeRw28WOfxmScnhKYsc8TxRKHmhvHbDNB8AcF+T539X+13PeCoLHQIv95OSw1MWOfxmScnhKYsc8TxRqFTCHQXQrjEyy8y2T37SzMYAzMo6ieQIyTGSY5XK/VmvaXiuqLLQIfByPyk5PGWRw2+WlByessgRzyMaabeaZv8WvxvI+oWZjQIYBbLnjHgqCx0CL/eTksNTFjn8ZknJ4SmLHPE8UUh8Nc0Wkm+e/CTJpQB+1M+FPZWFDoGX+0nJ4SmLHH6zpOTwlEWOeJ4olHyYpl3PyOkAvkry9fhD42MQwL4AXtXPhT2VhV6xeh22bN2O8fHdmDc0jOVLl2Bhl5OSvNxPSg5PWeTwmyUlh6cscsTziEY6WtpL8kUAnlV7uNPMruj0AlnDNHmj2jRCCCHyIvelvd/9TLilvX+2JPelvR3twGpm3wbw7chZhBBCCNEDZa/aq0J5QgghhCiUvaY2TYghFg31CCGEcIkK5QkhhBCiUBJf2iuEEEIIERVV7Q3kUeXfOA5PWeTwmyUlh6cscsTzBKfk+4zsNVV7Q3hU+dfvZyOHPpsUHJ6yyNGfJ++lvb/7z43BvswH/nyZr6q9MUmtEqMq/4Z3eMoih98sKTk8ZZEjnkc00nFjhOQTSD4h1IVViTFOjpQcnrLI4TdLSg5PWeSI54lCyYdpWjZGWOUckncDuB7ADSR/SfLsfi+sSoxxcqTk8JRFDr9ZUnJ4yiJHPE8UrBLuKIB2PSOnAzgRwHFm9jgzeyyA5wE4keQ7sk4iOUJyjORYpXJ/09eoEmOcHCk5PGWRw2+WlByessgRzyMaadcYORXAYjP7+cQTZnYTgOHa75piZqNmNmhmg1OmHND0NarEGCdHSg5PWeTwmyUlh6cscsTzRKHkwzTtNj3bx8zunvykmf2S5D79XDi1Soyq/Bve4SmLHH6zpOTwlEWOeJ4olHwH1pZLe0n+2MyO7fZ39Xip2hsCbQcvhBCiE3Jf2vv1D4Rb2rvgdHdVe59NcneT5wlg/wh5hBBCCNEtJd8OvmVjxMym5hVECCGEED1S8mEaFcrrAi+VfwEN9wghhEgHNUaEEEKIspPyMI0QQgghSkDJh2kKrdorhBBCCFFoYyS1stD9Olat3YC5CxZhaHhZT9cPlcOTw1MWOfxmScnhKYsc8TzBKfl28C33GQlB1j4jZS0L3a+j1QTWsW07MG1gACvXrMfmTRtbXi9rAquX97WMn40c+myKdnjKIkd/ntz3GfnSP4TbZ+TVq3LfZ6SwnpHUykKHcAzOmY2ZM6Z3dU6MHF4cnrLI4TdLSg5PWeSI5xGNtKva+866n0+Z9Lu1/Vw4tbLQXkpLe7mX1D4bOfxmScnhKYsc8TxRKHltmnY9I4vqfj5r0u/mZ53USdXe1MpCeykt7eVeUvts5PCbJSWHpyxyxPNEwSzcUQDtGiPM+LnZ40fopGpvamWhvZSW9nIvqX02cvjNkpLDUxY54nlEI+0aI5bxc7PHXZFaWWgvpaW93Etqn40cfrOk5PCURY54niiUfJim00J5BDBQVzSv70J5qZWFDuFYsXodtmzdjvHx3Zg3NIzlS5dgYZeTo7zcS2qfjRx+s6Tk8JRFjnieKJR807PClvburag2jRBCpE/uS3s/+//CLe19/Zrcl/ZqO3ghhBCi7Kg2jRBCCCEKpeTDNGqM5Eyo4ZUQwz0a6hFCCNEtJOcD+CCAqQAuMLN1k37/ZAAXAnhM7TXvMrNLWzlVKE8IIYQoOzntM0JyKoAPA3gpgKMALCZ51KSXrQLwBTM7BtX9yj7SLr56RoQQQoiyk98wzXMB3GhmNwEAyYsBnAzg2rrXGIAZtZ9nArgDbVDPiBBCCCEeoX4X9doxUvfrQwDcVvd4V+25es4BMExyF4BLAfxNu2sW2hhJrSy0F8eqtRswd8EiDA0v6+n8UDn02aTt8JQlJYenLHLE8wQn4KZn9buo147Ruis1W/Y7eWxnMYBPmdmhAF4G4DMkW9fCK2qfkbKWhfbiaDWBdWzbDkwbGMDKNeuxedPGzNdlTWD18n54yiKH3ywpOTxlkaM/T+77jFxwRrh9Rt60ITM7yeMBnGNmL6k9PgsAzOy9da/ZCWC+md1We3wTgOeb2V1Z3nZVe5/c1R10QWplob04AGBwzmzMnDG96/NC5tBnk7bDU5aUHJ6yyBHPU3K2ADiC5FNI7ovqBNVLJr3mVgDzAIDkM1Hdsf2XraTthmk2T/xA8svdJm5FamWhvThC4OlevGSRw2+WlByessgRzxMDq1iwo+V1zB4CcBqAbwK4DtVVMztJnkvylbWX/R2AN5P8HwAXAXiDtRmGabeapr6r5qltXvuHk6qTXUYAgFNnolnl3tTKQntxhMDTvXjJIoffLCk5PGWRI54nCjluelbbM+TSSc+dXffztQBO7MbZT9Xe7JPqJr80a4gA6ZWF9uIIgad78ZJFDr9ZUnJ4yiJHPI9opF1j5Nkkd5O8D8Cf1H7eTfK+ugq+PZFaWWgvjhB4uhcvWeTwmyUlh6cscsTzRMEq4Y4CaDlMY2ZTY104tbLQXhwAsGL1OmzZuh3j47sxb2gYy5cuwcIuJll5uhcvWeTwmyUlh6cscsTzRKHNXA/vFLa0V/SHatMIIYRf8l7a+9sPnxbsu3ba287PNTug7eCFEEKI8qOqvUIIIYQoFDVGRBGEGGLRUI8QQiSClyXGPaJCeUIIIYQoFPWMCCGEEGWn5MM0qtrrLIsXh5fKv6E8coR3eMqSksNTFjnieYJTsXBHAahqr6MsqvwbxyNHeIenLCk5PGWRoz9P7kt7178p3NLeMy/IfWlvYT0jqVViTMkB+Kj8G8ojR3iHpywpOTxlkSOeJwol34G1ZWOE5Mkk31b3+Ackb6odr+7nwqlVYkzJEQJ9Nmk7PGVJyeEpixzxPFEo+TBNu56RdwK4pO7xfgCOA/BCAG/NOonkCMkxkmOVyv1Zr2l4rsyVGFNyhECfTdoOT1lScnjKIkc8j2ik3Wqafc3strrH3zOzewDcQ7J5OV5Uq/YCGAWy54ykVokxJUcI9Nmk7fCUJSWHpyxyxPPEwBJfTfPY+gdmdlrdwyf0c+HUKjGm5AiBPpu0HZ6ypOTwlEWOeJ4olHyYpl3PyA9IvtnMPl7/JMm3APhhPxdOrRJjSg7AR+XfUB45wjs8ZUnJ4SmLHPE8opGWS3tJ/hGAzQAeAPDj2tPPQXXuyJCZte2fUtVev2g7eCGEiEPeS3vv/4fhYN+1B6za5Ktqr5ndBeAEkicBOLr29NfN7IroyYQQQgjRGQUNr4Sio+3ga40PNUCEEEIIERzVphFCCCHKTslX06gxIoQQQpSdkg/TFFooTwghhBBCPSNCCCFE2SmopkwoCu0ZSa0sdEqOVWs3YO6CRRgaXtbT+aFyhPLIEd7hKUtKDk9Z5IjnCU7JNz1ruc9ICLL2GSlrWeiUHK32GRnbtgPTBgawcs16bN60MfN1WfuM6LNJ2+EpS0oOT1nk6M+T+z4j7z4l3D4j//jF3PcZKaxnJLWy0Ck5AGBwzmzMnDG96/NC5/DynsjhN0tKDk9Z5IjniYFVKsGOImjZGCH5IZL/knX0c+HUykKn5AiBPpu0HZ6ypOTwlEWOeJ4olHyYpt0E1rG6n98DYHUnUpIjAEYAgFNnYsqUxgK/qZWFTskRAn02aTs8ZUnJ4SmLHPE8opF228FfOPEzydPrH7c5bxTAKJA9ZyS1stApOUKgzyZth6csKTk8ZZEjnicKe9E+I0HvNLWy0Ck5QqDPJm2HpywpOTxlkSOeJwpWCXcUQGH7jKRWFjolBwCsWL0OW7Zux/j4bswbGsbypUuwsIuJWvps0nZ4ypKSw1MWOeJ5RCMtl/aSvA9/6BGZBuC3E78CYGY2o90FsoZpRPG0WtrbKVlLe4UQYm8m76W9vznjlcG+ax+94ZLcl/a2mzPS+9pOIYQQQuSC7UVzRoQQQgghgqPaNHsxIYZYNNQjhBAOKHnPiBojQgghRNkpaOfUUGiYRgghhBCFop4RIYQQouyUfJim0J6R1MpCy7Enq9ZuwNwFizA0vKyn80NmkSO8w1OWlByessgRzxOcktemabnPSAiy9hkpa1loOfak1QTWsW07MG1gACvXrMfmTRszX9dqAmsZ35O9weEpS0oOT1nk6M+T9z4j9y2bH+zLfPrGy3LfZ6SwnpHUykLL0cjgnNmYOaO/rWq83I8cfrOk5PCURY54nhiYWbCjCFo2RkjeR3J3k+M+krv7uXBqZaHliIOX+5HDb5aUHJ6yyBHPE4WSD9NE2YGV5AiAEQDg1JmYMuWAZq9pdr1ur9O3w1OWlByh8HI/cvjNkpLDUxY54nlEI1FW05jZKIBRIHvOSGploeWIg5f7kcNvlpQcnrLIEc8TBa2m6Y3UykLLEQcv9yOH3ywpOTxlkSOeJwZWsWBHERS2z0hqZaHlaGTF6nXYsnU7xsd3Y97QMJYvXYKFXU728nI/cvjNkpLDUxY54nlEI4Ut7RVpoNo0QgjRSN5Le3/9V/OCfdfOvPDy3Jf2agdWIYQQouyUuzSNatMIIYQQoljUMyL6IsQQi4Z6hBCiP4qaeBoKNUaEEEKIslPyxoiGaYQQQghRKKra6yyLHHuiyr9+HZ6ypOTwlEWOeJ7gVAIeBaCqvY6y7K0OVf4tn8NTlpQcnrLI0Z8n76W9vzrlhcG+zB/7xSv9VO1tUSRvN8lfkrya5LxeL5xaJUY5wjsAVf716vCUJSWHpyxyxPOIRjIbI2Y23cxmNDsAHATgLQA+2OuFU6vEKEd4Ryi83E9KDk9ZUnJ4yiJHPE8USj5M09NqGjN7GMD/kPxQs9+raq8cnqpbermflByesqTk8JRFjnieGJR9aW9fE1jN7GMZz4+a2aCZDTZriADpVWKUI7wjFF7uJyWHpywpOTxlkSOeRzSiqr2OssgRBy/3k5LDU5aUHJ6yyBHPE4W9cZgmBKlVYpQjvANQ5V+vDk9ZUnJ4yiJHPE8MrOS1aVS1VxSOtoMXQqRG3kt771nwgmDftY/7+nf8LO0VQgghhMgD1aYRQgghSk7Zh2nUGBGF42WIJcRwEeDnfoQQexElb4xomEYIIYQQhaKeESGEEKLklH2YRj0jQgghRMmxSrijHSTnk/wJyRtJvivjNa8heS3JnSQ/185ZaGMktbLQcoR3eMmyau0GzF2wCEPDy3q6fqgcnhyesqTk8JRFjnieskJyKoAPA3gpgKMALCZ51KTXHAHgLAAnmtnRAE5v6y1qn5GyloWWIz9H3llaTWAd27YD0wYGsHLNemzetLHl9bImsHp5X8v42ewtDk9Z5OjPk/c+I794Ubh9Rp747ex9RkgeD+AcM3tJ7fFZAGBm7617zfsA3GBmF3R6zZY9IyQPbfG7V3R6kWakVhZajvAOT1kG58zGzBnTuzonRg4vDk9ZUnJ4yiJHPE8UjMEOkiMkx+qOkborHQLgtrrHu2rP1XMkgCNJfp/k1STnt4vfbpjmcpKzJj9J8o0APtBO3orUykLLEd7hLUu/eLmX1D6blByessgRz+Od+mK3tWO07tfNek0m98o8CsARAF4IYDGAC0g+ptU12zVG3gHgP2rjP9UU1S6ZdwB4QdZJ9a2qSuX+rNc0PFfmstByhHd4y9IvXu4ltc8mJYenLHLE88QgxwmsuwAcVvf4UAB3NHnN18zsQTP7OYCfoNo4yaTl0l4zu5TkAwC+QXIIwJsAHAdgrpn9qsV5owBGgew5I6mVhZYjvMNbln7xci+pfTYpOTxlkSOeJwZWyW2KyhYAR5B8CoDbASwC8LpJr9mMao/Ip0g+HtVhm5taSduupjGzywG8AcCVAJ4KYF6rhkinpFYWWo7wDm9Z+sXLvaT22aTk8JRFjnieMmNmDwE4DcA3AVwH4AtmtpPkuSRfWXvZNwHcQ/JaAN8GsMLM7mnlbdkzQvI+VMeCCGA/APMA3MVqX5WZ2Yxebyi1stByhHd4yrJi9Tps2bod4+O7MW9oGMuXLsHCLieuebmX1D6blByessgRzxODPDc9M7NLAVw66bmz6342AGfUjo4obGmvEN5QbRohRCjyXtp7+/EnBfuuPeSqK3LNDmgHViGEEEIUjGrTCCGEECWn7LVp1BgRQgghSk6Oq2mioGEaIYQQQhSKekaEEEKIkuNk77WeUWNECCGEKDkapumD1MpCyxHe4SXLqrUbMHfBIgwNL+vp+qFyeHJ4ypKSw1MWOeJ5xJ4Uts9IWctCy5GfI+8srfYZGdu2A9MGBrByzXps3rSx5fWy9hnx8r6W8bPZWxyessjRnyfvfUZunvPiYF/ms7b9R3n2GSF5ej8XTq0stBzhHZ6yDM6ZjZkzpnd1TowcXhyesqTk8JRFjnieGJiFO4qgn2Gajrd5bUZqZaHlCO/wlqVfvNxLap9NSg5PWeSI5xGN9DOBNbMbh+QIgBEA4NSZmDLlgGavaXiuzGWh5Qjv8JalX7zcS2qfTUoOT1nkiOeJQdknsPbTGMn8BMxsFMAokD1nJLWy0HKEd3jL0i9e7iW1zyYlh6cscsTzxMCs3I2RlsM0JO8jubvJcR+Ag1ud247UykLLEd7hLUu/eLmX1D6blByessgRzyMaadkzYmb9zdhrQWploeUI7/CUZcXqddiydTvGx3dj3tAwli9dgoVdTlzzci+pfTYpOTxlkSOeJwZlr01T2NJeIbzRamlvN2Qt7RVC7D3kvbT3hmfOD/Zde+R1l5Vnaa8QQgghRAi0HbwQNUL1aIToYVHvihCiG8o+gVWNESGEEKLklH1pr4ZphBBCCFEo6hkRQgghSo6Tvdd6RlV7nWWRw2+WEI4Q1X+93IunLCk5PGWRI54nNFZhsKMIVLXXURY5/GbpxhGi+q/3yr+esqTk8JRFjv48eS/tvfZpC4J9mR/1s6/vPUt7U6vEKEd4h6csoe6n3+q/nu7FS5aUHJ6yyBHPE4OKMdhRBIU1RlKrxChHeIenLF6qdXq6Fy9ZUnJ4yiJHPE8MzBjsKIKWE1hJXtLq92b2yozzVLVXjr4dnrJ4qdbp6V68ZEnJ4SmLHPE8opF2q2mOB3AbgIsA/ABAR00mVe2VQ59NHDzdi5csKTk8ZZEjnicGZW8TtRumOQjASgDPAvBBAC8GcLeZfcfMvtPPhVOrxChHeIenLF6qdXq6Fy9ZUnJ4yiJHPE8Myj5npF3V3ocBXAbgMpL7AVgM4EqS55rZh/q5cGqVGOUI7/CUJdT99Fv919O9eMmSksNTFjnieUQjbZf21hohC1BtiMwCcAmAT5jZ7Z1cQFV7xd6GatMIIfJe2rv1yScH+6495tav5d490m4C64WoDtF8A8B7zOyaXFIJIYQQomPKPmek3QTWJQDuB3AkgLfXzSQmADOzGRGzCSGEEGIvoN2cERXSE6JLQgyxaKhHCNENRU08DYUK5QkhhBAlp6jNykKhng8hhBBCFIp6RoQQQoiSU/ZhmkJ7RlIrCy1HeIenLF4cq9ZuwNwFizA0vKyn80PlCOWRw28WOeJ5QmMBjyJou89Iv2TtM1LWstBy5OfwlCVvR6sJrGPbdmDawABWrlmPzZs2Zr4uawKrPhu/Dk9Z5OjPk/c+I//9pIXBvsxPuPPLuXezFNYzklpZaDnCOzxl8eIAgME5szFzxvSuzwudw8t7kpLDUxY54nlEIy0bIyTPbnH8v34unFpZaDnCOzxl8eIIgT4bvw5PWeSI54mBGYMdRdBuAuv9TZ6bBuBNAB4HYE2zk0iOABgBAE6diSlTDmj2mobnylwWWo7wDk9ZvDhCoM/Gr8NTFjnieWJQKTpAn7Tb9Oy8iZ9JTgfwtwDeCOBiAOe1OG8UwCiQPWcktbLQcoR3eMrixRECfTZ+HZ6yyBHPIxppO2eE5IEk/wHAdlQbL8ea2d+b2V39XDi1stByhHd4yuLFEQJ9Nn4dnrLIEc8TAwODHUXQrlDePwP4S1R7OWab2W9CXTi1stByhHd4yuLFAQArVq/Dlq3bMT6+G/OGhrF86RIs7GISnT4bvw5PWeSI54lBxcdoUc+0XNpLsgLgAQAPYc/lxx0XyssaphFCZKPaNEKUm7yX9l75xFOCfde+8BdfzL17RIXyhBBCiJJTKWh4JRTaDl4IIYQoOUXN9QiFGiNCOCTEEIuGeoQQZUGNESGEEKLkJL3PiBBCCCH8U/ZhGlXtdZZFDr9ZUnKEqPwbKoscfrPIEc8j9kRVex1lkcNvljI6Ylb+LeJ+9gaHpyxy9OfJe2nvZU9cFOzLfP4vLvZZtZfk/iSfRfJokvuHuHBqlRjlCO/wlCUlB9B/5d9QWeTwm0WOeJ4YVAIeRdCuau+jSL4PwC4AFwLYBOA2ku8juU8/F06tEqMc4R2esqTkCIWX+0nJ4SmLHPE8opF2PSP/DOBAAE8xs+eY2TEAngbgMQDWZ51EcoTkGMmxSqVZ4d/0KjHKEd7hKUtKjlB4uZ+UHJ6yyBHPE4Oka9MAeDmAI63u3Taz3STfCuB6VKv4NqCqvXLos/HrCIWX+0nJ4SmLHPE8MaiUezFN254RsybNPjN7GHvWquma1CoxyhHe4SlLSo5QeLmflByessgRzyObvBdPAAAgAElEQVQaadczci3JU83s0/VPkhxGtWekZ1KrxChHeIenLCk5gP4r/4bKIoffLHLE88Sg7LVp2lXtPQTAVwD8DsCPUO0NOQ7AAIBXmdnt7S6gqr1CFIO2gxeiOPJe2rv5oNcF+64d+t/PuavaezuA55E8CcDRAAjgG2Z2eR7hhBBCCJE+HW0Hb2ZXALgichYhhBBC9IBq0wghXKLKv0LsPVSaLDsuE4XWphFCCCGEUM+IEEIIUXLKvlJEjREhhBCi5JR9zkihwzSplYWWI7zDUxY59mTV2g2Yu2ARhoaX9XR+yCwpOTxlkSOeR+xJy31GQpC1z0hZy0LLkZ/DU5a91dFqAuvYth2YNjCAlWvWY/OmjZmvazWBtYzvSUyHpyxy9OfJe5+Riw5+fbAv88V3fDb32bCF9YykVhZajvAOT1nkaGRwzmzMnDG96/NCZ0nJ4SmLHPE8MaiAwY4iaNkYIbk/ydNJnk/yLSSDzTFJrSy0HOEdnrLIEQcv9+PF4SmLHPE8ZYfkfJI/IXkjyXe1eN2rSRrJwXbOdj0jFwIYBLADwEsBnNdh0BGSYyTHKpX7s17T8FyZy0LLEd7hKYsccfByP14cnrLIEc8TAwt4tILkVAAfRrVNcBSAxSSPavK66QDeDuAHneRv19NxlJnNron/FcAPO5Ga2SiAUSB7zkhqZaHlCO/wlEWOOHi5Hy8OT1nkiOeJQSW/0ZXnArjRzG4CAJIXAzgZwLWTXrcGwPsAnNmJtF3PyIMTP5jZQx1H7YDUykLLEd7hKYsccfByP14cnrLIEc/jnfrRjdoxUvfrQwDcVvd4V+25+vOPAXCYmf17p9ds1zPybJK7J/wABmqPCcDMbEanF5pMamWh5Qjv8JRFjkZWrF6HLVu3Y3x8N+YNDWP50iVY2OVkPi/348XhKYsc8TwxCLnPSP3oRhOa9cE8MgJCcgqA9wN4QzfXLGxprxDCP6pNI0Rv5L2095OHDAf7rv3r2zdlZid5PIBzzOwltcdnAYCZvbf2eCaAnwH4Te2UgwDcC+CVZjaW5VVtGiGEEEJ0yhYAR5B8Csl9ASwCcMnEL83s12b2eDObZWazAFyNNg0RQNvBCyGEEKUnrwmsZvYQydMAfBPAVACfMLOdJM8FMGZml7Q2NEeNESFEJiGGWDTUI0R88qxNY2aXArh00nNnZ7z2hZ04NUwjhBBCiEJRz4gQQghRcspetVeNESGEEKLkWDElZYJR6DBNamWh5Qjv8JRFjvCeVWs3YO6CRRgaXtZzhhA5PDk8ZZEjnkfsSUf7jJCcBuDptYc/MbMHOr1A1j4jZS0LLUd+Dk9Z5Ojd02oC69i2HZg2MICVa9Zj86aNma9rNYHVy3ui/+bTdnTryXufkY8cFm6fkeW3Ze8zEot2VXv3IfkBVLd7/SSqhfNumqjSV9vytSdSKwstR3iHpyxyxPEMzpmNmTOmd33t0Dm8ODxlkSOeJwaVgEcRtBumOQ/AowEcbmbPMbNjADwTwFNJfhTAV3q9cGploeUI7/CURY54nn7x8p54el/lCO8I6RGNtJvA+jIAR1jdWI6Z7Sb5VgB3o1pCuIFaUZ0RAODUmZgy5YBmr2l4rsxloeUI7/CURY54nn7x8p54el/lCO8I6YmBjxS9064xUrEm77SZPUzyl2Z2dbOT6ovsZM0ZSa0stBzhHZ6yyBHP0y9e3hNP76sc4R0hPTHIawfWWLQbprmW5KmTnyQ5DOC6fi6cWlloOcI7PGWRI56nX7y8J57eVznCO0J6RCPtekbeBuArJN8I4Eeo9gQdB2AAwKv6uXBqZaHlCO/wlEWOOJ4Vq9dhy9btGB/fjXlDw1i+dAkWdjkh0Mt74ul9lSO8I6QnBmXf9KzTpb0nATgaAAHsNLPLO71A1jCNEGLvQLVpxN5I3kt7z3tyuKW9f3dr/kt7O9qB1cyuAHBF5CxCCCGE2AvRdvBCCCFEySn7EIQaI0KIqIQYYgkx1ANouEekS9lX06gxIoQQQpScsk9gLbRQnhBCCCGEqvY6yyKH3yxy+MySWuVfT1nkiOcJjQU8iqCjpb39oKq9cuizSc+Rd5YQlX+B7Dkje+v7Kkc6VXv/8fDXB/syf/ctn/VVtTcmqVVilCO8w1MWOfxmSanyr6cscsTziEZ6aoyQnEry9f1cOLVKjHKEd3jKIofvLP3i6V68ZJEjnicGlYBHEbRsjJCcQfIskueT/AtW+RsANwF4TYvzRkiOkRyrVO7Pek3Dc2WuxChHeIenLHL4ztIvnu7FSxY54nliUPY5I+2W9n4GwK8AXAXgTQBWANgXwMlmti3rJFXtlUOfTdoOb1n6xdO9eMkiRzyPaKTdMM1TzewNZvYxAIsBDAJ4eauGSKekVolRjvAOT1nk8J2lXzzdi5cscsTzxKDswzTtekYenPjBzB4m+XMzuy/EhVOrxChHeIenLHL4zZJS5V9PWeSI54lB2Xdgbbm0l+TDACYmfRDAAIDf1n42M5vR7gKq2iuE6BdtBy/KRt5Le8+eFW5p77k357+0t2XPiJlNzSuIEEIIIXqjUvJSeapNI4QQQpSccjdF1BgRQpSAUMMrIYZ7NNQjRHjUGBFCCCFKTtmr9qoxIoQQQpScss8ZKbRqrxBCCCFEoY2R1MpCyxHe4SmLHH6zhHCsWrsBcxcswtDwsp7OD5UjlEeO8I6QntCUfTv4lvuMhCBrn5GyloWWIz+Hpyxy+M3SjaPVBNaxbTswbWAAK9esx+ZNGzNflzWBdW9+X/cGR7eevPcZOXPW4mBf5utvvij3fUbaFco7juRBdY9PJfk1kv9C8sB+LpxaWWg5wjs8ZZHDb5ZQ9zM4ZzZmzpje9Xmhc3h5T+SI5xGNtBum+RiA3wMAybkA1gH4NIBfo1YIr1dSKwstR3iHpyxy+M3ipay73te0HSE9MajAgh1F0G41zVQzu7f282sBjJrZlwF8mWRmsTySIwBGAIBTZ2LKlAOavabhuTKXhZYjvMNTFjn8ZvFS1l3va9qOkJ4Y+EjRO+16RqaSnGiwzANwRd3vMhsyZjZqZoNmNtisIQKkVxZajvAOT1nk8JvFS1l3va9pO0J6RCPtGiMXAfgOya8B+B2A7wIAyaejOlTTM6mVhZYjvMNTFjn8ZvFS1l3va9qOkJ4YVAIeRdCuUN4/krwcwJMAfMv+0B81BcDf9HPh1MpCyxHe4SmLHH6zhLqfFavXYcvW7Rgf3415Q8NYvnQJFnYxOVHva9qOkJ4YWMkHagpb2iuEEHmj2jQiL/Je2vv2Wa8N9l37Lzd/PvelvdoOXgghhCg5qk0jhBBCiEIpe20aNUaEEHsNIYZYNNQjRHjUGBFCCCFKTrn7RdQYEUIIIUpP2YdpCq3aK4QQQgiR2Rip23k1GqmVhZYjvMNTFjn8ZvHiWLV2A+YuWISh4WU9nR8yixzhHSE9oSn7pmeZ+4yQ/LGZHdvvBbL2GSlrWWg58nN4yiKH3yx5O1pNYB3btgPTBgawcs16bN60MfN1rSawlvE92Rsc3Xry3mfkTbNeHWyc5oKbv5T7PiOthmmihkmtLLQc4R2essjhN4sXBwAMzpmNmTOmd31e6CxyhHeE9IhGWjVGnkDyjKyj3wunVhZajvAOT1nk8JvFiyMUXu5HjnieGJR9mKbVvJCpAB6NHnpISI4AGAEATp2JZpV7UysLLUd4h6cscvjN4sURCi/3I0c8TwzKXpumVWPkTjM7txepmY0CGAWy54ykVhZajvAOT1nk8JvFiyMUXu5Hjnge0Uhhc0ZSKwstR3iHpyxy+M3ixREKL/cjRzxPDFIeppkX88KplYWWI7zDUxY5/Gbx4gCAFavXYcvW7Rgf3415Q8NYvnQJFnY5wdHL/cgRzxODipPhol7JXNobiqxhGiGEKCOqTSM6Ie+lvUsO/8tg37WfueUruS/t1XbwQgghRMkp+7/61RgRQoguUOVf4RHVphFCCCGE6AP1jAghhBAlJ+V9RoQQQghRAopakhuKQodpUqvEKEd4h6cscvjNkpJDlX/9OkJ6xJ4UtrS3rJUY5cjP4SmLHH6zlNGhyr/lc3TryXtp7ymHnxzsy/yLt3zNVdXeqKRWiVGO8A5PWeTwmyUlB6DKv14dIT0xsID/K4KWjZEm1XrfQXIJyaf0e+HUKjHKEd7hKYscfrOk5AiFl/tJyRHSIxpp1zMyfdIxA8AggG+QXJR1EskRkmMkxyqV+7Ne0/BcmSsxyhHe4SmLHH6zpOQIhZf7SckR0hODlGvTwMze0+x5kgcC+E8AF2ecp6q9cuizSdjhKUtKjlB4uZ+UHCE9MfDSKOqVnuaMmNm96LOqb2qVGOUI7/CURQ6/WVJyhMLL/aTkCOkpOyTnk/wJyRtJvqvJ788geS3J7SQvJ3l4O2dP+4yQPAnAr3o5d4LUKjHKEd7hKYscfrOk5ABU+derI6QnBnltB09yKoAPA3gxgF0AtpC8xMyurXvZVgCDZvZbkm8F8D4Ar23pbdW1Q3IHGuvvHAjgDgCnmtn17YKraq8QQuyJatOkT95Le1/x5JcH+679t1v/PTM7yeMBnGNmL6k9PgsAzOy9Ga8/BsD5ZnZiq2u26xl5+aTHBuAeM2s+K1UIIYQQuRNySS7JEQAjdU+N1uaCAsAhAG6r+90uAM9roVsK4BvtrtluAust7QRCCCGESIf6RShNaNZr0rQlRHIY1RW4L2h3TdWmEUIIIUpOXnNGUO0JOazu8aGoTt3YA5J/DuDdAF5gZg+0k6oxIoQQQpScHJf2bgFwRG3z09sBLALwuvoX1OaJfAzAfDO7qxNpoYXyhBBCCFEezOwhAKcB+CaA6wB8wcx2kjyX5CtrL/tnAI8G8EWS20he0s6rnhEhhBCi5OS5c6qZXQrg0knPnV3385936yy0ZyS1stByhHd4yiKH3ywpOVat3YC5CxZhaHhZT+eHzCJHPE9oyl4or+U+IyHI2mekrGWh5cjP4SmLHH6zlNHRap+RsW07MG1gACvXrMfmTRszX9dqn5EyvifeHd168t5n5C8Omx/sy/xbt12Wa3agRc8IyfNJnhDrwqmVhZYjvMNTFjn8ZknJAQCDc2Zj5ozpXZ8XOosc8TwxqMCCHUXQapjmpwDOI3kzyX8iOSfkhVMrCy1HeIenLHL4zZKSIxRe7iclR0hPDMws2FEEmY0RM/ugmR2P6mYl9wL4JMnrSJ5N8shWUpIjJMdIjlUqzTdrTa0stBzhHZ6yyOE3S0qOUHi5n5QcIT2ikbYTWM3sFjP7JzM7BtW1xK9CdTlPq3NGzWzQzAanTDmg6WtSKwstR3iHpyxy+M2SkiMUXu4nJUdITwxSHqYBAJDch+QrSH4W1f3lbwCwsN8Lp1YWWo7wDk9Z5PCbJSVHKLzcT0qOkJ4YlH01TeY+IyRfDGAxgAUAfgjgYgAjoYrkpVYWWo7wDk9Z5PCbJSUHAKxYvQ5btm7H+PhuzBsaxvKlS7Cwy0mSXu4nJUdIj2gkc2kvyW8D+ByAL5vZvb1eIGtprxBC7K20WtrbKa2W9oriyXtp79xD5gX7rv2v2y/PfWlvZs+Imb0ozyBCCCGE6I2y/6tftWmEEEIIUSiqTSOEEDkTYohFQz2inqJWwYRCjREhhBCi5JS9MaJhGiGEEEIUiqr2Ossih98scvjNkpIjhEeVf+M4QnpCU/bt4FW111EWOfxmkcNvlpQc3XhU+dfvZwPkv7T3uQe/INiX+Q/v+I6fqr0AQPJ0kseRDD63JLVKjHKEd3jKIoffLCk5QnlU+Te8I6RHNNJumOZQAB8EcBfJK0muJbmA5IH9Xji1SoxyhHd4yiKH3ywpOUJ6+sXLe+LFEdITg2S3gwcAMzsTAEjuC2AQwAkA3gjg4yTHzeyoXi+cWiVGOcI7PGWRw2+WlBwhPf3i5T3x4gjpiYGXHL3S6QTWAQAzAMysHXcA+EHWi0mOkBwjOVapNC9lk1olRjnCOzxlkcNvlpQcIT394uU98eII6RGNtJszMkry+wA+D+B4AP8N4BQzGzSzv846z8xGa68ZnDLlgKavSa0SoxzhHZ6yyOE3S0qOkJ5+8fKeeHGE9MSgAgt2FEG7ialPBrAfgJ8CuB3ALgDjIS6cWiVGOcI7PGWRw2+WlByhPKr8G94R0hODsg/TtF3ay+og2dGozhc5AcCzANwL4CozW93uAqraK4QQ4dF28L7Je2nvMQedGOy7duv/ft9P1d4JrNpauYbkOIBf146XA3gugLaNESGEEELEpezbwbdsjJB8O6q9IScCeBDA9wFcBeATAHZETyeEEEKIthS1JDcU7XpGZgH4EoB3mNmd8eMIIUT6aIhFiD1pt8/IGXkFEUIIIURvVEo+gTX4Nu9CCCGEyJeyD9MUWrVXCCGEEKLQxkhqZaHlCO/wlEUOv1lScqxauwFzFyzC0PCyns4PmUWOeJ7QVMyCHUXQdp+RfsnaZ6SsZaHlyM/hKYscfrOU0dFqAuvYth2YNjCAlWvWY/OmjZmvazWBtYzviXdHt5689xl5xh8dF+zL/Pq7tuS+z0hmzwjJw1r8ru9p3KmVhZYjvMNTFjn8ZknJAQCDc2Zj5ozpXZ8XOosc8TyikVbDNN8h+U6Sj0xyJflEkpsAbOj3wqmVhZYjvMNTFjn8ZknJEQov95OSI6QnBmUfpmnVGHkOgKcB2EryJJJ/C+CHqG569rxW0k6q9qZWFlqO8A5PWeTwmyUlRyi83E9KjpCeGFjA/xVB5tJeM/sVgLfUGiH/CeAOAM83s13tpGY2CmAUyJ4zklpZaDnCOzxlkcNvlpQcofByPyk5QnpEI63mjDyG5McA/DWA+ajuxPoNkieFuHBqZaHlCO/wlEUOv1lScoTCy/2k5AjpiUHZh2labXr2YwAfAfA2M3sIwLdIzgHwEZK3mNnifi6cWlloOcI7PGWRw2+WlBwAsGL1OmzZuh3j47sxb2gYy5cuwcIuJ0l6uZ+UHCE9MSj7pmeZS3tJHpo1JEPyzWb28U4ukDVMI4QQeyuqTZM+eS/tferjjwn2XXvT3VtzX9rbas5I5tyQThsiQgghhIiPWaXoCH2h2jRCCCFEyamUfJhGjREhhMiZEEMsGuoRKaHGiBBCCFFyvOx30itqjAghhBAlp+zDNIVW7RVCCCGEKLQxklpZaDnCOzxlkcNvlpQcITyr1m7A3AWLMDS8rOcMIXKk5gjpCY2ZBTuKoNU+I5cCWG5mN/dzgax9RspaFlqO/ByessjhN0tKjm48rSawjm3bgWkDA1i5Zj02b9qY+bpWE1i9vCdeHN168t5n5EmPOSpYK+LO8Wtz32ekVc/Ip1DddfXdJPcJfeHUykLLEd7hKYscfrOk5AjlGZwzGzNnTO/62qFzpOQI6RGNZDZGzOwLAI4BMAPAGMkzSZ4xcfR74dTKQssR3uEpixx+s6TkCOnpFy/viRdHSE8Mkq3aW+NBAPcD2A/AdAAdbfFGcgTACABw6kxMmXJAs9c0PFfmstByhHd4yiKH3ywpOUJ6+sXLe+LFEdITAy85eiWzMUJyPoANAC4BcKyZ/bZTqZmNAhgFsueMpFYWWo7wDk9Z5PCbJSVHSE+/eHlPvDhCemKQ8tLedwM4xcze1U1DpFNSKwstR3iHpyxy+M2SkiOkp1+8vCdeHCE9opFWhfKi7hOcWlloOcI7PGWRw2+WlByhPCtWr8OWrdsxPr4b84aGsXzpEizscqKll/fEiyOkJwZlH6bJXNobiqxhGiGEEL2j2jS+yXtp74HTjwj2XXvvfT91tbRXCCGEECI6qk0jhBBClJyyD9OoMSKEEDmjIRYRmpRX0wghhBBCREc9I0IIIUTJKfswjar2Ossih98scvjNkpJDFXf9OkJ6QlMxC3YUQWFLe8taiVGO/ByessjhN0sZHaq4Wz5Ht568l/Y+etpTgn2Z/+a3P/e1tJdk5i45JE/p58KpVWKUI7zDUxY5/GZJyQGo4q5XR0hPDMpeKK/dMM2lJL9N8pAmvzurnwunVolRjvAOT1nk8JslJUcovNxPSo6QnhiUfZimXWNkO4DPAbi6SU9IZjcOyRGSYyTHKpX7s17T8FyZKzHKEd7hKYscfrOk5AiFl/tJyRHSIxpp1xgxM/s4gHkA3knykySnTfyuxUmjZjZoZoNTphzQ9DWpVWKUI7zDUxY5/GZJyREKL/eTkiOkJwZmFuwogo5W05jZDQCOB/ALAFtJPq/fC6dWiVGO8A5PWeTwmyUlRyi83E9KjpCeGJR9zki7fUYe6ZMys4cAvIvkZQAuAvCEfi6cWiVGOcI7PGWRw2+WlByAKu56dYT0iEZaLu0lOWRmm5s8/1gAbzGzde0uoKq9QgixJ9oOPn3yXtq7736HBvuu/f0Du3wt7W3WEKk9/6tOGiJCCCGEiE+ec0ZIzif5E5I3knxXk9/vR/Lztd//gOSsdk7VphFCCCFER5CcCuDDAF4K4CgAi0keNellSwH8ysyeDuD9AP6pnVeNESGEEKLkWMCjDc8FcKOZ3WRmvwdwMYCTJ73mZAAX1n7+EoB5bLYueo8bCNi100eX0IgcYR2essjhN4scfrOk5PCUxYvD8wFgBMBY3TFS97tXA7ig7vESAOdPOv8aAIfWPf4ZgMe3uqaXnpEROYI7QnnkCO8I5ZEjvCOUR444npQcbrG6vcJqx2jdr5v1cEzuUOnkNXvgpTEihBBCCP/sAnBY3eNDAdyR9RqSjwIwE8C9raRqjAghhBCiU7YAOILkU0juC2ARgEsmveYSAH9V+/nVAK6w2nhNFu02PcuL0fYvkaMgjxzhHaE8coR3hPLIEceTkqOUmNlDJE8D8E0AUwF8wsx2kjwXwJiZXQLgXwF8huSNqPaILGrnbbnpmRBCCCFEbDRMI4QQQohCUWNECCGEEIVSaGOE5KtIGsln9OF4mOQ2kv9D8sckT+jBcRDJi0n+jOS1JC8leWQPGXbWcpxBsuv3ts4zcTRss9ujZ1aX5z+R5OdI3kTyRySvIvmqLh2/mfT4DSTP78bRype3o/5cki8j+VOST84zQ+18I/mZusePIvlLkv/epeO8usdnkjynhyyHkvxa7b34GckP1ia0deOY+G/1GpJfJDmtzxw3kTyf5H595Pg3ko/pNkfN8+7a3wPba76uKpyTfFzdn9v/JXl73eOO3luSs0heM+m5c0ie2UWOK0m+ZNJzp5P8SIfnv5/k6XWPv0nygrrH55E8o0PXYSR/TvLA2uPH1h4f3tndAKzyPZIvrXvuNawWfu3U8apJf69uI1mpd4reKbpnZDGA76GDyS0t+J2ZzTGzZwM4C8B7uzmZJAF8FcCVZvY0MzsKwEoAT+whw9EAXgzgZQBWd5Njkmfi6LX+z2TPzZ2eWHs/NgP4LzN7qpk9B9XP59AesyQFyXkAPgRgvpndWkCE+wE8i+RA7fGLAdzepeMBAH9J8vG9hqj9d/IVAJvN7AgARwJ4NIB/7FI18d/qswD8HsCyPnMcAWAAwPv6yHEvgLd1eT5IHg/g5QCONbM/AfDnAG7rxmFm90z8uQWwEcD76/4c/77bTH1wERr/Xl5Ue74T/hvACQBQ+4fZ4wEcXff7EwB8vxORmd0G4KMAJv4+XAdg1Mxu6TALais5lgHYQHJ/kgeg+t9qx5+zmX21/u9VAB8B8F1UJ3KKPimsMULy0QBORHUP+34aI/XMAPCrLs95EYAHzWzjxBNmts3MeiqraWZ3obohzmm1vyjLxkkAfj/p/bjFzD5UYCYXkPwzAB8HsMDMflZglG8AWFD7eTE6/4KY4CFUVwO8o48MJwH4PzP7JACY2cM13xt76d2o8V0ATw+U49Ta3zG9cBWAQ3o470kA7jazB2pZ7jazyfsvlIUvAXj5RA9TrXf1YFT/8dgJ30etMYJqI+QaAPfVejX2A/BMAFu7yPN+AM+v9bb8KYDz2ry+ATO7BsC/Afh7VP+x+Ole/xyz2nN+NoAlZlbpxSH2pMiekSEAl5nZDQDuJXlsj56BWnfZ9QAuALCmy/OfBeBHPV67KWZ2E6rv7R91eerEvUwcr+0xQr3nq12eezSAH/d43awM2wCcG8BZJPsB+BqAITO7vuAsFwNYRHJ/AH8C4Ac9OD4M4PUkZ/aY4WhM+nNjZrsB3IruGxQTGyO9FMCOQDlu7jHHVADz0LhvQid8C8BhJG8g+RGSL+jB4QIzuwfADwHMrz21CMDn2+0VUXf+HQAeqg1lnoBqA+8HAI4HMAhgezc9PWb2IIAVqDZKTu+jl+g9AF6H6n9r3faeAQBI7gPgcwDOLKh3NEmKbIwsRvUvVdT+f3GPnonu1Weg+gfn0056JHrJMHl45fM9Xrve09Vcj8mQ/DCr82C29JFhDqr/iigzD6La9by06CBmth3ALFT/zFzao2M3gE8DeHuPMYjm2ztnPZ/FQK2xOoZqQ+ZfA+bohokc9wA4EMB/dHk+zOw3AJ6Das/oLwF8nuQbuvUEIOv973Yfh/qhmm6GaCaY6B2ZaIxcVff4v7t0AdUGxJ2o/gOyJ8zsfgCfB/CZiR6sHlgDYKeZXdz2laJjCmmMkHwcqt2rF5C8GdUW72v7bUSY2VWojk0+oYvTdqL6F0gwSD4VwMMA7grpzYmdAB7ppTKzt6H6L8Vu3tMUqQB4DYDjSK4sOgyq/3Jfj+6/IOr5AKqNqwN6OHcnqv/CfQSSM1DdArqbru/6Ruvf9PAv3qwcTwTwk25zADgcwL7oYc4IUB0mMrMrzWw1gNMALOzF0yf3AHjspOcOBHB3l57NqFZbPRbAgJl122M6MW9kNqrDNFej2jPS8XyRCUjOQXV+1PMBvIPkk7rMUk+ldnQNyRei+pme1sf1RROK6hl5NarjdYeb2SwzOwzAz1EdC+wZVlflTEX1D2OnXAFgP5JvrvMc12sXK8knoDrx7PxOuzSdcQWA/Um+te65XucAJIWZ/VqoFvgAAAICSURBVBbVCYqvJ1l0D8knAJxrZt0OazyCmd0L4AvorbfncgDTSJ4KPDK8cR6AT9Xep7zIynG+mf2uW5mZ/RrV3qIza93xHUPyj0keUffUHAAdT7IMRa2H5s7aZGvUVqHMR+fzPeo9V6L631ovjd7vo/rn5d5aI+1eAI9BtUFyVaeS2j9SP4rq8MytAP4Z1YZ4rpB8LIBPAjjVzO7L+/qpU1RjZDGqK1jq+TKqY3nd8sjcBFS73/6qNomtI2oNhlcBeDGryxN3AjgHjYV/OsmwE8B/ojp2/J4uzp/smTh6XU3TM7X3YwjAC2rL534I4EJUJ32VltqchF67ZR+h9hfqfACrSJ7cg2IayV11R0fLG5vk2GVmH+zl3Emch2pvYrfXn/hzcwrJnwK4AcD/oboSLTfqcry6luMeABUz63ZVT71zK4D/QfcT6x8N4EJWtwfYDuAoVP8uKYJTUf1vdBuq/8B4T4+TNS8C8Gz8YUi9G3ag+t/W1ZOe+7WZddNL82YAt5rZxNDZRwA8o4A5OctQnQf40UBz+0Qd2g5e7BWQfDaAj5vZc4vOIuLB6j5DFwH4SzMLOjFdCBEPNUZE8pBchmrX++lm9q2i8wghhNgTNUaEEEIIUShF78AqhBBCiL0cNUaEEEIIUShqjAghhBCiUNQYEUIIIUShqDEihBBCiEL5//ze2n5Q4LwxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = confusion_matrix(y_train, np.rint(model.predict(np.array(x_train)/25)))\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preformance for the last 3 shift is super terrible, that is to say, the model doesn't learn the shift at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xihajun/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(26, input_dim=1, kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(26, input_dim=1, init='uniform'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softplus'))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.add(Dense(26))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "26/26 [==============================] - 5s 189ms/step - loss: 203.9504 - acc: 0.0000e+00\n",
      "Epoch 2/250\n",
      "26/26 [==============================] - 0s 154us/step - loss: 174.6086 - acc: 0.0000e+00\n",
      "Epoch 3/250\n",
      "26/26 [==============================] - 0s 178us/step - loss: 135.0521 - acc: 0.0385\n",
      "Epoch 4/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 107.0437 - acc: 0.0000e+00\n",
      "Epoch 5/250\n",
      "26/26 [==============================] - 0s 187us/step - loss: 93.5683 - acc: 0.0385\n",
      "Epoch 6/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 103.4830 - acc: 0.0769\n",
      "Epoch 7/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 78.2236 - acc: 0.0385\n",
      "Epoch 8/250\n",
      "26/26 [==============================] - 0s 192us/step - loss: 54.3740 - acc: 0.1154\n",
      "Epoch 9/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 69.9004 - acc: 0.0385\n",
      "Epoch 10/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 52.5465 - acc: 0.0769\n",
      "Epoch 11/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 46.9445 - acc: 0.0769\n",
      "Epoch 12/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 50.2133 - acc: 0.1154\n",
      "Epoch 13/250\n",
      "26/26 [==============================] - 0s 187us/step - loss: 46.2386 - acc: 0.0769\n",
      "Epoch 14/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 96.3920 - acc: 0.0385\n",
      "Epoch 15/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 58.9865 - acc: 0.1154\n",
      "Epoch 16/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 52.9635 - acc: 0.0385\n",
      "Epoch 17/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 66.8978 - acc: 0.0000e+00\n",
      "Epoch 18/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 63.6628 - acc: 0.0000e+00\n",
      "Epoch 19/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 62.6540 - acc: 0.1538\n",
      "Epoch 20/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 41.4746 - acc: 0.0385\n",
      "Epoch 21/250\n",
      "26/26 [==============================] - 0s 209us/step - loss: 31.7777 - acc: 0.0769\n",
      "Epoch 22/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 32.3830 - acc: 0.0000e+00\n",
      "Epoch 23/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 40.2686 - acc: 0.0769\n",
      "Epoch 24/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 39.4968 - acc: 0.0385\n",
      "Epoch 25/250\n",
      "26/26 [==============================] - 0s 192us/step - loss: 27.1723 - acc: 0.0769\n",
      "Epoch 26/250\n",
      "26/26 [==============================] - 0s 178us/step - loss: 36.5264 - acc: 0.0769\n",
      "Epoch 27/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 29.6880 - acc: 0.0385\n",
      "Epoch 28/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 28.2678 - acc: 0.0769\n",
      "Epoch 29/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 39.1977 - acc: 0.1154\n",
      "Epoch 30/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 23.1287 - acc: 0.0385\n",
      "Epoch 31/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 26.1647 - acc: 0.1154\n",
      "Epoch 32/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 17.7556 - acc: 0.1538\n",
      "Epoch 33/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 24.4501 - acc: 0.0769\n",
      "Epoch 34/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 18.2456 - acc: 0.0769\n",
      "Epoch 35/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 20.6559 - acc: 0.0769\n",
      "Epoch 36/250\n",
      "26/26 [==============================] - 0s 192us/step - loss: 13.6991 - acc: 0.1538\n",
      "Epoch 37/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 19.2821 - acc: 0.1923\n",
      "Epoch 38/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 21.6525 - acc: 0.0385\n",
      "Epoch 39/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 40.5364 - acc: 0.1538\n",
      "Epoch 40/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 32.0885 - acc: 0.0769\n",
      "Epoch 41/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 24.4900 - acc: 0.1538\n",
      "Epoch 42/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 27.9245 - acc: 0.2692\n",
      "Epoch 43/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 12.3134 - acc: 0.2692\n",
      "Epoch 44/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 12.4365 - acc: 0.2308\n",
      "Epoch 45/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 11.9269 - acc: 0.1154\n",
      "Epoch 46/250\n",
      "26/26 [==============================] - 0s 262us/step - loss: 10.6724 - acc: 0.1538\n",
      "Epoch 47/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 13.8907 - acc: 0.1538\n",
      "Epoch 48/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 20.1287 - acc: 0.1154\n",
      "Epoch 49/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 15.6264 - acc: 0.1154\n",
      "Epoch 50/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 14.3730 - acc: 0.3462\n",
      "Epoch 51/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 10.2041 - acc: 0.1923\n",
      "Epoch 52/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 9.6118 - acc: 0.2308\n",
      "Epoch 53/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 9.8271 - acc: 0.4231\n",
      "Epoch 54/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 14.1786 - acc: 0.2308\n",
      "Epoch 55/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 9.5789 - acc: 0.1923\n",
      "Epoch 56/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 12.6673 - acc: 0.1923\n",
      "Epoch 57/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 6.1994 - acc: 0.2308\n",
      "Epoch 58/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 16.3574 - acc: 0.2308\n",
      "Epoch 59/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 11.8842 - acc: 0.2308\n",
      "Epoch 60/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 8.0534 - acc: 0.3077\n",
      "Epoch 61/250\n",
      "26/26 [==============================] - 0s 192us/step - loss: 7.4202 - acc: 0.4615\n",
      "Epoch 62/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 17.1411 - acc: 0.2692\n",
      "Epoch 63/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 13.7875 - acc: 0.3846\n",
      "Epoch 64/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 16.2078 - acc: 0.1538\n",
      "Epoch 65/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 15.2966 - acc: 0.1538\n",
      "Epoch 66/250\n",
      "26/26 [==============================] - 0s 214us/step - loss: 7.4379 - acc: 0.2692\n",
      "Epoch 67/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 9.0713 - acc: 0.4615\n",
      "Epoch 68/250\n",
      "26/26 [==============================] - 0s 217us/step - loss: 12.6987 - acc: 0.1154\n",
      "Epoch 69/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 10.2749 - acc: 0.3462\n",
      "Epoch 70/250\n",
      "26/26 [==============================] - 0s 207us/step - loss: 10.5092 - acc: 0.3462\n",
      "Epoch 71/250\n",
      "26/26 [==============================] - 0s 222us/step - loss: 7.2405 - acc: 0.4615\n",
      "Epoch 72/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 18.5937 - acc: 0.2692\n",
      "Epoch 73/250\n",
      "26/26 [==============================] - 0s 221us/step - loss: 10.2081 - acc: 0.2308\n",
      "Epoch 74/250\n",
      "26/26 [==============================] - 0s 211us/step - loss: 11.6877 - acc: 0.3077\n",
      "Epoch 75/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 14.5823 - acc: 0.2308\n",
      "Epoch 76/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 12.7065 - acc: 0.3077\n",
      "Epoch 77/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 11.7995 - acc: 0.3462\n",
      "Epoch 78/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 9.3046 - acc: 0.3462\n",
      "Epoch 79/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 10.5783 - acc: 0.3077\n",
      "Epoch 80/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 4.1460 - acc: 0.2308\n",
      "Epoch 81/250\n",
      "26/26 [==============================] - 0s 210us/step - loss: 9.5351 - acc: 0.3077\n",
      "Epoch 82/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 10.9310 - acc: 0.1923\n",
      "Epoch 83/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 10.7716 - acc: 0.2692\n",
      "Epoch 84/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 9.3663 - acc: 0.2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 8.6183 - acc: 0.3077\n",
      "Epoch 86/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 5.1042 - acc: 0.2308\n",
      "Epoch 87/250\n",
      "26/26 [==============================] - 0s 184us/step - loss: 8.5768 - acc: 0.3846\n",
      "Epoch 88/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 11.3664 - acc: 0.0769\n",
      "Epoch 89/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 8.9674 - acc: 0.1923\n",
      "Epoch 90/250\n",
      "26/26 [==============================] - 0s 166us/step - loss: 34.7879 - acc: 0.1923\n",
      "Epoch 91/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 7.8544 - acc: 0.1923\n",
      "Epoch 92/250\n",
      "26/26 [==============================] - 0s 182us/step - loss: 10.6983 - acc: 0.0385\n",
      "Epoch 93/250\n",
      "26/26 [==============================] - 0s 162us/step - loss: 5.8522 - acc: 0.2308\n",
      "Epoch 94/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 4.7872 - acc: 0.5000\n",
      "Epoch 95/250\n",
      "26/26 [==============================] - 0s 186us/step - loss: 16.1676 - acc: 0.4615\n",
      "Epoch 96/250\n",
      "26/26 [==============================] - 0s 172us/step - loss: 11.2452 - acc: 0.3846\n",
      "Epoch 97/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 12.8803 - acc: 0.1538\n",
      "Epoch 98/250\n",
      "26/26 [==============================] - 0s 186us/step - loss: 9.9397 - acc: 0.1923\n",
      "Epoch 99/250\n",
      "26/26 [==============================] - 0s 172us/step - loss: 14.4754 - acc: 0.2308\n",
      "Epoch 100/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 14.4084 - acc: 0.2308\n",
      "Epoch 101/250\n",
      "26/26 [==============================] - 0s 186us/step - loss: 8.6097 - acc: 0.4231\n",
      "Epoch 102/250\n",
      "26/26 [==============================] - 0s 167us/step - loss: 7.7096 - acc: 0.3462\n",
      "Epoch 103/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 5.3932 - acc: 0.3077\n",
      "Epoch 104/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 21.0562 - acc: 0.1154\n",
      "Epoch 105/250\n",
      "26/26 [==============================] - 0s 166us/step - loss: 15.6071 - acc: 0.3462\n",
      "Epoch 106/250\n",
      "26/26 [==============================] - 0s 183us/step - loss: 13.3065 - acc: 0.1923\n",
      "Epoch 107/250\n",
      "26/26 [==============================] - 0s 248us/step - loss: 22.1272 - acc: 0.3077\n",
      "Epoch 108/250\n",
      "26/26 [==============================] - 0s 180us/step - loss: 14.1086 - acc: 0.1923\n",
      "Epoch 109/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 14.9763 - acc: 0.1154\n",
      "Epoch 110/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 21.4149 - acc: 0.1154\n",
      "Epoch 111/250\n",
      "26/26 [==============================] - 0s 163us/step - loss: 18.2464 - acc: 0.2308\n",
      "Epoch 112/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 18.4296 - acc: 0.3462\n",
      "Epoch 113/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 11.7270 - acc: 0.4231\n",
      "Epoch 114/250\n",
      "26/26 [==============================] - 0s 167us/step - loss: 12.7663 - acc: 0.3462\n",
      "Epoch 115/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 10.3696 - acc: 0.3846\n",
      "Epoch 116/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 13.5582 - acc: 0.1154\n",
      "Epoch 117/250\n",
      "26/26 [==============================] - 0s 170us/step - loss: 12.1475 - acc: 0.0769\n",
      "Epoch 118/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 11.2632 - acc: 0.1538\n",
      "Epoch 119/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 13.3269 - acc: 0.4231\n",
      "Epoch 120/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 15.4800 - acc: 0.3462\n",
      "Epoch 121/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 10.0107 - acc: 0.3077\n",
      "Epoch 122/250\n",
      "26/26 [==============================] - 0s 180us/step - loss: 16.6124 - acc: 0.1538\n",
      "Epoch 123/250\n",
      "26/26 [==============================] - 0s 167us/step - loss: 5.7248 - acc: 0.1923\n",
      "Epoch 124/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 8.9395 - acc: 0.1154\n",
      "Epoch 125/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 10.2329 - acc: 0.1538\n",
      "Epoch 126/250\n",
      "26/26 [==============================] - 0s 166us/step - loss: 12.3409 - acc: 0.0769\n",
      "Epoch 127/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 33.7604 - acc: 0.1923\n",
      "Epoch 128/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 9.0823 - acc: 0.5769\n",
      "Epoch 129/250\n",
      "26/26 [==============================] - 0s 178us/step - loss: 11.6954 - acc: 0.3462\n",
      "Epoch 130/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 10.1534 - acc: 0.5000\n",
      "Epoch 131/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 8.5250 - acc: 0.3846\n",
      "Epoch 132/250\n",
      "26/26 [==============================] - 0s 176us/step - loss: 13.7746 - acc: 0.2308\n",
      "Epoch 133/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 16.4553 - acc: 0.0769\n",
      "Epoch 134/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 11.7203 - acc: 0.2692\n",
      "Epoch 135/250\n",
      "26/26 [==============================] - 0s 172us/step - loss: 15.0531 - acc: 0.2692\n",
      "Epoch 136/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 7.4712 - acc: 0.6154\n",
      "Epoch 137/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 7.2543 - acc: 0.3462\n",
      "Epoch 138/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 18.3473 - acc: 0.2692\n",
      "Epoch 139/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 8.4049 - acc: 0.2308\n",
      "Epoch 140/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 20.2974 - acc: 0.0769\n",
      "Epoch 141/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 14.2234 - acc: 0.3846\n",
      "Epoch 142/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 15.3888 - acc: 0.2692\n",
      "Epoch 143/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 17.2013 - acc: 0.5769\n",
      "Epoch 144/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 9.1610 - acc: 0.4231\n",
      "Epoch 145/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 48.4350 - acc: 0.1538\n",
      "Epoch 146/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 22.7973 - acc: 0.1154\n",
      "Epoch 147/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 26.3011 - acc: 0.1154\n",
      "Epoch 148/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 13.4927 - acc: 0.2308\n",
      "Epoch 149/250\n",
      "26/26 [==============================] - 0s 179us/step - loss: 7.0961 - acc: 0.1923\n",
      "Epoch 150/250\n",
      "26/26 [==============================] - 0s 181us/step - loss: 21.6554 - acc: 0.2308\n",
      "Epoch 151/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 11.8397 - acc: 0.3077\n",
      "Epoch 152/250\n",
      "26/26 [==============================] - 0s 179us/step - loss: 9.7706 - acc: 0.2308\n",
      "Epoch 153/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 9.1800 - acc: 0.4615\n",
      "Epoch 154/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 8.2543 - acc: 0.5000\n",
      "Epoch 155/250\n",
      "26/26 [==============================] - 0s 175us/step - loss: 23.8822 - acc: 0.1538\n",
      "Epoch 156/250\n",
      "26/26 [==============================] - 0s 182us/step - loss: 16.8349 - acc: 0.3077\n",
      "Epoch 157/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 23.9467 - acc: 0.3077\n",
      "Epoch 158/250\n",
      "26/26 [==============================] - 0s 186us/step - loss: 16.2037 - acc: 0.1538\n",
      "Epoch 159/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 15.9367 - acc: 0.1538\n",
      "Epoch 160/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 19.2751 - acc: 0.2308\n",
      "Epoch 161/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 10.3893 - acc: 0.2692\n",
      "Epoch 162/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 10.2950 - acc: 0.0769\n",
      "Epoch 163/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 18.9626 - acc: 0.1538\n",
      "Epoch 164/250\n",
      "26/26 [==============================] - 0s 184us/step - loss: 24.5846 - acc: 0.1923\n",
      "Epoch 165/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 8.8174 - acc: 0.4615\n",
      "Epoch 166/250\n",
      "26/26 [==============================] - 0s 194us/step - loss: 9.2807 - acc: 0.1923\n",
      "Epoch 167/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 8.2926 - acc: 0.3462\n",
      "Epoch 168/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 7.9535 - acc: 0.3462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 10.3431 - acc: 0.2308\n",
      "Epoch 170/250\n",
      "26/26 [==============================] - 0s 187us/step - loss: 8.4199 - acc: 0.2692\n",
      "Epoch 171/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 9.6830 - acc: 0.3077\n",
      "Epoch 172/250\n",
      "26/26 [==============================] - 0s 180us/step - loss: 15.9832 - acc: 0.3846\n",
      "Epoch 173/250\n",
      "26/26 [==============================] - 0s 169us/step - loss: 25.8259 - acc: 0.3077\n",
      "Epoch 174/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 10.3679 - acc: 0.3462\n",
      "Epoch 175/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 8.5171 - acc: 0.2308\n",
      "Epoch 176/250\n",
      "26/26 [==============================] - 0s 168us/step - loss: 8.9891 - acc: 0.3462\n",
      "Epoch 177/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 20.5634 - acc: 0.3077\n",
      "Epoch 178/250\n",
      "26/26 [==============================] - 0s 182us/step - loss: 10.8380 - acc: 0.2308\n",
      "Epoch 179/250\n",
      "26/26 [==============================] - 0s 173us/step - loss: 17.0942 - acc: 0.3462\n",
      "Epoch 180/250\n",
      "26/26 [==============================] - 0s 192us/step - loss: 9.0200 - acc: 0.3846\n",
      "Epoch 181/250\n",
      "26/26 [==============================] - 0s 181us/step - loss: 10.2818 - acc: 0.2308\n",
      "Epoch 182/250\n",
      "26/26 [==============================] - 0s 183us/step - loss: 6.4671 - acc: 0.1154\n",
      "Epoch 183/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 8.4297 - acc: 0.1538\n",
      "Epoch 184/250\n",
      "26/26 [==============================] - 0s 183us/step - loss: 12.7500 - acc: 0.2692\n",
      "Epoch 185/250\n",
      "26/26 [==============================] - 0s 173us/step - loss: 23.3649 - acc: 0.0769\n",
      "Epoch 186/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 16.2246 - acc: 0.1538\n",
      "Epoch 187/250\n",
      "26/26 [==============================] - 0s 182us/step - loss: 40.6199 - acc: 0.0000e+00\n",
      "Epoch 188/250\n",
      "26/26 [==============================] - 0s 185us/step - loss: 25.8896 - acc: 0.1538\n",
      "Epoch 189/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 37.8268 - acc: 0.1538\n",
      "Epoch 190/250\n",
      "26/26 [==============================] - 0s 177us/step - loss: 15.3965 - acc: 0.2308\n",
      "Epoch 191/250\n",
      "26/26 [==============================] - 0s 163us/step - loss: 14.0572 - acc: 0.1923\n",
      "Epoch 192/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 15.5218 - acc: 0.2692\n",
      "Epoch 193/250\n",
      "26/26 [==============================] - 0s 177us/step - loss: 22.3234 - acc: 0.1923\n",
      "Epoch 194/250\n",
      "26/26 [==============================] - 0s 184us/step - loss: 15.1143 - acc: 0.2692\n",
      "Epoch 195/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 14.2974 - acc: 0.3846\n",
      "Epoch 196/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 31.1357 - acc: 0.3077\n",
      "Epoch 197/250\n",
      "26/26 [==============================] - 0s 181us/step - loss: 23.2503 - acc: 0.1538\n",
      "Epoch 198/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 17.6377 - acc: 0.1538\n",
      "Epoch 199/250\n",
      "26/26 [==============================] - 0s 179us/step - loss: 12.8434 - acc: 0.3846\n",
      "Epoch 200/250\n",
      "26/26 [==============================] - 0s 168us/step - loss: 9.8629 - acc: 0.2692\n",
      "Epoch 201/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 8.9524 - acc: 0.3462\n",
      "Epoch 202/250\n",
      "26/26 [==============================] - 0s 183us/step - loss: 11.5337 - acc: 0.4231\n",
      "Epoch 203/250\n",
      "26/26 [==============================] - 0s 173us/step - loss: 5.3315 - acc: 0.3077\n",
      "Epoch 204/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 7.9541 - acc: 0.4615\n",
      "Epoch 205/250\n",
      "26/26 [==============================] - 0s 188us/step - loss: 11.8950 - acc: 0.1923\n",
      "Epoch 206/250\n",
      "26/26 [==============================] - 0s 176us/step - loss: 7.5967 - acc: 0.5769\n",
      "Epoch 207/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 3.4481 - acc: 0.3846\n",
      "Epoch 208/250\n",
      "26/26 [==============================] - 0s 174us/step - loss: 7.7346 - acc: 0.3462\n",
      "Epoch 209/250\n",
      "26/26 [==============================] - 0s 164us/step - loss: 8.4477 - acc: 0.1538\n",
      "Epoch 210/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 19.6863 - acc: 0.1154\n",
      "Epoch 211/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 13.3574 - acc: 0.3077\n",
      "Epoch 212/250\n",
      "26/26 [==============================] - 0s 173us/step - loss: 9.5450 - acc: 0.2692\n",
      "Epoch 213/250\n",
      "26/26 [==============================] - 0s 195us/step - loss: 10.0368 - acc: 0.2692\n",
      "Epoch 214/250\n",
      "26/26 [==============================] - 0s 193us/step - loss: 9.0272 - acc: 0.1923\n",
      "Epoch 215/250\n",
      "26/26 [==============================] - 0s 166us/step - loss: 3.8795 - acc: 0.2308\n",
      "Epoch 216/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 8.5840 - acc: 0.3846\n",
      "Epoch 217/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 3.2890 - acc: 0.5385\n",
      "Epoch 218/250\n",
      "26/26 [==============================] - 0s 191us/step - loss: 8.1311 - acc: 0.2308\n",
      "Epoch 219/250\n",
      "26/26 [==============================] - 0s 197us/step - loss: 6.9415 - acc: 0.4231\n",
      "Epoch 220/250\n",
      "26/26 [==============================] - 0s 189us/step - loss: 5.1547 - acc: 0.4615\n",
      "Epoch 221/250\n",
      "26/26 [==============================] - 0s 199us/step - loss: 17.0830 - acc: 0.1923\n",
      "Epoch 222/250\n",
      "26/26 [==============================] - 0s 196us/step - loss: 10.7477 - acc: 0.2692\n",
      "Epoch 223/250\n",
      "26/26 [==============================] - 0s 206us/step - loss: 39.4071 - acc: 0.1923\n",
      "Epoch 224/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 31.2073 - acc: 0.2308\n",
      "Epoch 225/250\n",
      "26/26 [==============================] - 0s 227us/step - loss: 9.5411 - acc: 0.3077\n",
      "Epoch 226/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 8.5875 - acc: 0.4231\n",
      "Epoch 227/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 26.8493 - acc: 0.3077\n",
      "Epoch 228/250\n",
      "26/26 [==============================] - 0s 202us/step - loss: 13.6462 - acc: 0.4615\n",
      "Epoch 229/250\n",
      "26/26 [==============================] - 0s 198us/step - loss: 6.0266 - acc: 0.2692\n",
      "Epoch 230/250\n",
      "26/26 [==============================] - 0s 204us/step - loss: 26.8241 - acc: 0.3462\n",
      "Epoch 231/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 13.9236 - acc: 0.3846\n",
      "Epoch 232/250\n",
      "26/26 [==============================] - 0s 212us/step - loss: 10.2385 - acc: 0.3077\n",
      "Epoch 233/250\n",
      "26/26 [==============================] - 0s 218us/step - loss: 7.3076 - acc: 0.3846\n",
      "Epoch 234/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 7.3122 - acc: 0.6154\n",
      "Epoch 235/250\n",
      "26/26 [==============================] - 0s 203us/step - loss: 10.3505 - acc: 0.1154\n",
      "Epoch 236/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 23.2582 - acc: 0.1923\n",
      "Epoch 237/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 21.1257 - acc: 0.2308\n",
      "Epoch 238/250\n",
      "26/26 [==============================] - 0s 217us/step - loss: 4.2255 - acc: 0.4231\n",
      "Epoch 239/250\n",
      "26/26 [==============================] - 0s 208us/step - loss: 7.0864 - acc: 0.5000\n",
      "Epoch 240/250\n",
      "26/26 [==============================] - 0s 216us/step - loss: 6.8825 - acc: 0.3462\n",
      "Epoch 241/250\n",
      "26/26 [==============================] - 0s 213us/step - loss: 6.8020 - acc: 0.3846\n",
      "Epoch 242/250\n",
      "26/26 [==============================] - 0s 214us/step - loss: 9.9028 - acc: 0.4231\n",
      "Epoch 243/250\n",
      "26/26 [==============================] - 0s 255us/step - loss: 29.2772 - acc: 0.1154\n",
      "Epoch 244/250\n",
      "26/26 [==============================] - 0s 226us/step - loss: 8.2812 - acc: 0.3077\n",
      "Epoch 245/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 5.8138 - acc: 0.3846\n",
      "Epoch 246/250\n",
      "26/26 [==============================] - 0s 200us/step - loss: 5.5590 - acc: 0.5385\n",
      "Epoch 247/250\n",
      "26/26 [==============================] - 0s 190us/step - loss: 9.0273 - acc: 0.4615\n",
      "Epoch 248/250\n",
      "26/26 [==============================] - 0s 201us/step - loss: 22.3428 - acc: 0.3077\n",
      "Epoch 249/250\n",
      "26/26 [==============================] - 0s 215us/step - loss: 20.0587 - acc: 0.3077\n",
      "Epoch 250/250\n",
      "26/26 [==============================] - 0s 205us/step - loss: 8.1324 - acc: 0.5385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c6337dfd0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(x_train), np.array(y_train), epochs = 250)#, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not amazing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def mean_pred(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning structure 2: One input vs 26 output (as a classification problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, x_train, y_train = caeserde(\"ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ\", x_as_vector=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 0, 1, 2] y_train: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train:\",x_train,\"y_train:\",y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(26, input_dim=1, activation='relu'))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(26, activation='relu'))\n",
    "\n",
    "# change sigomiod to softmax made acc increasing 100%\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "52/52 [==============================] - 2s 34ms/step - loss: 0.6973 - acc: 0.6206\n",
      "Epoch 2/300\n",
      "52/52 [==============================] - 0s 102us/step - loss: 0.6754 - acc: 0.6923\n",
      "Epoch 3/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.6636 - acc: 0.6842\n",
      "Epoch 4/300\n",
      "52/52 [==============================] - 0s 109us/step - loss: 0.6524 - acc: 0.6746\n",
      "Epoch 5/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.6414 - acc: 0.6583\n",
      "Epoch 6/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.6307 - acc: 0.6908\n",
      "Epoch 7/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.6192 - acc: 0.7034\n",
      "Epoch 8/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.6058 - acc: 0.7293\n",
      "Epoch 9/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.5889 - acc: 0.7729\n",
      "Epoch 10/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.5621 - acc: 0.7714\n",
      "Epoch 11/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.5303 - acc: 0.7707\n",
      "Epoch 12/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.4991 - acc: 0.7803\n",
      "Epoch 13/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.4697 - acc: 0.7929\n",
      "Epoch 14/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.4394 - acc: 0.8277\n",
      "Epoch 15/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.4105 - acc: 0.8632\n",
      "Epoch 16/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.3830 - acc: 0.9231\n",
      "Epoch 17/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.3572 - acc: 0.9615\n",
      "Epoch 18/300\n",
      "52/52 [==============================] - 0s 112us/step - loss: 0.3354 - acc: 0.9615\n",
      "Epoch 19/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.3158 - acc: 0.9615\n",
      "Epoch 20/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.3017 - acc: 0.9615\n",
      "Epoch 21/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.2909 - acc: 0.9615\n",
      "Epoch 22/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.2840 - acc: 0.9615\n",
      "Epoch 23/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.2780 - acc: 0.9615\n",
      "Epoch 24/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.2730 - acc: 0.9615\n",
      "Epoch 25/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.2690 - acc: 0.9615\n",
      "Epoch 26/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.2640 - acc: 0.9615\n",
      "Epoch 27/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.2590 - acc: 0.9615\n",
      "Epoch 28/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.2545 - acc: 0.9615\n",
      "Epoch 29/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.2497 - acc: 0.9615\n",
      "Epoch 30/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.2452 - acc: 0.9615\n",
      "Epoch 31/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.2414 - acc: 0.9615\n",
      "Epoch 32/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.2378 - acc: 0.9615\n",
      "Epoch 33/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.2337 - acc: 0.9615\n",
      "Epoch 34/300\n",
      "52/52 [==============================] - 0s 134us/step - loss: 0.2303 - acc: 0.9615\n",
      "Epoch 35/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.2267 - acc: 0.9615\n",
      "Epoch 36/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.2233 - acc: 0.9615\n",
      "Epoch 37/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.2199 - acc: 0.9615\n",
      "Epoch 38/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.2166 - acc: 0.9615\n",
      "Epoch 39/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.2131 - acc: 0.9615\n",
      "Epoch 40/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.2100 - acc: 0.9615\n",
      "Epoch 41/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.2064 - acc: 0.9615\n",
      "Epoch 42/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.2025 - acc: 0.9615\n",
      "Epoch 43/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.1988 - acc: 0.9615\n",
      "Epoch 44/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.1955 - acc: 0.9615\n",
      "Epoch 45/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.1919 - acc: 0.9615\n",
      "Epoch 46/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1884 - acc: 0.9615\n",
      "Epoch 47/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1851 - acc: 0.9615\n",
      "Epoch 48/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1818 - acc: 0.9615\n",
      "Epoch 49/300\n",
      "52/52 [==============================] - 0s 133us/step - loss: 0.1780 - acc: 0.9615\n",
      "Epoch 50/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1752 - acc: 0.9615\n",
      "Epoch 51/300\n",
      "52/52 [==============================] - 0s 110us/step - loss: 0.1722 - acc: 0.9615\n",
      "Epoch 52/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1700 - acc: 0.9615\n",
      "Epoch 53/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1671 - acc: 0.9615\n",
      "Epoch 54/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1651 - acc: 0.9615\n",
      "Epoch 55/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1629 - acc: 0.9615\n",
      "Epoch 56/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1614 - acc: 0.9615\n",
      "Epoch 57/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1599 - acc: 0.9615\n",
      "Epoch 58/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1587 - acc: 0.9615\n",
      "Epoch 59/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1579 - acc: 0.9615\n",
      "Epoch 60/300\n",
      "52/52 [==============================] - 0s 135us/step - loss: 0.1567 - acc: 0.9615\n",
      "Epoch 61/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.1559 - acc: 0.9615\n",
      "Epoch 62/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1549 - acc: 0.9615\n",
      "Epoch 63/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1542 - acc: 0.9615\n",
      "Epoch 64/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.1538 - acc: 0.9615\n",
      "Epoch 65/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1532 - acc: 0.9615\n",
      "Epoch 66/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1527 - acc: 0.9615\n",
      "Epoch 67/300\n",
      "52/52 [==============================] - 0s 113us/step - loss: 0.1519 - acc: 0.9615\n",
      "Epoch 68/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1512 - acc: 0.9615\n",
      "Epoch 69/300\n",
      "52/52 [==============================] - 0s 112us/step - loss: 0.1501 - acc: 0.9615\n",
      "Epoch 70/300\n",
      "52/52 [==============================] - 0s 112us/step - loss: 0.1499 - acc: 0.9615\n",
      "Epoch 71/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1493 - acc: 0.9615\n",
      "Epoch 72/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.1483 - acc: 0.9615\n",
      "Epoch 73/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.1479 - acc: 0.9615\n",
      "Epoch 74/300\n",
      "52/52 [==============================] - 0s 131us/step - loss: 0.1473 - acc: 0.9615\n",
      "Epoch 75/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.1469 - acc: 0.9615\n",
      "Epoch 76/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1463 - acc: 0.9615\n",
      "Epoch 77/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.1460 - acc: 0.9615\n",
      "Epoch 78/300\n",
      "52/52 [==============================] - 0s 132us/step - loss: 0.1459 - acc: 0.9615\n",
      "Epoch 79/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1458 - acc: 0.9615\n",
      "Epoch 80/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.1454 - acc: 0.9615\n",
      "Epoch 81/300\n",
      "52/52 [==============================] - 0s 132us/step - loss: 0.1444 - acc: 0.9615\n",
      "Epoch 82/300\n",
      "52/52 [==============================] - 0s 129us/step - loss: 0.1441 - acc: 0.9615\n",
      "Epoch 83/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1432 - acc: 0.9615\n",
      "Epoch 84/300\n",
      "52/52 [==============================] - 0s 113us/step - loss: 0.1430 - acc: 0.9615\n",
      "Epoch 85/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 146us/step - loss: 0.1424 - acc: 0.9615\n",
      "Epoch 86/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1417 - acc: 0.9615\n",
      "Epoch 87/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1416 - acc: 0.9615\n",
      "Epoch 88/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1407 - acc: 0.9615\n",
      "Epoch 89/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1405 - acc: 0.9615\n",
      "Epoch 90/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1399 - acc: 0.9615\n",
      "Epoch 91/300\n",
      "52/52 [==============================] - 0s 105us/step - loss: 0.1393 - acc: 0.9615\n",
      "Epoch 92/300\n",
      "52/52 [==============================] - 0s 107us/step - loss: 0.1391 - acc: 0.9615\n",
      "Epoch 93/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1385 - acc: 0.9615\n",
      "Epoch 94/300\n",
      "52/52 [==============================] - 0s 113us/step - loss: 0.1382 - acc: 0.9615\n",
      "Epoch 95/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.1377 - acc: 0.9615\n",
      "Epoch 96/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1372 - acc: 0.9615\n",
      "Epoch 97/300\n",
      "52/52 [==============================] - 0s 138us/step - loss: 0.1364 - acc: 0.9615\n",
      "Epoch 98/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1361 - acc: 0.9615\n",
      "Epoch 99/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1357 - acc: 0.9615\n",
      "Epoch 100/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.1351 - acc: 0.9615\n",
      "Epoch 101/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1347 - acc: 0.9615\n",
      "Epoch 102/300\n",
      "52/52 [==============================] - 0s 142us/step - loss: 0.1340 - acc: 0.9615\n",
      "Epoch 103/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1337 - acc: 0.9615\n",
      "Epoch 104/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1336 - acc: 0.9615\n",
      "Epoch 105/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1331 - acc: 0.9615\n",
      "Epoch 106/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1323 - acc: 0.9615\n",
      "Epoch 107/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1321 - acc: 0.9615\n",
      "Epoch 108/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1314 - acc: 0.9615\n",
      "Epoch 109/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1311 - acc: 0.9615\n",
      "Epoch 110/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1306 - acc: 0.9615\n",
      "Epoch 111/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1304 - acc: 0.9615\n",
      "Epoch 112/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1297 - acc: 0.9615\n",
      "Epoch 113/300\n",
      "52/52 [==============================] - 0s 134us/step - loss: 0.1291 - acc: 0.9615\n",
      "Epoch 114/300\n",
      "52/52 [==============================] - 0s 130us/step - loss: 0.1289 - acc: 0.9615\n",
      "Epoch 115/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1284 - acc: 0.9615\n",
      "Epoch 116/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1279 - acc: 0.9615\n",
      "Epoch 117/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1274 - acc: 0.9615\n",
      "Epoch 118/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1271 - acc: 0.9615\n",
      "Epoch 119/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.1271 - acc: 0.9615\n",
      "Epoch 120/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1269 - acc: 0.9615\n",
      "Epoch 121/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1259 - acc: 0.9615\n",
      "Epoch 122/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1253 - acc: 0.9615\n",
      "Epoch 123/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1252 - acc: 0.9615\n",
      "Epoch 124/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1250 - acc: 0.9615\n",
      "Epoch 125/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1246 - acc: 0.9615\n",
      "Epoch 126/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.1245 - acc: 0.9615\n",
      "Epoch 127/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.1236 - acc: 0.9615\n",
      "Epoch 128/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.1233 - acc: 0.9615\n",
      "Epoch 129/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1226 - acc: 0.9615\n",
      "Epoch 130/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1225 - acc: 0.9615\n",
      "Epoch 131/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.1222 - acc: 0.9615\n",
      "Epoch 132/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1213 - acc: 0.9615\n",
      "Epoch 133/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.1207 - acc: 0.9615\n",
      "Epoch 134/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1207 - acc: 0.9615\n",
      "Epoch 135/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1210 - acc: 0.9615\n",
      "Epoch 136/300\n",
      "52/52 [==============================] - 0s 139us/step - loss: 0.1202 - acc: 0.9615\n",
      "Epoch 137/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1193 - acc: 0.9615\n",
      "Epoch 138/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.1191 - acc: 0.9615\n",
      "Epoch 139/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.1186 - acc: 0.9615\n",
      "Epoch 140/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1182 - acc: 0.9615\n",
      "Epoch 141/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1173 - acc: 0.9615\n",
      "Epoch 142/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1169 - acc: 0.9615\n",
      "Epoch 143/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1167 - acc: 0.9615\n",
      "Epoch 144/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1169 - acc: 0.9615\n",
      "Epoch 145/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1160 - acc: 0.9615\n",
      "Epoch 146/300\n",
      "52/52 [==============================] - 0s 138us/step - loss: 0.1151 - acc: 0.9615\n",
      "Epoch 147/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1145 - acc: 0.9615\n",
      "Epoch 148/300\n",
      "52/52 [==============================] - 0s 131us/step - loss: 0.1143 - acc: 0.9615\n",
      "Epoch 149/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1140 - acc: 0.9623\n",
      "Epoch 150/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.1140 - acc: 0.9630\n",
      "Epoch 151/300\n",
      "52/52 [==============================] - 0s 133us/step - loss: 0.1150 - acc: 0.9630\n",
      "Epoch 152/300\n",
      "52/52 [==============================] - 0s 130us/step - loss: 0.1129 - acc: 0.9630\n",
      "Epoch 153/300\n",
      "52/52 [==============================] - 0s 131us/step - loss: 0.1128 - acc: 0.9630\n",
      "Epoch 154/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1118 - acc: 0.9630\n",
      "Epoch 155/300\n",
      "52/52 [==============================] - 0s 134us/step - loss: 0.1119 - acc: 0.9630\n",
      "Epoch 156/300\n",
      "52/52 [==============================] - 0s 111us/step - loss: 0.1114 - acc: 0.9630\n",
      "Epoch 157/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.1111 - acc: 0.9630\n",
      "Epoch 158/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.1112 - acc: 0.9630\n",
      "Epoch 159/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.1109 - acc: 0.9630\n",
      "Epoch 160/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1094 - acc: 0.9630\n",
      "Epoch 161/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.1096 - acc: 0.9630\n",
      "Epoch 162/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1091 - acc: 0.9630\n",
      "Epoch 163/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1091 - acc: 0.9630\n",
      "Epoch 164/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.1091 - acc: 0.9630\n",
      "Epoch 165/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1080 - acc: 0.9630\n",
      "Epoch 166/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.1076 - acc: 0.9630\n",
      "Epoch 167/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1069 - acc: 0.9630\n",
      "Epoch 168/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 119us/step - loss: 0.1063 - acc: 0.9630\n",
      "Epoch 169/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.1050 - acc: 0.9630\n",
      "Epoch 170/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.1051 - acc: 0.9630\n",
      "Epoch 171/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.1045 - acc: 0.9630\n",
      "Epoch 172/300\n",
      "52/52 [==============================] - 0s 131us/step - loss: 0.1040 - acc: 0.9630\n",
      "Epoch 173/300\n",
      "52/52 [==============================] - 0s 106us/step - loss: 0.1043 - acc: 0.9630\n",
      "Epoch 174/300\n",
      "52/52 [==============================] - 0s 109us/step - loss: 0.1035 - acc: 0.9630\n",
      "Epoch 175/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1032 - acc: 0.9630\n",
      "Epoch 176/300\n",
      "52/52 [==============================] - 0s 113us/step - loss: 0.1027 - acc: 0.9630\n",
      "Epoch 177/300\n",
      "52/52 [==============================] - 0s 129us/step - loss: 0.1017 - acc: 0.9630\n",
      "Epoch 178/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.1023 - acc: 0.9630\n",
      "Epoch 179/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.1009 - acc: 0.9630\n",
      "Epoch 180/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.1005 - acc: 0.9630\n",
      "Epoch 181/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.1001 - acc: 0.9630\n",
      "Epoch 182/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.0999 - acc: 0.9630\n",
      "Epoch 183/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.0991 - acc: 0.9630\n",
      "Epoch 184/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.0991 - acc: 0.9630\n",
      "Epoch 185/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0993 - acc: 0.9630\n",
      "Epoch 186/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0997 - acc: 0.9630\n",
      "Epoch 187/300\n",
      "52/52 [==============================] - 0s 134us/step - loss: 0.1012 - acc: 0.9630\n",
      "Epoch 188/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.0999 - acc: 0.9630\n",
      "Epoch 189/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.0982 - acc: 0.9630\n",
      "Epoch 190/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0980 - acc: 0.9630\n",
      "Epoch 191/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.0970 - acc: 0.9630\n",
      "Epoch 192/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.0978 - acc: 0.9630\n",
      "Epoch 193/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0987 - acc: 0.9630\n",
      "Epoch 194/300\n",
      "52/52 [==============================] - 0s 132us/step - loss: 0.0974 - acc: 0.9630\n",
      "Epoch 195/300\n",
      "52/52 [==============================] - 0s 140us/step - loss: 0.0973 - acc: 0.9630\n",
      "Epoch 196/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0965 - acc: 0.9630\n",
      "Epoch 197/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.0958 - acc: 0.9623\n",
      "Epoch 198/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.0955 - acc: 0.9615\n",
      "Epoch 199/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.0947 - acc: 0.9615\n",
      "Epoch 200/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0949 - acc: 0.9615\n",
      "Epoch 201/300\n",
      "52/52 [==============================] - 0s 132us/step - loss: 0.0940 - acc: 0.9615\n",
      "Epoch 202/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.0940 - acc: 0.9623\n",
      "Epoch 203/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.0931 - acc: 0.9630\n",
      "Epoch 204/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.0927 - acc: 0.9615\n",
      "Epoch 205/300\n",
      "52/52 [==============================] - 0s 131us/step - loss: 0.0935 - acc: 0.9638\n",
      "Epoch 206/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0922 - acc: 0.9630\n",
      "Epoch 207/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0926 - acc: 0.9645\n",
      "Epoch 208/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0926 - acc: 0.9615\n",
      "Epoch 209/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.0914 - acc: 0.9645\n",
      "Epoch 210/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0911 - acc: 0.9615\n",
      "Epoch 211/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.0902 - acc: 0.9630\n",
      "Epoch 212/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.0904 - acc: 0.9645\n",
      "Epoch 213/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.0898 - acc: 0.9645\n",
      "Epoch 214/300\n",
      "52/52 [==============================] - 0s 129us/step - loss: 0.0890 - acc: 0.9660\n",
      "Epoch 215/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0896 - acc: 0.9645\n",
      "Epoch 216/300\n",
      "52/52 [==============================] - 0s 141us/step - loss: 0.0898 - acc: 0.9645\n",
      "Epoch 217/300\n",
      "52/52 [==============================] - 0s 140us/step - loss: 0.0887 - acc: 0.9652\n",
      "Epoch 218/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0877 - acc: 0.9645\n",
      "Epoch 219/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.0876 - acc: 0.9645\n",
      "Epoch 220/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0876 - acc: 0.9652\n",
      "Epoch 221/300\n",
      "52/52 [==============================] - 0s 137us/step - loss: 0.0873 - acc: 0.9645\n",
      "Epoch 222/300\n",
      "52/52 [==============================] - 0s 147us/step - loss: 0.0873 - acc: 0.9638\n",
      "Epoch 223/300\n",
      "52/52 [==============================] - 0s 164us/step - loss: 0.0868 - acc: 0.9645\n",
      "Epoch 224/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0878 - acc: 0.9660\n",
      "Epoch 225/300\n",
      "52/52 [==============================] - 0s 139us/step - loss: 0.0891 - acc: 0.9645\n",
      "Epoch 226/300\n",
      "52/52 [==============================] - 0s 197us/step - loss: 0.0916 - acc: 0.9660\n",
      "Epoch 227/300\n",
      "52/52 [==============================] - 0s 142us/step - loss: 0.0890 - acc: 0.9645\n",
      "Epoch 228/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.0861 - acc: 0.9660\n",
      "Epoch 229/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.0902 - acc: 0.9645\n",
      "Epoch 230/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0877 - acc: 0.9645\n",
      "Epoch 231/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.0860 - acc: 0.9645\n",
      "Epoch 232/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.0857 - acc: 0.9675\n",
      "Epoch 233/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0862 - acc: 0.9645\n",
      "Epoch 234/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.0849 - acc: 0.9689\n",
      "Epoch 235/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0844 - acc: 0.9697\n",
      "Epoch 236/300\n",
      "52/52 [==============================] - 0s 113us/step - loss: 0.0830 - acc: 0.9704\n",
      "Epoch 237/300\n",
      "52/52 [==============================] - 0s 128us/step - loss: 0.0828 - acc: 0.9689\n",
      "Epoch 238/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0835 - acc: 0.9682\n",
      "Epoch 239/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0827 - acc: 0.9689\n",
      "Epoch 240/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.0823 - acc: 0.9660\n",
      "Epoch 241/300\n",
      "52/52 [==============================] - 0s 133us/step - loss: 0.0820 - acc: 0.9660\n",
      "Epoch 242/300\n",
      "52/52 [==============================] - 0s 155us/step - loss: 0.0815 - acc: 0.9660\n",
      "Epoch 243/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0827 - acc: 0.9675\n",
      "Epoch 244/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0819 - acc: 0.9682\n",
      "Epoch 245/300\n",
      "52/52 [==============================] - 0s 131us/step - loss: 0.0809 - acc: 0.9675\n",
      "Epoch 246/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0810 - acc: 0.9667\n",
      "Epoch 247/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0808 - acc: 0.9667\n",
      "Epoch 248/300\n",
      "52/52 [==============================] - 0s 138us/step - loss: 0.0805 - acc: 0.9697\n",
      "Epoch 249/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0815 - acc: 0.9689\n",
      "Epoch 250/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.0806 - acc: 0.9682\n",
      "Epoch 251/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 110us/step - loss: 0.0792 - acc: 0.9675\n",
      "Epoch 252/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.0789 - acc: 0.9675\n",
      "Epoch 253/300\n",
      "52/52 [==============================] - 0s 114us/step - loss: 0.0790 - acc: 0.9689\n",
      "Epoch 254/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.0784 - acc: 0.9675\n",
      "Epoch 255/300\n",
      "52/52 [==============================] - 0s 111us/step - loss: 0.0778 - acc: 0.9675\n",
      "Epoch 256/300\n",
      "52/52 [==============================] - 0s 110us/step - loss: 0.0782 - acc: 0.9667\n",
      "Epoch 257/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.0779 - acc: 0.9682\n",
      "Epoch 258/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.0777 - acc: 0.9697\n",
      "Epoch 259/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.0775 - acc: 0.9689\n",
      "Epoch 260/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0767 - acc: 0.9682\n",
      "Epoch 261/300\n",
      "52/52 [==============================] - 0s 130us/step - loss: 0.0768 - acc: 0.9675\n",
      "Epoch 262/300\n",
      "52/52 [==============================] - 0s 123us/step - loss: 0.0771 - acc: 0.9682\n",
      "Epoch 263/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.0774 - acc: 0.9704\n",
      "Epoch 264/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0777 - acc: 0.9719\n",
      "Epoch 265/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.0770 - acc: 0.9712\n",
      "Epoch 266/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.0769 - acc: 0.9682\n",
      "Epoch 267/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.0794 - acc: 0.9675\n",
      "Epoch 268/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.0763 - acc: 0.9719\n",
      "Epoch 269/300\n",
      "52/52 [==============================] - 0s 125us/step - loss: 0.0752 - acc: 0.9704\n",
      "Epoch 270/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.0761 - acc: 0.9682\n",
      "Epoch 271/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.0751 - acc: 0.9682\n",
      "Epoch 272/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0747 - acc: 0.9726\n",
      "Epoch 273/300\n",
      "52/52 [==============================] - 0s 117us/step - loss: 0.0732 - acc: 0.9763\n",
      "Epoch 274/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0751 - acc: 0.9726\n",
      "Epoch 275/300\n",
      "52/52 [==============================] - 0s 119us/step - loss: 0.0739 - acc: 0.9719\n",
      "Epoch 276/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0733 - acc: 0.9704\n",
      "Epoch 277/300\n",
      "52/52 [==============================] - 0s 126us/step - loss: 0.0716 - acc: 0.9719\n",
      "Epoch 278/300\n",
      "52/52 [==============================] - 0s 138us/step - loss: 0.0722 - acc: 0.9712\n",
      "Epoch 279/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.0724 - acc: 0.9734\n",
      "Epoch 280/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0723 - acc: 0.9741\n",
      "Epoch 281/300\n",
      "52/52 [==============================] - 0s 130us/step - loss: 0.0720 - acc: 0.9719\n",
      "Epoch 282/300\n",
      "52/52 [==============================] - 0s 112us/step - loss: 0.0716 - acc: 0.9726\n",
      "Epoch 283/300\n",
      "52/52 [==============================] - 0s 122us/step - loss: 0.0709 - acc: 0.9726\n",
      "Epoch 284/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0704 - acc: 0.9712\n",
      "Epoch 285/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0701 - acc: 0.9726\n",
      "Epoch 286/300\n",
      "52/52 [==============================] - 0s 127us/step - loss: 0.0706 - acc: 0.9756\n",
      "Epoch 287/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.0699 - acc: 0.9778\n",
      "Epoch 288/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.0696 - acc: 0.9756\n",
      "Epoch 289/300\n",
      "52/52 [==============================] - 0s 111us/step - loss: 0.0699 - acc: 0.9719\n",
      "Epoch 290/300\n",
      "52/52 [==============================] - 0s 115us/step - loss: 0.0703 - acc: 0.9741\n",
      "Epoch 291/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0754 - acc: 0.9689\n",
      "Epoch 292/300\n",
      "52/52 [==============================] - 0s 120us/step - loss: 0.0805 - acc: 0.9712\n",
      "Epoch 293/300\n",
      "52/52 [==============================] - 0s 116us/step - loss: 0.0763 - acc: 0.9704\n",
      "Epoch 294/300\n",
      "52/52 [==============================] - 0s 121us/step - loss: 0.0740 - acc: 0.9719\n",
      "Epoch 295/300\n",
      "52/52 [==============================] - 0s 124us/step - loss: 0.0783 - acc: 0.9645\n",
      "Epoch 296/300\n",
      "52/52 [==============================] - 0s 148us/step - loss: 0.0811 - acc: 0.9682\n",
      "Epoch 297/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.0848 - acc: 0.9689\n",
      "Epoch 298/300\n",
      "52/52 [==============================] - 0s 118us/step - loss: 0.0741 - acc: 0.9726\n",
      "Epoch 299/300\n",
      "52/52 [==============================] - 0s 111us/step - loss: 0.0705 - acc: 0.9719\n",
      "Epoch 300/300\n",
      "52/52 [==============================] - 0s 112us/step - loss: 0.0724 - acc: 0.9719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3953dc50>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return (I2L[index])\n",
    "\n",
    "def predict_results(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index = np.argmax(predictions, axis=1)\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index))\n",
    "\n",
    "    return (prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = 'IAMREALLYGOOD'\n",
    "text, x_train, y_train = caeserde(mystr,x_as_vector=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: IAMREALLYGOOD\n",
      "Cipertext: LDPUHDOOBJRRG\n",
      "Prediction: IALPEAKKYGMMD\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",mystr)\n",
    "print(\"Cipertext:\",text)\n",
    "print(\"Prediction:\",\"\".join(predict_results(model, x_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 26 outputs, the results seems better than 1 output by using the binary cross entropy which are able get the 100% accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(26, input_dim=1, activation='relu'))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(50, activation='tanh'))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(26, activation='relu'))\n",
    "\n",
    "# change sigomiod to softmax made acc increasing 100%\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "13/13 [==============================] - 2s 125ms/step - loss: 3.3540 - acc: 0.0000e+00\n",
      "Epoch 2/600\n",
      "13/13 [==============================] - 0s 192us/step - loss: 3.3023 - acc: 0.0000e+00\n",
      "Epoch 3/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 3.2566 - acc: 0.0000e+00\n",
      "Epoch 4/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 3.2162 - acc: 0.0000e+00\n",
      "Epoch 5/600\n",
      "13/13 [==============================] - 0s 180us/step - loss: 3.1872 - acc: 0.1538\n",
      "Epoch 6/600\n",
      "13/13 [==============================] - 0s 180us/step - loss: 3.1624 - acc: 0.1538\n",
      "Epoch 7/600\n",
      "13/13 [==============================] - 0s 181us/step - loss: 3.1408 - acc: 0.1538\n",
      "Epoch 8/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 3.1213 - acc: 0.1538\n",
      "Epoch 9/600\n",
      "13/13 [==============================] - 0s 189us/step - loss: 3.1020 - acc: 0.1538\n",
      "Epoch 10/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 3.0839 - acc: 0.1538\n",
      "Epoch 11/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 3.0647 - acc: 0.1538\n",
      "Epoch 12/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 3.0462 - acc: 0.1538\n",
      "Epoch 13/600\n",
      "13/13 [==============================] - 0s 194us/step - loss: 3.0301 - acc: 0.1538\n",
      "Epoch 14/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 3.0158 - acc: 0.1538\n",
      "Epoch 15/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 2.9994 - acc: 0.1538\n",
      "Epoch 16/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 2.9818 - acc: 0.1538\n",
      "Epoch 17/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 2.9632 - acc: 0.1538\n",
      "Epoch 18/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 2.9438 - acc: 0.1538\n",
      "Epoch 19/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 2.9243 - acc: 0.1538\n",
      "Epoch 20/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 2.9040 - acc: 0.1538\n",
      "Epoch 21/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 2.8839 - acc: 0.1538\n",
      "Epoch 22/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 2.8648 - acc: 0.1538\n",
      "Epoch 23/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 2.8472 - acc: 0.1538\n",
      "Epoch 24/600\n",
      "13/13 [==============================] - 0s 242us/step - loss: 2.8307 - acc: 0.1538\n",
      "Epoch 25/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 2.8138 - acc: 0.1538\n",
      "Epoch 26/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 2.7962 - acc: 0.1538\n",
      "Epoch 27/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 2.7783 - acc: 0.1538\n",
      "Epoch 28/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 2.7604 - acc: 0.1538\n",
      "Epoch 29/600\n",
      "13/13 [==============================] - 0s 194us/step - loss: 2.7424 - acc: 0.1538\n",
      "Epoch 30/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 2.7247 - acc: 0.3077\n",
      "Epoch 31/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 2.7067 - acc: 0.3077\n",
      "Epoch 32/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 2.6887 - acc: 0.3077\n",
      "Epoch 33/600\n",
      "13/13 [==============================] - 0s 186us/step - loss: 2.6705 - acc: 0.3077\n",
      "Epoch 34/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 2.6522 - acc: 0.3077\n",
      "Epoch 35/600\n",
      "13/13 [==============================] - 0s 233us/step - loss: 2.6340 - acc: 0.3077\n",
      "Epoch 36/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 2.6156 - acc: 0.3077\n",
      "Epoch 37/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 2.5954 - acc: 0.3077\n",
      "Epoch 38/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 2.5763 - acc: 0.3077\n",
      "Epoch 39/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 2.5582 - acc: 0.3077\n",
      "Epoch 40/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 2.5390 - acc: 0.3077\n",
      "Epoch 41/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 2.5202 - acc: 0.3077\n",
      "Epoch 42/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 2.4995 - acc: 0.3077\n",
      "Epoch 43/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 2.4777 - acc: 0.3077\n",
      "Epoch 44/600\n",
      "13/13 [==============================] - 0s 195us/step - loss: 2.4568 - acc: 0.3077\n",
      "Epoch 45/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 2.4363 - acc: 0.3077\n",
      "Epoch 46/600\n",
      "13/13 [==============================] - 0s 189us/step - loss: 2.4157 - acc: 0.3077\n",
      "Epoch 47/600\n",
      "13/13 [==============================] - 0s 187us/step - loss: 2.3942 - acc: 0.3077\n",
      "Epoch 48/600\n",
      "13/13 [==============================] - 0s 187us/step - loss: 2.3718 - acc: 0.3077\n",
      "Epoch 49/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 2.3492 - acc: 0.3077\n",
      "Epoch 50/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 2.3265 - acc: 0.3077\n",
      "Epoch 51/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 2.3042 - acc: 0.3077\n",
      "Epoch 52/600\n",
      "13/13 [==============================] - 0s 197us/step - loss: 2.2818 - acc: 0.3077\n",
      "Epoch 53/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 2.2587 - acc: 0.3077\n",
      "Epoch 54/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 2.2354 - acc: 0.3077\n",
      "Epoch 55/600\n",
      "13/13 [==============================] - 0s 185us/step - loss: 2.2117 - acc: 0.3077\n",
      "Epoch 56/600\n",
      "13/13 [==============================] - 0s 194us/step - loss: 2.1874 - acc: 0.3077\n",
      "Epoch 57/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 2.1639 - acc: 0.3077\n",
      "Epoch 58/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 2.1400 - acc: 0.3077\n",
      "Epoch 59/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 2.1158 - acc: 0.3077\n",
      "Epoch 60/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 2.0911 - acc: 0.3077\n",
      "Epoch 61/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 2.0658 - acc: 0.3077\n",
      "Epoch 62/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 2.0405 - acc: 0.3077\n",
      "Epoch 63/600\n",
      "13/13 [==============================] - 0s 291us/step - loss: 2.0154 - acc: 0.3077\n",
      "Epoch 64/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.9906 - acc: 0.3077\n",
      "Epoch 65/600\n",
      "13/13 [==============================] - 0s 234us/step - loss: 1.9669 - acc: 0.3077\n",
      "Epoch 66/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 1.9422 - acc: 0.3077\n",
      "Epoch 67/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 1.9179 - acc: 0.3077\n",
      "Epoch 68/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 1.8938 - acc: 0.3077\n",
      "Epoch 69/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 1.8698 - acc: 0.3077\n",
      "Epoch 70/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 1.8460 - acc: 0.3077\n",
      "Epoch 71/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 1.8231 - acc: 0.3077\n",
      "Epoch 72/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 1.8006 - acc: 0.3077\n",
      "Epoch 73/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.7781 - acc: 0.3077\n",
      "Epoch 74/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 1.7557 - acc: 0.3077\n",
      "Epoch 75/600\n",
      "13/13 [==============================] - 0s 195us/step - loss: 1.7339 - acc: 0.3077\n",
      "Epoch 76/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 1.7129 - acc: 0.3077\n",
      "Epoch 77/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 1.6929 - acc: 0.3077\n",
      "Epoch 78/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 1.6737 - acc: 0.3077\n",
      "Epoch 79/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 1.6546 - acc: 0.3077\n",
      "Epoch 80/600\n",
      "13/13 [==============================] - 0s 233us/step - loss: 1.6361 - acc: 0.3077\n",
      "Epoch 81/600\n",
      "13/13 [==============================] - 0s 195us/step - loss: 1.6178 - acc: 0.3077\n",
      "Epoch 82/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 1.5998 - acc: 0.3077\n",
      "Epoch 83/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 1.5834 - acc: 0.3077\n",
      "Epoch 84/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 1.5671 - acc: 0.3077\n",
      "Epoch 85/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 1.5514 - acc: 0.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 1.5357 - acc: 0.3077\n",
      "Epoch 87/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 1.5213 - acc: 0.3077\n",
      "Epoch 88/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 1.5064 - acc: 0.3077\n",
      "Epoch 89/600\n",
      "13/13 [==============================] - 0s 227us/step - loss: 1.4917 - acc: 0.3077\n",
      "Epoch 90/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 1.4786 - acc: 0.3846\n",
      "Epoch 91/600\n",
      "13/13 [==============================] - 0s 255us/step - loss: 1.4640 - acc: 0.3846\n",
      "Epoch 92/600\n",
      "13/13 [==============================] - 0s 251us/step - loss: 1.4509 - acc: 0.5385\n",
      "Epoch 93/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 1.4372 - acc: 0.3846\n",
      "Epoch 94/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.4238 - acc: 0.5385\n",
      "Epoch 95/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.4117 - acc: 0.5385\n",
      "Epoch 96/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 1.3993 - acc: 0.5385\n",
      "Epoch 97/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 1.3867 - acc: 0.5385\n",
      "Epoch 98/600\n",
      "13/13 [==============================] - 0s 304us/step - loss: 1.3746 - acc: 0.3846\n",
      "Epoch 99/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.3627 - acc: 0.3846\n",
      "Epoch 100/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 1.3510 - acc: 0.5385\n",
      "Epoch 101/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 1.3397 - acc: 0.5385\n",
      "Epoch 102/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 1.3274 - acc: 0.5385\n",
      "Epoch 103/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 1.3157 - acc: 0.5385\n",
      "Epoch 104/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 1.3053 - acc: 0.5385\n",
      "Epoch 105/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 1.2947 - acc: 0.3846\n",
      "Epoch 106/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 1.2832 - acc: 0.6154\n",
      "Epoch 107/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 1.2732 - acc: 0.6154\n",
      "Epoch 108/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 1.2613 - acc: 0.6154\n",
      "Epoch 109/600\n",
      "13/13 [==============================] - 0s 195us/step - loss: 1.2508 - acc: 0.6154\n",
      "Epoch 110/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 1.2404 - acc: 0.6154\n",
      "Epoch 111/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 1.2292 - acc: 0.6154\n",
      "Epoch 112/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 1.2188 - acc: 0.6154\n",
      "Epoch 113/600\n",
      "13/13 [==============================] - 0s 261us/step - loss: 1.2065 - acc: 0.6923\n",
      "Epoch 114/600\n",
      "13/13 [==============================] - 0s 189us/step - loss: 1.1963 - acc: 0.6923\n",
      "Epoch 115/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 1.1852 - acc: 0.6923\n",
      "Epoch 116/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 1.1742 - acc: 0.6923\n",
      "Epoch 117/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 1.1643 - acc: 0.6923\n",
      "Epoch 118/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 1.1563 - acc: 0.6923\n",
      "Epoch 119/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 1.1448 - acc: 0.6923\n",
      "Epoch 120/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 1.1336 - acc: 0.6923\n",
      "Epoch 121/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 1.1239 - acc: 0.6923\n",
      "Epoch 122/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 1.1150 - acc: 0.6923\n",
      "Epoch 123/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 1.1039 - acc: 0.6923\n",
      "Epoch 124/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 1.0985 - acc: 0.6923\n",
      "Epoch 125/600\n",
      "13/13 [==============================] - 0s 290us/step - loss: 1.0856 - acc: 0.6923\n",
      "Epoch 126/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 1.0759 - acc: 0.6923\n",
      "Epoch 127/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 1.0668 - acc: 0.6923\n",
      "Epoch 128/600\n",
      "13/13 [==============================] - 0s 282us/step - loss: 1.0568 - acc: 0.6923\n",
      "Epoch 129/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.0476 - acc: 0.6923\n",
      "Epoch 130/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 1.0381 - acc: 0.6923\n",
      "Epoch 131/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.0291 - acc: 0.6923\n",
      "Epoch 132/600\n",
      "13/13 [==============================] - 0s 240us/step - loss: 1.0250 - acc: 0.6923\n",
      "Epoch 133/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 1.0110 - acc: 0.6923\n",
      "Epoch 134/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 1.0032 - acc: 0.7692\n",
      "Epoch 135/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 1.0036 - acc: 0.7692\n",
      "Epoch 136/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.9861 - acc: 0.8462\n",
      "Epoch 137/600\n",
      "13/13 [==============================] - 0s 266us/step - loss: 0.9885 - acc: 0.7692\n",
      "Epoch 138/600\n",
      "13/13 [==============================] - 0s 244us/step - loss: 0.9695 - acc: 0.7692\n",
      "Epoch 139/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.9716 - acc: 0.8462\n",
      "Epoch 140/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.9544 - acc: 0.7692\n",
      "Epoch 141/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.9529 - acc: 0.7692\n",
      "Epoch 142/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.9415 - acc: 0.7692\n",
      "Epoch 143/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.9347 - acc: 0.7692\n",
      "Epoch 144/600\n",
      "13/13 [==============================] - 0s 233us/step - loss: 0.9292 - acc: 0.8462\n",
      "Epoch 145/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 0.9174 - acc: 0.7692\n",
      "Epoch 146/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.9183 - acc: 0.7692\n",
      "Epoch 147/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.9021 - acc: 0.8462\n",
      "Epoch 148/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.9065 - acc: 0.8462\n",
      "Epoch 149/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.8864 - acc: 0.8462\n",
      "Epoch 150/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.8886 - acc: 0.7692\n",
      "Epoch 151/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.8722 - acc: 0.8462\n",
      "Epoch 152/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.8712 - acc: 0.8462\n",
      "Epoch 153/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.8589 - acc: 0.8462\n",
      "Epoch 154/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.8586 - acc: 0.8462\n",
      "Epoch 155/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.8492 - acc: 0.8462\n",
      "Epoch 156/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.8406 - acc: 0.8462\n",
      "Epoch 157/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.8357 - acc: 0.8462\n",
      "Epoch 158/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.8306 - acc: 0.8462\n",
      "Epoch 159/600\n",
      "13/13 [==============================] - 0s 251us/step - loss: 0.8222 - acc: 0.9231\n",
      "Epoch 160/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.8175 - acc: 0.8462\n",
      "Epoch 161/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.8065 - acc: 0.8462\n",
      "Epoch 162/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.8023 - acc: 0.9231\n",
      "Epoch 163/600\n",
      "13/13 [==============================] - 0s 190us/step - loss: 0.7951 - acc: 0.9231\n",
      "Epoch 164/600\n",
      "13/13 [==============================] - 0s 250us/step - loss: 0.7898 - acc: 0.8462\n",
      "Epoch 165/600\n",
      "13/13 [==============================] - 0s 197us/step - loss: 0.7829 - acc: 0.9231\n",
      "Epoch 166/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.7779 - acc: 0.9231\n",
      "Epoch 167/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.7700 - acc: 0.9231\n",
      "Epoch 168/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.7672 - acc: 0.8462\n",
      "Epoch 169/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.7577 - acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.7535 - acc: 0.9231\n",
      "Epoch 171/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.7461 - acc: 0.9231\n",
      "Epoch 172/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.7409 - acc: 0.9231\n",
      "Epoch 173/600\n",
      "13/13 [==============================] - 0s 242us/step - loss: 0.7371 - acc: 0.9231\n",
      "Epoch 174/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.7289 - acc: 0.9231\n",
      "Epoch 175/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.7229 - acc: 0.9231\n",
      "Epoch 176/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.7172 - acc: 0.9231\n",
      "Epoch 177/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.7120 - acc: 0.9231\n",
      "Epoch 178/600\n",
      "13/13 [==============================] - 0s 260us/step - loss: 0.7075 - acc: 0.9231\n",
      "Epoch 179/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.7025 - acc: 0.9231\n",
      "Epoch 180/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.6943 - acc: 0.9231\n",
      "Epoch 181/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.6959 - acc: 0.9231\n",
      "Epoch 182/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.6882 - acc: 0.9231\n",
      "Epoch 183/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.6831 - acc: 0.9231\n",
      "Epoch 184/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.6735 - acc: 0.9231\n",
      "Epoch 185/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.6725 - acc: 0.9231\n",
      "Epoch 186/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.6688 - acc: 0.9231\n",
      "Epoch 187/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.6585 - acc: 0.9231\n",
      "Epoch 188/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 0.6579 - acc: 0.9231\n",
      "Epoch 189/600\n",
      "13/13 [==============================] - 0s 239us/step - loss: 0.6581 - acc: 0.9231\n",
      "Epoch 190/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.6445 - acc: 0.9231\n",
      "Epoch 191/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.6478 - acc: 0.9231\n",
      "Epoch 192/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.6389 - acc: 0.9231\n",
      "Epoch 193/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.6361 - acc: 0.9231\n",
      "Epoch 194/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 0.6282 - acc: 0.9231\n",
      "Epoch 195/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.6252 - acc: 0.9231\n",
      "Epoch 196/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.6220 - acc: 0.9231\n",
      "Epoch 197/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.6108 - acc: 0.9231\n",
      "Epoch 198/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.6151 - acc: 0.9231\n",
      "Epoch 199/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.6034 - acc: 0.9231\n",
      "Epoch 200/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.6029 - acc: 0.9231\n",
      "Epoch 201/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.6030 - acc: 0.9231\n",
      "Epoch 202/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.5879 - acc: 0.9231\n",
      "Epoch 203/600\n",
      "13/13 [==============================] - 0s 227us/step - loss: 0.5938 - acc: 0.9231\n",
      "Epoch 204/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.5800 - acc: 0.9231\n",
      "Epoch 205/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.5801 - acc: 0.9231\n",
      "Epoch 206/600\n",
      "13/13 [==============================] - 0s 233us/step - loss: 0.5776 - acc: 0.9231\n",
      "Epoch 207/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.5669 - acc: 0.9231\n",
      "Epoch 208/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.5713 - acc: 0.9231\n",
      "Epoch 209/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.5595 - acc: 0.9231\n",
      "Epoch 210/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.5588 - acc: 0.9231\n",
      "Epoch 211/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.5589 - acc: 0.9231\n",
      "Epoch 212/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.5455 - acc: 0.9231\n",
      "Epoch 213/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.5477 - acc: 0.9231\n",
      "Epoch 214/600\n",
      "13/13 [==============================] - 0s 325us/step - loss: 0.5398 - acc: 0.9231\n",
      "Epoch 215/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.5334 - acc: 0.9231\n",
      "Epoch 216/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.5314 - acc: 0.9231\n",
      "Epoch 217/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.5257 - acc: 0.9231\n",
      "Epoch 218/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.5227 - acc: 0.9231\n",
      "Epoch 219/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.5198 - acc: 0.9231\n",
      "Epoch 220/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.5155 - acc: 0.9231\n",
      "Epoch 221/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.5102 - acc: 0.9231\n",
      "Epoch 222/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.5065 - acc: 0.9231\n",
      "Epoch 223/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.5036 - acc: 0.9231\n",
      "Epoch 224/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.5029 - acc: 0.9231\n",
      "Epoch 225/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.4968 - acc: 0.9231\n",
      "Epoch 226/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.4938 - acc: 0.9231\n",
      "Epoch 227/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.4879 - acc: 0.9231\n",
      "Epoch 228/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.4835 - acc: 0.9231\n",
      "Epoch 229/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.4822 - acc: 0.9231\n",
      "Epoch 230/600\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.4819 - acc: 0.9231\n",
      "Epoch 231/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.4788 - acc: 0.9231\n",
      "Epoch 232/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.4697 - acc: 0.9231\n",
      "Epoch 233/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.4704 - acc: 0.9231\n",
      "Epoch 234/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.4747 - acc: 0.9231\n",
      "Epoch 235/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.4594 - acc: 0.9231\n",
      "Epoch 236/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.4693 - acc: 0.9231\n",
      "Epoch 237/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.4658 - acc: 0.9231\n",
      "Epoch 238/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.4514 - acc: 0.9231\n",
      "Epoch 239/600\n",
      "13/13 [==============================] - 0s 268us/step - loss: 0.4622 - acc: 0.9231\n",
      "Epoch 240/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.4467 - acc: 0.9231\n",
      "Epoch 241/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.4488 - acc: 0.9231\n",
      "Epoch 242/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.4401 - acc: 0.9231\n",
      "Epoch 243/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.4380 - acc: 0.9231\n",
      "Epoch 244/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.4432 - acc: 0.9231\n",
      "Epoch 245/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.4276 - acc: 0.9231\n",
      "Epoch 246/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.4372 - acc: 0.9231\n",
      "Epoch 247/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.4246 - acc: 0.9231\n",
      "Epoch 248/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.4271 - acc: 0.9231\n",
      "Epoch 249/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.4242 - acc: 0.9231\n",
      "Epoch 250/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.4123 - acc: 0.9231\n",
      "Epoch 251/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.4143 - acc: 0.9231\n",
      "Epoch 252/600\n",
      "13/13 [==============================] - 0s 195us/step - loss: 0.4060 - acc: 0.9231\n",
      "Epoch 253/600\n",
      "13/13 [==============================] - 0s 190us/step - loss: 0.4077 - acc: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.4008 - acc: 0.9231\n",
      "Epoch 255/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.4005 - acc: 0.9231\n",
      "Epoch 256/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.3994 - acc: 1.0000\n",
      "Epoch 257/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.3905 - acc: 0.9231\n",
      "Epoch 258/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.3923 - acc: 0.9231\n",
      "Epoch 259/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.3858 - acc: 0.9231\n",
      "Epoch 260/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 0.3826 - acc: 0.9231\n",
      "Epoch 261/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.3872 - acc: 0.9231\n",
      "Epoch 262/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.3755 - acc: 0.9231\n",
      "Epoch 263/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.3770 - acc: 0.9231\n",
      "Epoch 264/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.3748 - acc: 0.9231\n",
      "Epoch 265/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.3676 - acc: 0.9231\n",
      "Epoch 266/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.3706 - acc: 0.9231\n",
      "Epoch 267/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.3666 - acc: 0.9231\n",
      "Epoch 268/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.3594 - acc: 0.9231\n",
      "Epoch 269/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.3655 - acc: 0.9231\n",
      "Epoch 270/600\n",
      "13/13 [==============================] - 0s 238us/step - loss: 0.3632 - acc: 0.9231\n",
      "Epoch 271/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.3517 - acc: 0.9231\n",
      "Epoch 272/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.3613 - acc: 1.0000\n",
      "Epoch 273/600\n",
      "13/13 [==============================] - 0s 245us/step - loss: 0.3542 - acc: 0.9231\n",
      "Epoch 274/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.3442 - acc: 0.9231\n",
      "Epoch 275/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.3496 - acc: 1.0000\n",
      "Epoch 276/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 0.3478 - acc: 0.9231\n",
      "Epoch 277/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.3404 - acc: 0.9231\n",
      "Epoch 278/600\n",
      "13/13 [==============================] - 0s 247us/step - loss: 0.3461 - acc: 1.0000\n",
      "Epoch 279/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.3331 - acc: 0.9231\n",
      "Epoch 280/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.3366 - acc: 0.9231\n",
      "Epoch 281/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.3323 - acc: 1.0000\n",
      "Epoch 282/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.3269 - acc: 0.9231\n",
      "Epoch 283/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.3278 - acc: 0.9231\n",
      "Epoch 284/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 0.3213 - acc: 0.9231\n",
      "Epoch 285/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.3232 - acc: 1.0000\n",
      "Epoch 286/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.3163 - acc: 0.9231\n",
      "Epoch 287/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.3168 - acc: 0.9231\n",
      "Epoch 288/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.3141 - acc: 1.0000\n",
      "Epoch 289/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.3082 - acc: 0.9231\n",
      "Epoch 290/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.3090 - acc: 0.9231\n",
      "Epoch 291/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 0.3046 - acc: 1.0000\n",
      "Epoch 292/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.3020 - acc: 1.0000\n",
      "Epoch 293/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.3044 - acc: 0.9231\n",
      "Epoch 294/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.2977 - acc: 1.0000\n",
      "Epoch 295/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.2943 - acc: 0.9231\n",
      "Epoch 296/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.2920 - acc: 0.9231\n",
      "Epoch 297/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.2903 - acc: 1.0000\n",
      "Epoch 298/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.2890 - acc: 0.9231\n",
      "Epoch 299/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.2896 - acc: 1.0000\n",
      "Epoch 300/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.2883 - acc: 0.9231\n",
      "Epoch 301/600\n",
      "13/13 [==============================] - 0s 236us/step - loss: 0.2871 - acc: 1.0000\n",
      "Epoch 302/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.2819 - acc: 0.9231\n",
      "Epoch 303/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.2775 - acc: 1.0000\n",
      "Epoch 304/600\n",
      "13/13 [==============================] - 0s 239us/step - loss: 0.2766 - acc: 1.0000\n",
      "Epoch 305/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.2757 - acc: 0.9231\n",
      "Epoch 306/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.2714 - acc: 1.0000\n",
      "Epoch 307/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.2738 - acc: 1.0000\n",
      "Epoch 308/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.2814 - acc: 0.9231\n",
      "Epoch 309/600\n",
      "13/13 [==============================] - 0s 197us/step - loss: 0.2674 - acc: 1.0000\n",
      "Epoch 310/600\n",
      "13/13 [==============================] - 0s 242us/step - loss: 0.2683 - acc: 1.0000\n",
      "Epoch 311/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.2810 - acc: 0.9231\n",
      "Epoch 312/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.2608 - acc: 1.0000\n",
      "Epoch 313/600\n",
      "13/13 [==============================] - 0s 240us/step - loss: 0.2734 - acc: 1.0000\n",
      "Epoch 314/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.2712 - acc: 0.9231\n",
      "Epoch 315/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.2590 - acc: 0.9231\n",
      "Epoch 316/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 0.2687 - acc: 1.0000\n",
      "Epoch 317/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.2526 - acc: 1.0000\n",
      "Epoch 318/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.2600 - acc: 0.9231\n",
      "Epoch 319/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.2523 - acc: 1.0000\n",
      "Epoch 320/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.2494 - acc: 1.0000\n",
      "Epoch 321/600\n",
      "13/13 [==============================] - 0s 234us/step - loss: 0.2504 - acc: 0.9231\n",
      "Epoch 322/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.2428 - acc: 1.0000\n",
      "Epoch 323/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 0.2467 - acc: 1.0000\n",
      "Epoch 324/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.2389 - acc: 1.0000\n",
      "Epoch 325/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.2415 - acc: 0.9231\n",
      "Epoch 326/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.2354 - acc: 1.0000\n",
      "Epoch 327/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.2354 - acc: 1.0000\n",
      "Epoch 328/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.2367 - acc: 0.9231\n",
      "Epoch 329/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.2307 - acc: 1.0000\n",
      "Epoch 330/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.2355 - acc: 1.0000\n",
      "Epoch 331/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.2293 - acc: 1.0000\n",
      "Epoch 332/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.2246 - acc: 1.0000\n",
      "Epoch 333/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.2283 - acc: 1.0000\n",
      "Epoch 334/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.2246 - acc: 0.9231\n",
      "Epoch 335/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.2194 - acc: 1.0000\n",
      "Epoch 336/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.2228 - acc: 1.0000\n",
      "Epoch 337/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.2210 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.2144 - acc: 1.0000\n",
      "Epoch 339/600\n",
      "13/13 [==============================] - 0s 234us/step - loss: 0.2199 - acc: 1.0000\n",
      "Epoch 340/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.2171 - acc: 1.0000\n",
      "Epoch 341/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.2107 - acc: 1.0000\n",
      "Epoch 342/600\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.2160 - acc: 1.0000\n",
      "Epoch 343/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.2107 - acc: 1.0000\n",
      "Epoch 344/600\n",
      "13/13 [==============================] - 0s 241us/step - loss: 0.2058 - acc: 1.0000\n",
      "Epoch 345/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.2098 - acc: 1.0000\n",
      "Epoch 346/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.2059 - acc: 1.0000\n",
      "Epoch 347/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.2009 - acc: 1.0000\n",
      "Epoch 348/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.2045 - acc: 1.0000\n",
      "Epoch 349/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.1977 - acc: 1.0000\n",
      "Epoch 350/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.1979 - acc: 1.0000\n",
      "Epoch 351/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.1966 - acc: 1.0000\n",
      "Epoch 352/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.1927 - acc: 1.0000\n",
      "Epoch 353/600\n",
      "13/13 [==============================] - 0s 233us/step - loss: 0.1957 - acc: 1.0000\n",
      "Epoch 354/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.1929 - acc: 1.0000\n",
      "Epoch 355/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.1881 - acc: 1.0000\n",
      "Epoch 356/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.1907 - acc: 1.0000\n",
      "Epoch 357/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.1890 - acc: 1.0000\n",
      "Epoch 358/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.1846 - acc: 1.0000\n",
      "Epoch 359/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 0.1824 - acc: 1.0000\n",
      "Epoch 360/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.1820 - acc: 1.0000\n",
      "Epoch 361/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.1809 - acc: 1.0000\n",
      "Epoch 362/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.1792 - acc: 1.0000\n",
      "Epoch 363/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.1778 - acc: 1.0000\n",
      "Epoch 364/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.1754 - acc: 1.0000\n",
      "Epoch 365/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.1746 - acc: 1.0000\n",
      "Epoch 366/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 0.1778 - acc: 1.0000\n",
      "Epoch 367/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.1735 - acc: 1.0000\n",
      "Epoch 368/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.1721 - acc: 1.0000\n",
      "Epoch 369/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.1693 - acc: 1.0000\n",
      "Epoch 370/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.1677 - acc: 1.0000\n",
      "Epoch 371/600\n",
      "13/13 [==============================] - 0s 236us/step - loss: 0.1665 - acc: 1.0000\n",
      "Epoch 372/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.1656 - acc: 1.0000\n",
      "Epoch 373/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.1666 - acc: 1.0000\n",
      "Epoch 374/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.1645 - acc: 1.0000\n",
      "Epoch 375/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.1640 - acc: 1.0000\n",
      "Epoch 376/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.1604 - acc: 1.0000\n",
      "Epoch 377/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.1588 - acc: 1.0000\n",
      "Epoch 378/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.1588 - acc: 1.0000\n",
      "Epoch 379/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.1569 - acc: 1.0000\n",
      "Epoch 380/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.1560 - acc: 1.0000\n",
      "Epoch 381/600\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.1545 - acc: 1.0000\n",
      "Epoch 382/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.1535 - acc: 1.0000\n",
      "Epoch 383/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.1515 - acc: 1.0000\n",
      "Epoch 384/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.1504 - acc: 1.0000\n",
      "Epoch 385/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.1514 - acc: 1.0000\n",
      "Epoch 386/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.1595 - acc: 1.0000\n",
      "Epoch 387/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.1542 - acc: 1.0000\n",
      "Epoch 388/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.1465 - acc: 1.0000\n",
      "Epoch 389/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 0.1452 - acc: 1.0000\n",
      "Epoch 390/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.1488 - acc: 1.0000\n",
      "Epoch 391/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.1458 - acc: 1.0000\n",
      "Epoch 392/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.1430 - acc: 1.0000\n",
      "Epoch 393/600\n",
      "13/13 [==============================] - 0s 192us/step - loss: 0.1403 - acc: 1.0000\n",
      "Epoch 394/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.1397 - acc: 1.0000\n",
      "Epoch 395/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.1408 - acc: 1.0000\n",
      "Epoch 396/600\n",
      "13/13 [==============================] - 0s 235us/step - loss: 0.1391 - acc: 1.0000\n",
      "Epoch 397/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 0.1390 - acc: 1.0000\n",
      "Epoch 398/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.1356 - acc: 1.0000\n",
      "Epoch 399/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.1342 - acc: 1.0000\n",
      "Epoch 400/600\n",
      "13/13 [==============================] - 0s 254us/step - loss: 0.1346 - acc: 1.0000\n",
      "Epoch 401/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 0.1331 - acc: 1.0000\n",
      "Epoch 402/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.1307 - acc: 1.0000\n",
      "Epoch 403/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.1282 - acc: 1.0000\n",
      "Epoch 404/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.1304 - acc: 1.0000\n",
      "Epoch 405/600\n",
      "13/13 [==============================] - 0s 233us/step - loss: 0.1305 - acc: 1.0000\n",
      "Epoch 406/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.1256 - acc: 1.0000\n",
      "Epoch 407/600\n",
      "13/13 [==============================] - 0s 191us/step - loss: 0.1249 - acc: 1.0000\n",
      "Epoch 408/600\n",
      "13/13 [==============================] - 0s 189us/step - loss: 0.1264 - acc: 1.0000\n",
      "Epoch 409/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.1232 - acc: 1.0000\n",
      "Epoch 410/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.1225 - acc: 1.0000\n",
      "Epoch 411/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.1196 - acc: 1.0000\n",
      "Epoch 412/600\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.1208 - acc: 1.0000\n",
      "Epoch 413/600\n",
      "13/13 [==============================] - 0s 235us/step - loss: 0.1226 - acc: 1.0000\n",
      "Epoch 414/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.1175 - acc: 1.0000\n",
      "Epoch 415/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.1166 - acc: 1.0000\n",
      "Epoch 416/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.1190 - acc: 1.0000\n",
      "Epoch 417/600\n",
      "13/13 [==============================] - 0s 197us/step - loss: 0.1153 - acc: 1.0000\n",
      "Epoch 418/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.1131 - acc: 1.0000\n",
      "Epoch 419/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.1132 - acc: 1.0000\n",
      "Epoch 420/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.1135 - acc: 1.0000\n",
      "Epoch 421/600\n",
      "13/13 [==============================] - 0s 199us/step - loss: 0.1126 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.1096 - acc: 1.0000\n",
      "Epoch 423/600\n",
      "13/13 [==============================] - 0s 235us/step - loss: 0.1083 - acc: 1.0000\n",
      "Epoch 424/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.1079 - acc: 1.0000\n",
      "Epoch 425/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.1075 - acc: 1.0000\n",
      "Epoch 426/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.1062 - acc: 1.0000\n",
      "Epoch 427/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.1050 - acc: 1.0000\n",
      "Epoch 428/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.1045 - acc: 1.0000\n",
      "Epoch 429/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.1041 - acc: 1.0000\n",
      "Epoch 430/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.1041 - acc: 1.0000\n",
      "Epoch 431/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.1044 - acc: 1.0000\n",
      "Epoch 432/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.1021 - acc: 1.0000\n",
      "Epoch 433/600\n",
      "13/13 [==============================] - 0s 206us/step - loss: 0.0996 - acc: 1.0000\n",
      "Epoch 434/600\n",
      "13/13 [==============================] - 0s 240us/step - loss: 0.0993 - acc: 1.0000\n",
      "Epoch 435/600\n",
      "13/13 [==============================] - 0s 212us/step - loss: 0.1005 - acc: 1.0000\n",
      "Epoch 436/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.1017 - acc: 1.0000\n",
      "Epoch 437/600\n",
      "13/13 [==============================] - 0s 202us/step - loss: 0.1027 - acc: 1.0000\n",
      "Epoch 438/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.0990 - acc: 1.0000\n",
      "Epoch 439/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.0959 - acc: 1.0000\n",
      "Epoch 440/600\n",
      "13/13 [==============================] - 0s 192us/step - loss: 0.0954 - acc: 1.0000\n",
      "Epoch 441/600\n",
      "13/13 [==============================] - 0s 244us/step - loss: 0.0950 - acc: 1.0000\n",
      "Epoch 442/600\n",
      "13/13 [==============================] - 0s 272us/step - loss: 0.0955 - acc: 1.0000\n",
      "Epoch 443/600\n",
      "13/13 [==============================] - 0s 234us/step - loss: 0.0953 - acc: 1.0000\n",
      "Epoch 444/600\n",
      "13/13 [==============================] - 0s 242us/step - loss: 0.0924 - acc: 1.0000\n",
      "Epoch 445/600\n",
      "13/13 [==============================] - 0s 248us/step - loss: 0.0917 - acc: 1.0000\n",
      "Epoch 446/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.0912 - acc: 1.0000\n",
      "Epoch 447/600\n",
      "13/13 [==============================] - 0s 240us/step - loss: 0.0905 - acc: 1.0000\n",
      "Epoch 448/600\n",
      "13/13 [==============================] - 0s 275us/step - loss: 0.0883 - acc: 1.0000\n",
      "Epoch 449/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.0863 - acc: 1.0000\n",
      "Epoch 450/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.0868 - acc: 1.0000\n",
      "Epoch 451/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.0859 - acc: 1.0000\n",
      "Epoch 452/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.0848 - acc: 1.0000\n",
      "Epoch 453/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.0845 - acc: 1.0000\n",
      "Epoch 454/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 0.0844 - acc: 1.0000\n",
      "Epoch 455/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0846 - acc: 1.0000\n",
      "Epoch 456/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.0857 - acc: 1.0000\n",
      "Epoch 457/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 0.0827 - acc: 1.0000\n",
      "Epoch 458/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0802 - acc: 1.0000\n",
      "Epoch 459/600\n",
      "13/13 [==============================] - 0s 234us/step - loss: 0.0795 - acc: 1.0000\n",
      "Epoch 460/600\n",
      "13/13 [==============================] - 0s 236us/step - loss: 0.0805 - acc: 1.0000\n",
      "Epoch 461/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.0798 - acc: 1.0000\n",
      "Epoch 462/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0808 - acc: 1.0000\n",
      "Epoch 463/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.0815 - acc: 1.0000\n",
      "Epoch 464/600\n",
      "13/13 [==============================] - 0s 227us/step - loss: 0.0778 - acc: 1.0000\n",
      "Epoch 465/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.0755 - acc: 1.0000\n",
      "Epoch 466/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0782 - acc: 1.0000\n",
      "Epoch 467/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.0785 - acc: 1.0000\n",
      "Epoch 468/600\n",
      "13/13 [==============================] - 0s 218us/step - loss: 0.0765 - acc: 1.0000\n",
      "Epoch 469/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0731 - acc: 1.0000\n",
      "Epoch 470/600\n",
      "13/13 [==============================] - 0s 244us/step - loss: 0.0750 - acc: 1.0000\n",
      "Epoch 471/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.0760 - acc: 1.0000\n",
      "Epoch 472/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0727 - acc: 1.0000\n",
      "Epoch 473/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.0709 - acc: 1.0000\n",
      "Epoch 474/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.0715 - acc: 1.0000\n",
      "Epoch 475/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.0728 - acc: 1.0000\n",
      "Epoch 476/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0706 - acc: 1.0000\n",
      "Epoch 477/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.0688 - acc: 1.0000\n",
      "Epoch 478/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.0685 - acc: 1.0000\n",
      "Epoch 479/600\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.0697 - acc: 1.0000\n",
      "Epoch 480/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.0690 - acc: 1.0000\n",
      "Epoch 481/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0671 - acc: 1.0000\n",
      "Epoch 482/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.0663 - acc: 1.0000\n",
      "Epoch 483/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.0677 - acc: 1.0000\n",
      "Epoch 484/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.0668 - acc: 1.0000\n",
      "Epoch 485/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.0651 - acc: 1.0000\n",
      "Epoch 486/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.0639 - acc: 1.0000\n",
      "Epoch 487/600\n",
      "13/13 [==============================] - 0s 203us/step - loss: 0.0644 - acc: 1.0000\n",
      "Epoch 488/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 0.0667 - acc: 1.0000\n",
      "Epoch 489/600\n",
      "13/13 [==============================] - 0s 238us/step - loss: 0.0671 - acc: 1.0000\n",
      "Epoch 490/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.0640 - acc: 1.0000\n",
      "Epoch 491/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.0618 - acc: 1.0000\n",
      "Epoch 492/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.0629 - acc: 1.0000\n",
      "Epoch 493/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.0641 - acc: 1.0000\n",
      "Epoch 494/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0630 - acc: 1.0000\n",
      "Epoch 495/600\n",
      "13/13 [==============================] - 0s 234us/step - loss: 0.0606 - acc: 1.0000\n",
      "Epoch 496/600\n",
      "13/13 [==============================] - 0s 249us/step - loss: 0.0594 - acc: 1.0000\n",
      "Epoch 497/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.0598 - acc: 1.0000\n",
      "Epoch 498/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.0597 - acc: 1.0000\n",
      "Epoch 499/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0586 - acc: 1.0000\n",
      "Epoch 500/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.0576 - acc: 1.0000\n",
      "Epoch 501/600\n",
      "13/13 [==============================] - 0s 237us/step - loss: 0.0578 - acc: 1.0000\n",
      "Epoch 502/600\n",
      "13/13 [==============================] - 0s 373us/step - loss: 0.0576 - acc: 1.0000\n",
      "Epoch 503/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.0566 - acc: 1.0000\n",
      "Epoch 504/600\n",
      "13/13 [==============================] - 0s 207us/step - loss: 0.0559 - acc: 1.0000\n",
      "Epoch 505/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0555 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 506/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.0551 - acc: 1.0000\n",
      "Epoch 507/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0546 - acc: 1.0000\n",
      "Epoch 508/600\n",
      "13/13 [==============================] - 0s 239us/step - loss: 0.0541 - acc: 1.0000\n",
      "Epoch 509/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.0539 - acc: 1.0000\n",
      "Epoch 510/600\n",
      "13/13 [==============================] - 0s 238us/step - loss: 0.0554 - acc: 1.0000\n",
      "Epoch 511/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.0563 - acc: 1.0000\n",
      "Epoch 512/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.0554 - acc: 1.0000\n",
      "Epoch 513/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0529 - acc: 1.0000\n",
      "Epoch 514/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.0519 - acc: 1.0000\n",
      "Epoch 515/600\n",
      "13/13 [==============================] - 0s 213us/step - loss: 0.0515 - acc: 1.0000\n",
      "Epoch 516/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.0517 - acc: 1.0000\n",
      "Epoch 517/600\n",
      "13/13 [==============================] - 0s 223us/step - loss: 0.0527 - acc: 1.0000\n",
      "Epoch 518/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0520 - acc: 1.0000\n",
      "Epoch 519/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.0501 - acc: 1.0000\n",
      "Epoch 520/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0497 - acc: 1.0000\n",
      "Epoch 521/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.0504 - acc: 1.0000\n",
      "Epoch 522/600\n",
      "13/13 [==============================] - 0s 208us/step - loss: 0.0506 - acc: 1.0000\n",
      "Epoch 523/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.0501 - acc: 1.0000\n",
      "Epoch 524/600\n",
      "13/13 [==============================] - 0s 220us/step - loss: 0.0485 - acc: 1.0000\n",
      "Epoch 525/600\n",
      "13/13 [==============================] - 0s 268us/step - loss: 0.0482 - acc: 1.0000\n",
      "Epoch 526/600\n",
      "13/13 [==============================] - 0s 284us/step - loss: 0.0487 - acc: 1.0000\n",
      "Epoch 527/600\n",
      "13/13 [==============================] - 0s 261us/step - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 528/600\n",
      "13/13 [==============================] - 0s 241us/step - loss: 0.0492 - acc: 1.0000\n",
      "Epoch 529/600\n",
      "13/13 [==============================] - 0s 274us/step - loss: 0.0472 - acc: 1.0000\n",
      "Epoch 530/600\n",
      "13/13 [==============================] - 0s 273us/step - loss: 0.0464 - acc: 1.0000\n",
      "Epoch 531/600\n",
      "13/13 [==============================] - 0s 297us/step - loss: 0.0466 - acc: 1.0000\n",
      "Epoch 532/600\n",
      "13/13 [==============================] - 0s 282us/step - loss: 0.0465 - acc: 1.0000\n",
      "Epoch 533/600\n",
      "13/13 [==============================] - 0s 277us/step - loss: 0.0462 - acc: 1.0000\n",
      "Epoch 534/600\n",
      "13/13 [==============================] - 0s 289us/step - loss: 0.0456 - acc: 1.0000\n",
      "Epoch 535/600\n",
      "13/13 [==============================] - 0s 261us/step - loss: 0.0450 - acc: 1.0000\n",
      "Epoch 536/600\n",
      "13/13 [==============================] - 0s 210us/step - loss: 0.0449 - acc: 1.0000\n",
      "Epoch 537/600\n",
      "13/13 [==============================] - 0s 245us/step - loss: 0.0459 - acc: 1.0000\n",
      "Epoch 538/600\n",
      "13/13 [==============================] - 0s 237us/step - loss: 0.0457 - acc: 1.0000\n",
      "Epoch 539/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.0439 - acc: 1.0000\n",
      "Epoch 540/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0437 - acc: 1.0000\n",
      "Epoch 541/600\n",
      "13/13 [==============================] - 0s 219us/step - loss: 0.0442 - acc: 1.0000\n",
      "Epoch 542/600\n",
      "13/13 [==============================] - 0s 249us/step - loss: 0.0435 - acc: 1.0000\n",
      "Epoch 543/600\n",
      "13/13 [==============================] - 0s 238us/step - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 544/600\n",
      "13/13 [==============================] - 0s 221us/step - loss: 0.0421 - acc: 1.0000\n",
      "Epoch 545/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 0.0418 - acc: 1.0000\n",
      "Epoch 546/600\n",
      "13/13 [==============================] - 0s 238us/step - loss: 0.0415 - acc: 1.0000\n",
      "Epoch 547/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.0412 - acc: 1.0000\n",
      "Epoch 548/600\n",
      "13/13 [==============================] - 0s 229us/step - loss: 0.0407 - acc: 1.0000\n",
      "Epoch 549/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0406 - acc: 1.0000\n",
      "Epoch 550/600\n",
      "13/13 [==============================] - 0s 226us/step - loss: 0.0404 - acc: 1.0000\n",
      "Epoch 551/600\n",
      "13/13 [==============================] - 0s 246us/step - loss: 0.0403 - acc: 1.0000\n",
      "Epoch 552/600\n",
      "13/13 [==============================] - 0s 256us/step - loss: 0.0415 - acc: 1.0000\n",
      "Epoch 553/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0416 - acc: 1.0000\n",
      "Epoch 554/600\n",
      "13/13 [==============================] - 0s 181us/step - loss: 0.0400 - acc: 1.0000\n",
      "Epoch 555/600\n",
      "13/13 [==============================] - 0s 232us/step - loss: 0.0394 - acc: 1.0000\n",
      "Epoch 556/600\n",
      "13/13 [==============================] - 0s 281us/step - loss: 0.0390 - acc: 1.0000\n",
      "Epoch 557/600\n",
      "13/13 [==============================] - 0s 244us/step - loss: 0.0389 - acc: 1.0000\n",
      "Epoch 558/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 559/600\n",
      "13/13 [==============================] - 0s 224us/step - loss: 0.0383 - acc: 1.0000\n",
      "Epoch 560/600\n",
      "13/13 [==============================] - 0s 194us/step - loss: 0.0378 - acc: 1.0000\n",
      "Epoch 561/600\n",
      "13/13 [==============================] - 0s 211us/step - loss: 0.0377 - acc: 1.0000\n",
      "Epoch 562/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.0372 - acc: 1.0000\n",
      "Epoch 563/600\n",
      "13/13 [==============================] - 0s 204us/step - loss: 0.0367 - acc: 1.0000\n",
      "Epoch 564/600\n",
      "13/13 [==============================] - 0s 295us/step - loss: 0.0366 - acc: 1.0000\n",
      "Epoch 565/600\n",
      "13/13 [==============================] - 0s 273us/step - loss: 0.0365 - acc: 1.0000\n",
      "Epoch 566/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.0363 - acc: 1.0000\n",
      "Epoch 567/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.0363 - acc: 1.0000\n",
      "Epoch 568/600\n",
      "13/13 [==============================] - 0s 236us/step - loss: 0.0361 - acc: 1.0000\n",
      "Epoch 569/600\n",
      "13/13 [==============================] - 0s 192us/step - loss: 0.0369 - acc: 1.0000\n",
      "Epoch 570/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.0360 - acc: 1.0000\n",
      "Epoch 571/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.0354 - acc: 1.0000\n",
      "Epoch 572/600\n",
      "13/13 [==============================] - 0s 225us/step - loss: 0.0347 - acc: 1.0000\n",
      "Epoch 573/600\n",
      "13/13 [==============================] - 0s 246us/step - loss: 0.0347 - acc: 1.0000\n",
      "Epoch 574/600\n",
      "13/13 [==============================] - 0s 231us/step - loss: 0.0350 - acc: 1.0000\n",
      "Epoch 575/600\n",
      "13/13 [==============================] - 0s 194us/step - loss: 0.0346 - acc: 1.0000\n",
      "Epoch 576/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 577/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0336 - acc: 1.0000\n",
      "Epoch 578/600\n",
      "13/13 [==============================] - 0s 188us/step - loss: 0.0334 - acc: 1.0000\n",
      "Epoch 579/600\n",
      "13/13 [==============================] - 0s 193us/step - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 580/600\n",
      "13/13 [==============================] - 0s 177us/step - loss: 0.0330 - acc: 1.0000\n",
      "Epoch 581/600\n",
      "13/13 [==============================] - 0s 174us/step - loss: 0.0328 - acc: 1.0000\n",
      "Epoch 582/600\n",
      "13/13 [==============================] - 0s 200us/step - loss: 0.0324 - acc: 1.0000\n",
      "Epoch 583/600\n",
      "13/13 [==============================] - 0s 197us/step - loss: 0.0323 - acc: 1.0000\n",
      "Epoch 584/600\n",
      "13/13 [==============================] - 0s 214us/step - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 585/600\n",
      "13/13 [==============================] - 0s 222us/step - loss: 0.0322 - acc: 1.0000\n",
      "Epoch 586/600\n",
      "13/13 [==============================] - 0s 215us/step - loss: 0.0324 - acc: 1.0000\n",
      "Epoch 587/600\n",
      "13/13 [==============================] - 0s 187us/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 588/600\n",
      "13/13 [==============================] - 0s 205us/step - loss: 0.0313 - acc: 1.0000\n",
      "Epoch 589/600\n",
      "13/13 [==============================] - 0s 196us/step - loss: 0.0311 - acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/600\n",
      "13/13 [==============================] - 0s 228us/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 591/600\n",
      "13/13 [==============================] - 0s 198us/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 592/600\n",
      "13/13 [==============================] - 0s 183us/step - loss: 0.0308 - acc: 1.0000\n",
      "Epoch 593/600\n",
      "13/13 [==============================] - 0s 230us/step - loss: 0.0315 - acc: 1.0000\n",
      "Epoch 594/600\n",
      "13/13 [==============================] - 0s 209us/step - loss: 0.0313 - acc: 1.0000\n",
      "Epoch 595/600\n",
      "13/13 [==============================] - 0s 237us/step - loss: 0.0301 - acc: 1.0000\n",
      "Epoch 596/600\n",
      "13/13 [==============================] - 0s 227us/step - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 597/600\n",
      "13/13 [==============================] - 0s 217us/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 598/600\n",
      "13/13 [==============================] - 0s 265us/step - loss: 0.0299 - acc: 1.0000\n",
      "Epoch 599/600\n",
      "13/13 [==============================] - 0s 216us/step - loss: 0.0296 - acc: 1.0000\n",
      "Epoch 600/600\n",
      "13/13 [==============================] - 0s 201us/step - loss: 0.0289 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a3a32db70>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = 'IAMREALLYGOOD'\n",
    "text, x_train, y_train = caeserde(mystr, x_as_vector=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: IAMREALLYGOOD\n",
      "Cipertext: LDPUHDOOBJRRG\n",
      "Prediction: IAMREALLYGOOD\n"
     ]
    }
   ],
   "source": [
    "#from keras.utils import to_categorical\n",
    "#categorical_labels = to_categorical(y_train, num_classes=None)\n",
    "\n",
    "print(\"Original:\",mystr)\n",
    "print(\"Cipertext:\",text)\n",
    "print(\"Prediction:\",\"\".join(predict_results(model, x_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "text, x_train, y_train = caeserde('ABCDEFGHIJKLMNOPQRSTUVWXYZ', x_as_vector=False, y_as_vector=False)\n",
    "\n",
    "predictions = model.predict_classes(x_train)\n",
    "\n",
    "#print(confusion_matrix(y_train, predictions.astype(int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a3afebb38>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = confusion_matrix(y_train, predictions.astype(int))\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning structure 3: 26 inputs vs one output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, x_train, y_train = caeserde(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",y_as_vector=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "# change sigomiod to softmax made acc increasing 100%\n",
    "model.add(Dense(1, activation='relu'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='mse', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 210.1044 - acc: 0.0385\n",
      "Epoch 2/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 203.2985 - acc: 0.0385\n",
      "Epoch 3/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 193.5643 - acc: 0.0385\n",
      "Epoch 4/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 183.4333 - acc: 0.0385\n",
      "Epoch 5/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 172.1029 - acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 159.0789 - acc: 0.0385\n",
      "Epoch 7/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 144.2473 - acc: 0.0769\n",
      "Epoch 8/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 127.5008 - acc: 0.0385\n",
      "Epoch 9/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 109.4971 - acc: 0.0769\n",
      "Epoch 10/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 91.8371 - acc: 0.0385\n",
      "Epoch 11/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 76.4462 - acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 64.7815 - acc: 0.0385\n",
      "Epoch 13/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 57.1897 - acc: 0.0385\n",
      "Epoch 14/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 52.7971 - acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 50.3369 - acc: 0.0385\n",
      "Epoch 16/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 48.8193 - acc: 0.0769\n",
      "Epoch 17/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 47.6967 - acc: 0.0769\n",
      "Epoch 18/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 46.7233 - acc: 0.1154\n",
      "Epoch 19/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 45.8011 - acc: 0.1154\n",
      "Epoch 20/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 44.8990 - acc: 0.1154\n",
      "Epoch 21/300\n",
      "26/26 [==============================] - 0s 51us/step - loss: 44.0055 - acc: 0.0385\n",
      "Epoch 22/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 43.1147 - acc: 0.0385\n",
      "Epoch 23/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 42.2243 - acc: 0.0385\n",
      "Epoch 24/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 41.3327 - acc: 0.0385\n",
      "Epoch 25/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 40.4401 - acc: 0.0385\n",
      "Epoch 26/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 39.5454 - acc: 0.0385\n",
      "Epoch 27/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 38.6488 - acc: 0.0385\n",
      "Epoch 28/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 37.7495 - acc: 0.0385\n",
      "Epoch 29/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 36.8465 - acc: 0.0385\n",
      "Epoch 30/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 35.9403 - acc: 0.0385\n",
      "Epoch 31/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 35.0318 - acc: 0.0385\n",
      "Epoch 32/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 34.1194 - acc: 0.0385\n",
      "Epoch 33/300\n",
      "26/26 [==============================] - 0s 46us/step - loss: 33.2041 - acc: 0.0769\n",
      "Epoch 34/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 32.2901 - acc: 0.0769\n",
      "Epoch 35/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 31.3725 - acc: 0.0769\n",
      "Epoch 36/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 30.4538 - acc: 0.0769\n",
      "Epoch 37/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 29.5324 - acc: 0.0769\n",
      "Epoch 38/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 28.6151 - acc: 0.0769\n",
      "Epoch 39/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 27.7006 - acc: 0.0769\n",
      "Epoch 40/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 26.7904 - acc: 0.0769\n",
      "Epoch 41/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 25.8775 - acc: 0.0769\n",
      "Epoch 42/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 24.9715 - acc: 0.0769\n",
      "Epoch 43/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 24.0755 - acc: 0.0769\n",
      "Epoch 44/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 23.1796 - acc: 0.0769\n",
      "Epoch 45/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 22.2880 - acc: 0.0769\n",
      "Epoch 46/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 21.4062 - acc: 0.0769\n",
      "Epoch 47/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 20.5326 - acc: 0.0769\n",
      "Epoch 48/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 19.6694 - acc: 0.0769\n",
      "Epoch 49/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 18.8151 - acc: 0.0769\n",
      "Epoch 50/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 17.9771 - acc: 0.0769\n",
      "Epoch 51/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 17.1491 - acc: 0.0769\n",
      "Epoch 52/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 16.3360 - acc: 0.0769\n",
      "Epoch 53/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 15.5429 - acc: 0.0769\n",
      "Epoch 54/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 14.7702 - acc: 0.0769\n",
      "Epoch 55/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 14.0059 - acc: 0.0769\n",
      "Epoch 56/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 13.2667 - acc: 0.0769\n",
      "Epoch 57/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 12.5535 - acc: 0.0769\n",
      "Epoch 58/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 11.8521 - acc: 0.0769\n",
      "Epoch 59/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 11.1819 - acc: 0.0769\n",
      "Epoch 60/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 10.5367 - acc: 0.0769\n",
      "Epoch 61/300\n",
      "26/26 [==============================] - 0s 42us/step - loss: 9.9097 - acc: 0.0769\n",
      "Epoch 62/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 9.3112 - acc: 0.0769\n",
      "Epoch 63/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 8.7450 - acc: 0.0769\n",
      "Epoch 64/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 8.2029 - acc: 0.0769\n",
      "Epoch 65/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 7.6818 - acc: 0.0769\n",
      "Epoch 66/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 7.1881 - acc: 0.0769\n",
      "Epoch 67/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 6.7212 - acc: 0.0769\n",
      "Epoch 68/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 6.2811 - acc: 0.0769\n",
      "Epoch 69/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 5.8594 - acc: 0.0769\n",
      "Epoch 70/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 5.4645 - acc: 0.1154\n",
      "Epoch 71/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 5.0928 - acc: 0.1154\n",
      "Epoch 72/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 4.7439 - acc: 0.1538\n",
      "Epoch 73/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 4.4102 - acc: 0.1538\n",
      "Epoch 74/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 4.0982 - acc: 0.1923\n",
      "Epoch 75/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 3.8046 - acc: 0.1923\n",
      "Epoch 76/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 3.5272 - acc: 0.1923\n",
      "Epoch 77/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 3.2666 - acc: 0.1923\n",
      "Epoch 78/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 3.0226 - acc: 0.1923\n",
      "Epoch 79/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 2.7995 - acc: 0.1923\n",
      "Epoch 80/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 2.5976 - acc: 0.1923\n",
      "Epoch 81/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 2.4088 - acc: 0.1923\n",
      "Epoch 82/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 2.2327 - acc: 0.1923\n",
      "Epoch 83/300\n",
      "26/26 [==============================] - 0s 55us/step - loss: 2.0685 - acc: 0.2308\n",
      "Epoch 84/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 1.9147 - acc: 0.2308\n",
      "Epoch 85/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 1.7713 - acc: 0.2308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 1.6386 - acc: 0.2308\n",
      "Epoch 87/300\n",
      "26/26 [==============================] - 0s 42us/step - loss: 1.5156 - acc: 0.2692\n",
      "Epoch 88/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 1.3998 - acc: 0.3077\n",
      "Epoch 89/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 1.2940 - acc: 0.3462\n",
      "Epoch 90/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 1.1962 - acc: 0.3462\n",
      "Epoch 91/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 1.1051 - acc: 0.3462\n",
      "Epoch 92/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 1.0207 - acc: 0.3462\n",
      "Epoch 93/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.9427 - acc: 0.3846\n",
      "Epoch 94/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.8703 - acc: 0.4231\n",
      "Epoch 95/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.8038 - acc: 0.4615\n",
      "Epoch 96/300\n",
      "26/26 [==============================] - 0s 42us/step - loss: 0.7417 - acc: 0.5000\n",
      "Epoch 97/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.6854 - acc: 0.5000\n",
      "Epoch 98/300\n",
      "26/26 [==============================] - 0s 39us/step - loss: 0.6336 - acc: 0.5000\n",
      "Epoch 99/300\n",
      "26/26 [==============================] - 0s 42us/step - loss: 0.5856 - acc: 0.5385\n",
      "Epoch 100/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.5418 - acc: 0.5385\n",
      "Epoch 101/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.5054 - acc: 0.5769\n",
      "Epoch 102/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.4717 - acc: 0.5769\n",
      "Epoch 103/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.4401 - acc: 0.6154\n",
      "Epoch 104/300\n",
      "26/26 [==============================] - 0s 26us/step - loss: 0.4130 - acc: 0.6538\n",
      "Epoch 105/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.3866 - acc: 0.6538\n",
      "Epoch 106/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.3631 - acc: 0.6538\n",
      "Epoch 107/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.3408 - acc: 0.6538\n",
      "Epoch 108/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.3198 - acc: 0.6538\n",
      "Epoch 109/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.3026 - acc: 0.6538\n",
      "Epoch 110/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.2835 - acc: 0.6538\n",
      "Epoch 111/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.2676 - acc: 0.6538\n",
      "Epoch 112/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.2530 - acc: 0.6538\n",
      "Epoch 113/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.2394 - acc: 0.6538\n",
      "Epoch 114/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 0.2271 - acc: 0.6923\n",
      "Epoch 115/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.2166 - acc: 0.6538\n",
      "Epoch 116/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.2046 - acc: 0.7692\n",
      "Epoch 117/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 0.1945 - acc: 0.7692\n",
      "Epoch 118/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.1854 - acc: 0.7692\n",
      "Epoch 119/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.1769 - acc: 0.7692\n",
      "Epoch 120/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.1689 - acc: 0.8077\n",
      "Epoch 121/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.8462\n",
      "Epoch 122/300\n",
      "26/26 [==============================] - 0s 39us/step - loss: 0.1549 - acc: 0.8462\n",
      "Epoch 123/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.1489 - acc: 0.8462\n",
      "Epoch 124/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.1435 - acc: 0.8462\n",
      "Epoch 125/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.1387 - acc: 0.8846\n",
      "Epoch 126/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.1336 - acc: 0.8846\n",
      "Epoch 127/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.1290 - acc: 0.8846\n",
      "Epoch 128/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.1248 - acc: 0.8846\n",
      "Epoch 129/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.1208 - acc: 0.8846\n",
      "Epoch 130/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.1173 - acc: 0.8846\n",
      "Epoch 131/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.1135 - acc: 0.8846\n",
      "Epoch 132/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.1100 - acc: 0.8846\n",
      "Epoch 133/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.1068 - acc: 0.8846\n",
      "Epoch 134/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.1037 - acc: 0.8846\n",
      "Epoch 135/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.1008 - acc: 0.8846\n",
      "Epoch 136/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0981 - acc: 0.8846\n",
      "Epoch 137/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0955 - acc: 0.8846\n",
      "Epoch 138/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0931 - acc: 0.8846\n",
      "Epoch 139/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0909 - acc: 0.8846\n",
      "Epoch 140/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0884 - acc: 0.8846\n",
      "Epoch 141/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 0.0862 - acc: 0.8846\n",
      "Epoch 142/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.0841 - acc: 0.8846\n",
      "Epoch 143/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0821 - acc: 0.8846\n",
      "Epoch 144/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0803 - acc: 0.8846\n",
      "Epoch 145/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0785 - acc: 0.8846\n",
      "Epoch 146/300\n",
      "26/26 [==============================] - 0s 43us/step - loss: 0.0767 - acc: 0.8846\n",
      "Epoch 147/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.0750 - acc: 0.8846\n",
      "Epoch 148/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0735 - acc: 0.9231\n",
      "Epoch 149/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0719 - acc: 0.9231\n",
      "Epoch 150/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0705 - acc: 0.9231\n",
      "Epoch 151/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0693 - acc: 0.9231\n",
      "Epoch 152/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0678 - acc: 0.9231\n",
      "Epoch 153/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0665 - acc: 0.9231\n",
      "Epoch 154/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0652 - acc: 0.9231\n",
      "Epoch 155/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0641 - acc: 0.9231\n",
      "Epoch 156/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0628 - acc: 0.9615\n",
      "Epoch 157/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0616 - acc: 0.9615\n",
      "Epoch 158/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0606 - acc: 0.9615\n",
      "Epoch 159/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0595 - acc: 0.9615\n",
      "Epoch 160/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0585 - acc: 0.9615\n",
      "Epoch 161/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0576 - acc: 0.9615\n",
      "Epoch 162/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0570 - acc: 0.9615\n",
      "Epoch 163/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0558 - acc: 0.9615\n",
      "Epoch 164/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0549 - acc: 0.9615\n",
      "Epoch 165/300\n",
      "26/26 [==============================] - 0s 43us/step - loss: 0.0539 - acc: 0.9615\n",
      "Epoch 166/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0530 - acc: 0.9615\n",
      "Epoch 167/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0522 - acc: 0.9615\n",
      "Epoch 168/300\n",
      "26/26 [==============================] - 0s 39us/step - loss: 0.0514 - acc: 0.9615\n",
      "Epoch 169/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0506 - acc: 0.9615\n",
      "Epoch 170/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0498 - acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0492 - acc: 0.9615\n",
      "Epoch 172/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0489 - acc: 0.9615\n",
      "Epoch 173/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0479 - acc: 0.9615\n",
      "Epoch 174/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0472 - acc: 0.9615\n",
      "Epoch 175/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0465 - acc: 0.9615\n",
      "Epoch 176/300\n",
      "26/26 [==============================] - 0s 51us/step - loss: 0.0459 - acc: 0.9615\n",
      "Epoch 177/300\n",
      "26/26 [==============================] - 0s 83us/step - loss: 0.0452 - acc: 0.9615\n",
      "Epoch 178/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0446 - acc: 0.9615\n",
      "Epoch 179/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0440 - acc: 0.9615\n",
      "Epoch 180/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0434 - acc: 0.9615\n",
      "Epoch 181/300\n",
      "26/26 [==============================] - 0s 44us/step - loss: 0.0429 - acc: 0.9615\n",
      "Epoch 182/300\n",
      "26/26 [==============================] - 0s 51us/step - loss: 0.0423 - acc: 0.9615\n",
      "Epoch 183/300\n",
      "26/26 [==============================] - 0s 91us/step - loss: 0.0418 - acc: 0.9615\n",
      "Epoch 184/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.0416 - acc: 0.9615\n",
      "Epoch 185/300\n",
      "26/26 [==============================] - 0s 44us/step - loss: 0.0409 - acc: 0.9615\n",
      "Epoch 186/300\n",
      "26/26 [==============================] - 0s 50us/step - loss: 0.0403 - acc: 0.9615\n",
      "Epoch 187/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 0.0398 - acc: 0.9615\n",
      "Epoch 188/300\n",
      "26/26 [==============================] - 0s 48us/step - loss: 0.0394 - acc: 0.9615\n",
      "Epoch 189/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0389 - acc: 0.9615\n",
      "Epoch 190/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0384 - acc: 0.9615\n",
      "Epoch 191/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 0.0379 - acc: 0.9615\n",
      "Epoch 192/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0375 - acc: 0.9615\n",
      "Epoch 193/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0371 - acc: 0.9615\n",
      "Epoch 194/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0366 - acc: 0.9615\n",
      "Epoch 195/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0362 - acc: 0.9615\n",
      "Epoch 196/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0358 - acc: 0.9615\n",
      "Epoch 197/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0355 - acc: 0.9615\n",
      "Epoch 198/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.0353 - acc: 0.9615\n",
      "Epoch 199/300\n",
      "26/26 [==============================] - 0s 49us/step - loss: 0.0347 - acc: 0.9615\n",
      "Epoch 200/300\n",
      "26/26 [==============================] - 0s 39us/step - loss: 0.0343 - acc: 0.9615\n",
      "Epoch 201/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0339 - acc: 0.9615\n",
      "Epoch 202/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0335 - acc: 0.9615\n",
      "Epoch 203/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0332 - acc: 0.9615\n",
      "Epoch 204/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0328 - acc: 0.9615\n",
      "Epoch 205/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0325 - acc: 0.9615\n",
      "Epoch 206/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0321 - acc: 0.9615\n",
      "Epoch 207/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0318 - acc: 0.9615\n",
      "Epoch 208/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0314 - acc: 0.9615\n",
      "Epoch 209/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0311 - acc: 0.9615\n",
      "Epoch 210/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0308 - acc: 0.9615\n",
      "Epoch 211/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.0308 - acc: 0.9615\n",
      "Epoch 212/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0303 - acc: 0.9615\n",
      "Epoch 213/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0300 - acc: 0.9615\n",
      "Epoch 214/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0296 - acc: 0.9615\n",
      "Epoch 215/300\n",
      "26/26 [==============================] - 0s 53us/step - loss: 0.0293 - acc: 0.9615\n",
      "Epoch 216/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0290 - acc: 0.9615\n",
      "Epoch 217/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 0.0287 - acc: 0.9615\n",
      "Epoch 218/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0285 - acc: 0.9615\n",
      "Epoch 219/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0282 - acc: 0.9615\n",
      "Epoch 220/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0279 - acc: 0.9615\n",
      "Epoch 221/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0276 - acc: 0.9615\n",
      "Epoch 222/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0274 - acc: 0.9615\n",
      "Epoch 223/300\n",
      "26/26 [==============================] - 0s 46us/step - loss: 0.0271 - acc: 0.9615\n",
      "Epoch 224/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 0.0269 - acc: 0.9615\n",
      "Epoch 225/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0267 - acc: 0.9615\n",
      "Epoch 226/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0266 - acc: 0.9615\n",
      "Epoch 227/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0262 - acc: 0.9615\n",
      "Epoch 228/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0260 - acc: 0.9615\n",
      "Epoch 229/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0257 - acc: 0.9615\n",
      "Epoch 230/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0254 - acc: 0.9615\n",
      "Epoch 231/300\n",
      "26/26 [==============================] - 0s 47us/step - loss: 0.0252 - acc: 0.9615\n",
      "Epoch 232/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0250 - acc: 0.9615\n",
      "Epoch 233/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0248 - acc: 0.9615\n",
      "Epoch 234/300\n",
      "26/26 [==============================] - 0s 26us/step - loss: 0.0245 - acc: 0.9615\n",
      "Epoch 235/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0243 - acc: 0.9615\n",
      "Epoch 236/300\n",
      "26/26 [==============================] - 0s 27us/step - loss: 0.0241 - acc: 0.9615\n",
      "Epoch 237/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0239 - acc: 0.9615\n",
      "Epoch 238/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0237 - acc: 0.9615\n",
      "Epoch 239/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0235 - acc: 0.9615\n",
      "Epoch 240/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0235 - acc: 0.9615\n",
      "Epoch 241/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0231 - acc: 0.9615\n",
      "Epoch 242/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0229 - acc: 0.9615\n",
      "Epoch 243/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0227 - acc: 0.9615\n",
      "Epoch 244/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0225 - acc: 0.9615\n",
      "Epoch 245/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0223 - acc: 0.9615\n",
      "Epoch 246/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0221 - acc: 0.9615\n",
      "Epoch 247/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0220 - acc: 0.9615\n",
      "Epoch 248/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0218 - acc: 0.9615\n",
      "Epoch 249/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0216 - acc: 0.9615\n",
      "Epoch 250/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0214 - acc: 0.9615\n",
      "Epoch 251/300\n",
      "26/26 [==============================] - 0s 47us/step - loss: 0.0212 - acc: 0.9615\n",
      "Epoch 252/300\n",
      "26/26 [==============================] - 0s 47us/step - loss: 0.0211 - acc: 0.9615\n",
      "Epoch 253/300\n",
      "26/26 [==============================] - 0s 44us/step - loss: 0.0209 - acc: 0.9615\n",
      "Epoch 254/300\n",
      "26/26 [==============================] - 0s 58us/step - loss: 0.0207 - acc: 0.9615\n",
      "Epoch 255/300\n",
      "26/26 [==============================] - 0s 29us/step - loss: 0.0205 - acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 256/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0204 - acc: 0.9615\n",
      "Epoch 257/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0204 - acc: 0.9615\n",
      "Epoch 258/300\n",
      "26/26 [==============================] - 0s 46us/step - loss: 0.0201 - acc: 0.9615\n",
      "Epoch 259/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.0199 - acc: 0.9615\n",
      "Epoch 260/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0198 - acc: 0.9615\n",
      "Epoch 261/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0196 - acc: 0.9615\n",
      "Epoch 262/300\n",
      "26/26 [==============================] - 0s 49us/step - loss: 0.0195 - acc: 0.9615\n",
      "Epoch 263/300\n",
      "26/26 [==============================] - 0s 45us/step - loss: 0.0193 - acc: 0.9615\n",
      "Epoch 264/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0191 - acc: 0.9615\n",
      "Epoch 265/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.0190 - acc: 0.9615\n",
      "Epoch 266/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0188 - acc: 0.9615\n",
      "Epoch 267/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0187 - acc: 0.9615\n",
      "Epoch 268/300\n",
      "26/26 [==============================] - 0s 43us/step - loss: 0.0186 - acc: 0.9615\n",
      "Epoch 269/300\n",
      "26/26 [==============================] - 0s 54us/step - loss: 0.0184 - acc: 0.9615\n",
      "Epoch 270/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0183 - acc: 0.9615\n",
      "Epoch 271/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0181 - acc: 0.9615\n",
      "Epoch 272/300\n",
      "26/26 [==============================] - 0s 31us/step - loss: 0.0180 - acc: 0.9615\n",
      "Epoch 273/300\n",
      "26/26 [==============================] - 0s 28us/step - loss: 0.0178 - acc: 0.9615\n",
      "Epoch 274/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0177 - acc: 0.9615\n",
      "Epoch 275/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.0176 - acc: 0.9615\n",
      "Epoch 276/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0176 - acc: 0.9615\n",
      "Epoch 277/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0174 - acc: 0.9615\n",
      "Epoch 278/300\n",
      "26/26 [==============================] - 0s 33us/step - loss: 0.0172 - acc: 0.9615\n",
      "Epoch 279/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0171 - acc: 0.9615\n",
      "Epoch 280/300\n",
      "26/26 [==============================] - 0s 30us/step - loss: 0.0170 - acc: 0.9615\n",
      "Epoch 281/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.0168 - acc: 0.9615\n",
      "Epoch 282/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0167 - acc: 0.9615\n",
      "Epoch 283/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0166 - acc: 0.9615\n",
      "Epoch 284/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0165 - acc: 0.9615\n",
      "Epoch 285/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0163 - acc: 0.9615\n",
      "Epoch 286/300\n",
      "26/26 [==============================] - 0s 46us/step - loss: 0.0162 - acc: 0.9615\n",
      "Epoch 287/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.0161 - acc: 0.9615\n",
      "Epoch 288/300\n",
      "26/26 [==============================] - 0s 39us/step - loss: 0.0160 - acc: 0.9615\n",
      "Epoch 289/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 0.0159 - acc: 0.9615\n",
      "Epoch 290/300\n",
      "26/26 [==============================] - 0s 59us/step - loss: 0.0157 - acc: 0.9615\n",
      "Epoch 291/300\n",
      "26/26 [==============================] - 0s 41us/step - loss: 0.0156 - acc: 0.9615\n",
      "Epoch 292/300\n",
      "26/26 [==============================] - 0s 36us/step - loss: 0.0155 - acc: 0.9615\n",
      "Epoch 293/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0154 - acc: 0.9615\n",
      "Epoch 294/300\n",
      "26/26 [==============================] - 0s 34us/step - loss: 0.0153 - acc: 0.9615\n",
      "Epoch 295/300\n",
      "26/26 [==============================] - 0s 32us/step - loss: 0.0152 - acc: 0.9615\n",
      "Epoch 296/300\n",
      "26/26 [==============================] - 0s 40us/step - loss: 0.0151 - acc: 0.9615\n",
      "Epoch 297/300\n",
      "26/26 [==============================] - 0s 38us/step - loss: 0.0151 - acc: 0.9615\n",
      "Epoch 298/300\n",
      "26/26 [==============================] - 0s 37us/step - loss: 0.0149 - acc: 0.9615\n",
      "Epoch 299/300\n",
      "26/26 [==============================] - 0s 63us/step - loss: 0.0148 - acc: 0.9615\n",
      "Epoch 300/300\n",
      "26/26 [==============================] - 0s 35us/step - loss: 0.0147 - acc: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a276cb438>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return (I2L[index])\n",
    "\n",
    "def predict_results(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index = np.argmax(predictions, axis=1)\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index))\n",
    "\n",
    "    return (prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6133307],\n",
       "       [ 1.0241398],\n",
       "       [ 2.0203376],\n",
       "       [ 2.9775467],\n",
       "       [ 3.9872074],\n",
       "       [ 4.9894595],\n",
       "       [ 6.002884 ],\n",
       "       [ 6.9982224],\n",
       "       [ 7.9920573],\n",
       "       [ 8.9853735],\n",
       "       [ 9.990207 ],\n",
       "       [10.994754 ],\n",
       "       [11.999034 ],\n",
       "       [12.999539 ],\n",
       "       [14.004254 ],\n",
       "       [15.004528 ],\n",
       "       [16.002241 ],\n",
       "       [16.99794  ],\n",
       "       [18.00107  ],\n",
       "       [19.001461 ],\n",
       "       [19.996624 ],\n",
       "       [21.00439  ],\n",
       "       [22.00569  ],\n",
       "       [23.001873 ],\n",
       "       [24.000912 ],\n",
       "       [25.003265 ]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the prediction is quite good compared with the first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: find out why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a28210470>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#print(confusion_matrix(y_train, np.rint(model.predict(np.array(x_train)/25))))\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = confusion_matrix(y_train, np.rint(model.predict(x_train)))\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 20)                540       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 561\n",
      "Trainable params: 561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Although the model just has 2 layers, we can still get good prediction results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning structure 4: 26 inputs vs 26 outputs (as a classification problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, x_train, y_train = caeserde(\"ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: 26 y_train: 26\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train:\",x_train.shape[1],\"y_train:\",y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "# change sigomiod to softmax made acc increasing 100%\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "52/52 [==============================] - 0s 5ms/step - loss: 0.1640 - acc: 0.9615\n",
      "Epoch 2/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1638 - acc: 0.9615\n",
      "Epoch 3/300\n",
      "52/52 [==============================] - 0s 67us/step - loss: 0.1635 - acc: 0.9615\n",
      "Epoch 4/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1632 - acc: 0.9615\n",
      "Epoch 5/300\n",
      "52/52 [==============================] - 0s 67us/step - loss: 0.1629 - acc: 0.9615\n",
      "Epoch 6/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 7/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 8/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 9/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 10/300\n",
      "52/52 [==============================] - 0s 89us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 11/300\n",
      "52/52 [==============================] - 0s 70us/step - loss: 0.1613 - acc: 0.9615\n",
      "Epoch 12/300\n",
      "52/52 [==============================] - 0s 80us/step - loss: 0.1610 - acc: 0.9615\n",
      "Epoch 13/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1608 - acc: 0.9615\n",
      "Epoch 14/300\n",
      "52/52 [==============================] - 0s 70us/step - loss: 0.1605 - acc: 0.9615\n",
      "Epoch 15/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.1602 - acc: 0.9615\n",
      "Epoch 16/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1600 - acc: 0.9615\n",
      "Epoch 17/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1597 - acc: 0.9615\n",
      "Epoch 18/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1594 - acc: 0.9615\n",
      "Epoch 19/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1592 - acc: 0.9615\n",
      "Epoch 20/300\n",
      "52/52 [==============================] - 0s 69us/step - loss: 0.1589 - acc: 0.9615\n",
      "Epoch 21/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.1586 - acc: 0.9615\n",
      "Epoch 22/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.1583 - acc: 0.9615\n",
      "Epoch 23/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1581 - acc: 0.9615\n",
      "Epoch 24/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1578 - acc: 0.9615\n",
      "Epoch 25/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1575 - acc: 0.9615\n",
      "Epoch 26/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.1572 - acc: 0.9615\n",
      "Epoch 27/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.1569 - acc: 0.9615\n",
      "Epoch 28/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1567 - acc: 0.9615\n",
      "Epoch 29/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1564 - acc: 0.9615\n",
      "Epoch 30/300\n",
      "52/52 [==============================] - 0s 76us/step - loss: 0.1561 - acc: 0.9615\n",
      "Epoch 31/300\n",
      "52/52 [==============================] - 0s 80us/step - loss: 0.1558 - acc: 0.9615\n",
      "Epoch 32/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.1555 - acc: 0.9615\n",
      "Epoch 33/300\n",
      "52/52 [==============================] - 0s 70us/step - loss: 0.1552 - acc: 0.9615\n",
      "Epoch 34/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1549 - acc: 0.9615\n",
      "Epoch 35/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1546 - acc: 0.9615\n",
      "Epoch 36/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1543 - acc: 0.9615\n",
      "Epoch 37/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1540 - acc: 0.9615\n",
      "Epoch 38/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1537 - acc: 0.9615\n",
      "Epoch 39/300\n",
      "52/52 [==============================] - 0s 82us/step - loss: 0.1534 - acc: 0.9615\n",
      "Epoch 40/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1530 - acc: 0.9615\n",
      "Epoch 41/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1527 - acc: 0.9615\n",
      "Epoch 42/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1524 - acc: 0.9615\n",
      "Epoch 43/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.1521 - acc: 0.9615\n",
      "Epoch 44/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1517 - acc: 0.9615\n",
      "Epoch 45/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1514 - acc: 0.9615\n",
      "Epoch 46/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.1510 - acc: 0.9615\n",
      "Epoch 47/300\n",
      "52/52 [==============================] - 0s 69us/step - loss: 0.1507 - acc: 0.9615\n",
      "Epoch 48/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.1503 - acc: 0.9615\n",
      "Epoch 49/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.1500 - acc: 0.9615\n",
      "Epoch 50/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1496 - acc: 0.9615\n",
      "Epoch 51/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1493 - acc: 0.9615\n",
      "Epoch 52/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1489 - acc: 0.9615\n",
      "Epoch 53/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1485 - acc: 0.9615\n",
      "Epoch 54/300\n",
      "52/52 [==============================] - 0s 77us/step - loss: 0.1481 - acc: 0.9615\n",
      "Epoch 55/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1478 - acc: 0.9615\n",
      "Epoch 56/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1474 - acc: 0.9615\n",
      "Epoch 57/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1470 - acc: 0.9615\n",
      "Epoch 58/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1466 - acc: 0.9615\n",
      "Epoch 59/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1462 - acc: 0.9615\n",
      "Epoch 60/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1458 - acc: 0.9615\n",
      "Epoch 61/300\n",
      "52/52 [==============================] - 0s 66us/step - loss: 0.1454 - acc: 0.9615\n",
      "Epoch 62/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1449 - acc: 0.9615\n",
      "Epoch 63/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1445 - acc: 0.9615\n",
      "Epoch 64/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.1441 - acc: 0.9615\n",
      "Epoch 65/300\n",
      "52/52 [==============================] - 0s 72us/step - loss: 0.1437 - acc: 0.9615\n",
      "Epoch 66/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.1433 - acc: 0.9615\n",
      "Epoch 67/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1428 - acc: 0.9615\n",
      "Epoch 68/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.1424 - acc: 0.9615\n",
      "Epoch 69/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1419 - acc: 0.9615\n",
      "Epoch 70/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.1415 - acc: 0.9615\n",
      "Epoch 71/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.1410 - acc: 0.9615\n",
      "Epoch 72/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1405 - acc: 0.9615\n",
      "Epoch 73/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1401 - acc: 0.9615\n",
      "Epoch 74/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.1396 - acc: 0.9615\n",
      "Epoch 75/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1391 - acc: 0.9615\n",
      "Epoch 76/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.1386 - acc: 0.9615\n",
      "Epoch 77/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.1381 - acc: 0.9615\n",
      "Epoch 78/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1376 - acc: 0.9615\n",
      "Epoch 79/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1371 - acc: 0.9615\n",
      "Epoch 80/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.1366 - acc: 0.9615\n",
      "Epoch 81/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1361 - acc: 0.9615\n",
      "Epoch 82/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.1356 - acc: 0.9615\n",
      "Epoch 83/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1351 - acc: 0.9615\n",
      "Epoch 84/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1346 - acc: 0.9615\n",
      "Epoch 85/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1340 - acc: 0.9615\n",
      "Epoch 86/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 54us/step - loss: 0.1335 - acc: 0.9615\n",
      "Epoch 87/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.1330 - acc: 0.9615\n",
      "Epoch 88/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.1324 - acc: 0.9615\n",
      "Epoch 89/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1319 - acc: 0.9615\n",
      "Epoch 90/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.1313 - acc: 0.9615\n",
      "Epoch 91/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.1307 - acc: 0.9615\n",
      "Epoch 92/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1302 - acc: 0.9615\n",
      "Epoch 93/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1296 - acc: 0.9615\n",
      "Epoch 94/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1291 - acc: 0.9615\n",
      "Epoch 95/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1285 - acc: 0.9615\n",
      "Epoch 96/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.1279 - acc: 0.9615\n",
      "Epoch 97/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.1273 - acc: 0.9615\n",
      "Epoch 98/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1267 - acc: 0.9615\n",
      "Epoch 99/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1261 - acc: 0.9615\n",
      "Epoch 100/300\n",
      "52/52 [==============================] - 0s 80us/step - loss: 0.1255 - acc: 0.9615\n",
      "Epoch 101/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.1249 - acc: 0.9615\n",
      "Epoch 102/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.1243 - acc: 0.9615\n",
      "Epoch 103/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.1237 - acc: 0.9615\n",
      "Epoch 104/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1231 - acc: 0.9615\n",
      "Epoch 105/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.1224 - acc: 0.9615\n",
      "Epoch 106/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.1218 - acc: 0.9615\n",
      "Epoch 107/300\n",
      "52/52 [==============================] - 0s 47us/step - loss: 0.1212 - acc: 0.9615\n",
      "Epoch 108/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.1205 - acc: 0.9615\n",
      "Epoch 109/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1198 - acc: 0.9615\n",
      "Epoch 110/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1192 - acc: 0.9615\n",
      "Epoch 111/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.1185 - acc: 0.9615\n",
      "Epoch 112/300\n",
      "52/52 [==============================] - 0s 47us/step - loss: 0.1179 - acc: 0.9615\n",
      "Epoch 113/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1172 - acc: 0.9615\n",
      "Epoch 114/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.1165 - acc: 0.9615\n",
      "Epoch 115/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1158 - acc: 0.9615\n",
      "Epoch 116/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1152 - acc: 0.9615\n",
      "Epoch 117/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1145 - acc: 0.9615\n",
      "Epoch 118/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1138 - acc: 0.9615\n",
      "Epoch 119/300\n",
      "52/52 [==============================] - 0s 72us/step - loss: 0.1131 - acc: 0.9615\n",
      "Epoch 120/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.1124 - acc: 0.9615\n",
      "Epoch 121/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1117 - acc: 0.9615\n",
      "Epoch 122/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1110 - acc: 0.9615\n",
      "Epoch 123/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.1103 - acc: 0.9615\n",
      "Epoch 124/300\n",
      "52/52 [==============================] - 0s 69us/step - loss: 0.1096 - acc: 0.9615\n",
      "Epoch 125/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.1089 - acc: 0.9615\n",
      "Epoch 126/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1082 - acc: 0.9615\n",
      "Epoch 127/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.1074 - acc: 0.9615\n",
      "Epoch 128/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.1067 - acc: 0.9615\n",
      "Epoch 129/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.1060 - acc: 0.9615\n",
      "Epoch 130/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.1052 - acc: 0.9615\n",
      "Epoch 131/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.1045 - acc: 0.9615\n",
      "Epoch 132/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.1038 - acc: 0.9630\n",
      "Epoch 133/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.1030 - acc: 0.9630\n",
      "Epoch 134/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.1023 - acc: 0.9630\n",
      "Epoch 135/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.1015 - acc: 0.9630\n",
      "Epoch 136/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.1008 - acc: 0.9630\n",
      "Epoch 137/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.1000 - acc: 0.9630\n",
      "Epoch 138/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0993 - acc: 0.9630\n",
      "Epoch 139/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0985 - acc: 0.9630\n",
      "Epoch 140/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0977 - acc: 0.9630\n",
      "Epoch 141/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.0970 - acc: 0.9630\n",
      "Epoch 142/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0962 - acc: 0.9630\n",
      "Epoch 143/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.0954 - acc: 0.9630\n",
      "Epoch 144/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.0947 - acc: 0.9630\n",
      "Epoch 145/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.0939 - acc: 0.9630\n",
      "Epoch 146/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.0931 - acc: 0.9630\n",
      "Epoch 147/300\n",
      "52/52 [==============================] - 0s 72us/step - loss: 0.0923 - acc: 0.9645\n",
      "Epoch 148/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0915 - acc: 0.9645\n",
      "Epoch 149/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0908 - acc: 0.9645\n",
      "Epoch 150/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0900 - acc: 0.9645\n",
      "Epoch 151/300\n",
      "52/52 [==============================] - 0s 66us/step - loss: 0.0892 - acc: 0.9645\n",
      "Epoch 152/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0884 - acc: 0.9645\n",
      "Epoch 153/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0876 - acc: 0.9645\n",
      "Epoch 154/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.0868 - acc: 0.9645\n",
      "Epoch 155/300\n",
      "52/52 [==============================] - 0s 66us/step - loss: 0.0860 - acc: 0.9645\n",
      "Epoch 156/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0852 - acc: 0.9645\n",
      "Epoch 157/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0844 - acc: 0.9645\n",
      "Epoch 158/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.0836 - acc: 0.9645\n",
      "Epoch 159/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0828 - acc: 0.9645\n",
      "Epoch 160/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0820 - acc: 0.9645\n",
      "Epoch 161/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0812 - acc: 0.9645\n",
      "Epoch 162/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0803 - acc: 0.9645\n",
      "Epoch 163/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0795 - acc: 0.9645\n",
      "Epoch 164/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0787 - acc: 0.9645\n",
      "Epoch 165/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.0779 - acc: 0.9645\n",
      "Epoch 166/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0771 - acc: 0.9645\n",
      "Epoch 167/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.0763 - acc: 0.9645\n",
      "Epoch 168/300\n",
      "52/52 [==============================] - 0s 80us/step - loss: 0.0755 - acc: 0.9645\n",
      "Epoch 169/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0747 - acc: 0.9645\n",
      "Epoch 170/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 70us/step - loss: 0.0739 - acc: 0.9660\n",
      "Epoch 171/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0731 - acc: 0.9660\n",
      "Epoch 172/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0722 - acc: 0.9675\n",
      "Epoch 173/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0714 - acc: 0.9675\n",
      "Epoch 174/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0706 - acc: 0.9675\n",
      "Epoch 175/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0698 - acc: 0.9675\n",
      "Epoch 176/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0690 - acc: 0.9689\n",
      "Epoch 177/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0682 - acc: 0.9689\n",
      "Epoch 178/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0674 - acc: 0.9689\n",
      "Epoch 179/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0666 - acc: 0.9689\n",
      "Epoch 180/300\n",
      "52/52 [==============================] - 0s 47us/step - loss: 0.0658 - acc: 0.9704\n",
      "Epoch 181/300\n",
      "52/52 [==============================] - 0s 46us/step - loss: 0.0650 - acc: 0.9719\n",
      "Epoch 182/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.0642 - acc: 0.9719\n",
      "Epoch 183/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0634 - acc: 0.9719\n",
      "Epoch 184/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0626 - acc: 0.9719\n",
      "Epoch 185/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0619 - acc: 0.9719\n",
      "Epoch 186/300\n",
      "52/52 [==============================] - 0s 47us/step - loss: 0.0611 - acc: 0.9719\n",
      "Epoch 187/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0603 - acc: 0.9719\n",
      "Epoch 188/300\n",
      "52/52 [==============================] - 0s 70us/step - loss: 0.0595 - acc: 0.9719\n",
      "Epoch 189/300\n",
      "52/52 [==============================] - 0s 80us/step - loss: 0.0587 - acc: 0.9719\n",
      "Epoch 190/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0580 - acc: 0.9734\n",
      "Epoch 191/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0572 - acc: 0.9734\n",
      "Epoch 192/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0565 - acc: 0.9734\n",
      "Epoch 193/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0557 - acc: 0.9734\n",
      "Epoch 194/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0549 - acc: 0.9734\n",
      "Epoch 195/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.0542 - acc: 0.9749\n",
      "Epoch 196/300\n",
      "52/52 [==============================] - 0s 73us/step - loss: 0.0534 - acc: 0.9749\n",
      "Epoch 197/300\n",
      "52/52 [==============================] - 0s 66us/step - loss: 0.0527 - acc: 0.9778\n",
      "Epoch 198/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0520 - acc: 0.9808\n",
      "Epoch 199/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0512 - acc: 0.9822\n",
      "Epoch 200/300\n",
      "52/52 [==============================] - 0s 66us/step - loss: 0.0505 - acc: 0.9822\n",
      "Epoch 201/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0498 - acc: 0.9822\n",
      "Epoch 202/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0491 - acc: 0.9822\n",
      "Epoch 203/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0483 - acc: 0.9822\n",
      "Epoch 204/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0477 - acc: 0.9822\n",
      "Epoch 205/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.0470 - acc: 0.9822\n",
      "Epoch 206/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0462 - acc: 0.9837\n",
      "Epoch 207/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0456 - acc: 0.9837\n",
      "Epoch 208/300\n",
      "52/52 [==============================] - 0s 76us/step - loss: 0.0449 - acc: 0.9837\n",
      "Epoch 209/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0442 - acc: 0.9837\n",
      "Epoch 210/300\n",
      "52/52 [==============================] - 0s 81us/step - loss: 0.0435 - acc: 0.9852\n",
      "Epoch 211/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0429 - acc: 0.9852\n",
      "Epoch 212/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.0422 - acc: 0.9852\n",
      "Epoch 213/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0415 - acc: 0.9852\n",
      "Epoch 214/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0409 - acc: 0.9867\n",
      "Epoch 215/300\n",
      "52/52 [==============================] - 0s 75us/step - loss: 0.0403 - acc: 0.9867\n",
      "Epoch 216/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.0396 - acc: 0.9882\n",
      "Epoch 217/300\n",
      "52/52 [==============================] - 0s 48us/step - loss: 0.0390 - acc: 0.9889\n",
      "Epoch 218/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0384 - acc: 0.9896\n",
      "Epoch 219/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0378 - acc: 0.9911\n",
      "Epoch 220/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0371 - acc: 0.9919\n",
      "Epoch 221/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0365 - acc: 0.9926\n",
      "Epoch 222/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0360 - acc: 0.9926\n",
      "Epoch 223/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0354 - acc: 0.9926\n",
      "Epoch 224/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0348 - acc: 0.9926\n",
      "Epoch 225/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.0342 - acc: 0.9926\n",
      "Epoch 226/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0336 - acc: 0.9926\n",
      "Epoch 227/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0331 - acc: 0.9926\n",
      "Epoch 228/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0325 - acc: 0.9926\n",
      "Epoch 229/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0320 - acc: 0.9941\n",
      "Epoch 230/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0315 - acc: 0.9941\n",
      "Epoch 231/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0309 - acc: 0.9941\n",
      "Epoch 232/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0304 - acc: 0.9963\n",
      "Epoch 233/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0299 - acc: 0.9985\n",
      "Epoch 234/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0294 - acc: 0.9985\n",
      "Epoch 235/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0289 - acc: 0.9985\n",
      "Epoch 236/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0284 - acc: 0.9985\n",
      "Epoch 237/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0279 - acc: 0.9985\n",
      "Epoch 238/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0274 - acc: 0.9985\n",
      "Epoch 239/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0269 - acc: 0.9985\n",
      "Epoch 240/300\n",
      "52/52 [==============================] - 0s 66us/step - loss: 0.0265 - acc: 0.9985\n",
      "Epoch 241/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0260 - acc: 0.9985\n",
      "Epoch 242/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0256 - acc: 0.9985\n",
      "Epoch 243/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0251 - acc: 0.9985\n",
      "Epoch 244/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0247 - acc: 0.9985\n",
      "Epoch 245/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0243 - acc: 0.9985\n",
      "Epoch 246/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0239 - acc: 0.9985\n",
      "Epoch 247/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0235 - acc: 0.9985\n",
      "Epoch 248/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0231 - acc: 0.9985\n",
      "Epoch 249/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0227 - acc: 0.9985\n",
      "Epoch 250/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0223 - acc: 0.9985\n",
      "Epoch 251/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0219 - acc: 0.9985\n",
      "Epoch 252/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0215 - acc: 0.9985\n",
      "Epoch 253/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0212 - acc: 0.9985\n",
      "Epoch 254/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 0s 72us/step - loss: 0.0208 - acc: 0.9985\n",
      "Epoch 255/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0205 - acc: 0.9985\n",
      "Epoch 256/300\n",
      "52/52 [==============================] - 0s 64us/step - loss: 0.0201 - acc: 0.9985\n",
      "Epoch 257/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0198 - acc: 0.9985\n",
      "Epoch 258/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0195 - acc: 0.9985\n",
      "Epoch 259/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0191 - acc: 0.9993\n",
      "Epoch 260/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 261/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 262/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 263/300\n",
      "52/52 [==============================] - 0s 52us/step - loss: 0.0179 - acc: 1.0000\n",
      "Epoch 264/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 265/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 266/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 267/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 268/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.0165 - acc: 1.0000\n",
      "Epoch 269/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 270/300\n",
      "52/52 [==============================] - 0s 65us/step - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 271/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 272/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 273/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 274/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 275/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 276/300\n",
      "52/52 [==============================] - 0s 50us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 277/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0143 - acc: 1.0000\n",
      "Epoch 278/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0141 - acc: 1.0000\n",
      "Epoch 279/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.0139 - acc: 1.0000\n",
      "Epoch 280/300\n",
      "52/52 [==============================] - 0s 51us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 281/300\n",
      "52/52 [==============================] - 0s 62us/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 282/300\n",
      "52/52 [==============================] - 0s 63us/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 283/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 284/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0129 - acc: 1.0000\n",
      "Epoch 285/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0127 - acc: 1.0000\n",
      "Epoch 286/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0125 - acc: 1.0000\n",
      "Epoch 287/300\n",
      "52/52 [==============================] - 0s 61us/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 288/300\n",
      "52/52 [==============================] - 0s 54us/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 289/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 290/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0118 - acc: 1.0000\n",
      "Epoch 291/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0116 - acc: 1.0000\n",
      "Epoch 292/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 293/300\n",
      "52/52 [==============================] - 0s 58us/step - loss: 0.0113 - acc: 1.0000\n",
      "Epoch 294/300\n",
      "52/52 [==============================] - 0s 60us/step - loss: 0.0112 - acc: 1.0000\n",
      "Epoch 295/300\n",
      "52/52 [==============================] - 0s 56us/step - loss: 0.0110 - acc: 1.0000\n",
      "Epoch 296/300\n",
      "52/52 [==============================] - 0s 55us/step - loss: 0.0108 - acc: 1.0000\n",
      "Epoch 297/300\n",
      "52/52 [==============================] - 0s 49us/step - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 298/300\n",
      "52/52 [==============================] - 0s 57us/step - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 299/300\n",
      "52/52 [==============================] - 0s 53us/step - loss: 0.0104 - acc: 1.0000\n",
      "Epoch 300/300\n",
      "52/52 [==============================] - 0s 59us/step - loss: 0.0103 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2816e048>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return (I2L[index])\n",
    "\n",
    "def predict_results(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index = np.argmax(predictions, axis=1)\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index))\n",
    "\n",
    "    return (prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystr = 'IAMREALLYGOOD'\n",
    "text, x_train, y_train = caeserde(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: IAMREALLYGOOD\n",
      "Cipertext: LDPUHDOOBJRRG\n",
      "Prediction: IAMREALLYGOOD\n"
     ]
    }
   ],
   "source": [
    "print(\"Original:\",mystr)\n",
    "print(\"Cipertext:\",text)\n",
    "print(\"Prediction:\",\"\".join(predict_results(model, x_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a29cdb0b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text, x_train, y_train = caeserde(\"ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ\", y_as_vector= False)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#print(confusion_matrix(y_train, np.rint(model.predict(np.array(x_train)/25))))\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = confusion_matrix(y_train, model.predict_classes(x_train))\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26 to 26 model has a best preformance for caeser decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Try to reduce it complixicity. (But it doesn't work, it will work for the position matrix input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=x_train.shape[1], activation='relu'))\n",
    "# change sigomiod to softmax made acc increasing 100%\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "13/13 [==============================] - 0s 16ms/step - loss: 0.6953 - acc: 0.5296\n",
      "Epoch 2/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.6947 - acc: 0.5325\n",
      "Epoch 3/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6939 - acc: 0.5325\n",
      "Epoch 4/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.6931 - acc: 0.5503\n",
      "Epoch 5/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.6922 - acc: 0.5503\n",
      "Epoch 6/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.6914 - acc: 0.5621\n",
      "Epoch 7/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.6906 - acc: 0.5680\n",
      "Epoch 8/300\n",
      "13/13 [==============================] - 0s 69us/step - loss: 0.6898 - acc: 0.5799\n",
      "Epoch 9/300\n",
      "13/13 [==============================] - 0s 102us/step - loss: 0.6890 - acc: 0.5976\n",
      "Epoch 10/300\n",
      "13/13 [==============================] - 0s 69us/step - loss: 0.6882 - acc: 0.6124\n",
      "Epoch 11/300\n",
      "13/13 [==============================] - 0s 91us/step - loss: 0.6874 - acc: 0.6243\n",
      "Epoch 12/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.6866 - acc: 0.6361\n",
      "Epoch 13/300\n",
      "13/13 [==============================] - 0s 91us/step - loss: 0.6858 - acc: 0.6450\n",
      "Epoch 14/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.6850 - acc: 0.6538\n",
      "Epoch 15/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.6842 - acc: 0.6538\n",
      "Epoch 16/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.6834 - acc: 0.6538\n",
      "Epoch 17/300\n",
      "13/13 [==============================] - 0s 94us/step - loss: 0.6826 - acc: 0.6657\n",
      "Epoch 18/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.6818 - acc: 0.6716\n",
      "Epoch 19/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.6810 - acc: 0.6775\n",
      "Epoch 20/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.6802 - acc: 0.6775\n",
      "Epoch 21/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6793 - acc: 0.6775\n",
      "Epoch 22/300\n",
      "13/13 [==============================] - 0s 88us/step - loss: 0.6785 - acc: 0.6805\n",
      "Epoch 23/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.6777 - acc: 0.6805\n",
      "Epoch 24/300\n",
      "13/13 [==============================] - 0s 106us/step - loss: 0.6769 - acc: 0.6923\n",
      "Epoch 25/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6761 - acc: 0.6923\n",
      "Epoch 26/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6753 - acc: 0.6893\n",
      "Epoch 27/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6745 - acc: 0.6893\n",
      "Epoch 28/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.6737 - acc: 0.6923\n",
      "Epoch 29/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6729 - acc: 0.7012\n",
      "Epoch 30/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6721 - acc: 0.7012\n",
      "Epoch 31/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.6713 - acc: 0.7071\n",
      "Epoch 32/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6704 - acc: 0.7160\n",
      "Epoch 33/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6696 - acc: 0.7160\n",
      "Epoch 34/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.6688 - acc: 0.7189\n",
      "Epoch 35/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6680 - acc: 0.7219\n",
      "Epoch 36/300\n",
      "13/13 [==============================] - 0s 91us/step - loss: 0.6672 - acc: 0.7219\n",
      "Epoch 37/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.6664 - acc: 0.7219\n",
      "Epoch 38/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6655 - acc: 0.7249\n",
      "Epoch 39/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.6647 - acc: 0.7249\n",
      "Epoch 40/300\n",
      "13/13 [==============================] - 0s 137us/step - loss: 0.6639 - acc: 0.7337\n",
      "Epoch 41/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.6630 - acc: 0.7396\n",
      "Epoch 42/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.6622 - acc: 0.7396\n",
      "Epoch 43/300\n",
      "13/13 [==============================] - 0s 92us/step - loss: 0.6614 - acc: 0.7396\n",
      "Epoch 44/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.6606 - acc: 0.7396\n",
      "Epoch 45/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.6597 - acc: 0.7426\n",
      "Epoch 46/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.6589 - acc: 0.7426\n",
      "Epoch 47/300\n",
      "13/13 [==============================] - 0s 90us/step - loss: 0.6580 - acc: 0.7456\n",
      "Epoch 48/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6572 - acc: 0.7456\n",
      "Epoch 49/300\n",
      "13/13 [==============================] - 0s 105us/step - loss: 0.6563 - acc: 0.7485\n",
      "Epoch 50/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.6555 - acc: 0.7485\n",
      "Epoch 51/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6546 - acc: 0.7633\n",
      "Epoch 52/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6538 - acc: 0.7633\n",
      "Epoch 53/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6529 - acc: 0.7781\n",
      "Epoch 54/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.6521 - acc: 0.7781\n",
      "Epoch 55/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.6512 - acc: 0.7781\n",
      "Epoch 56/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6504 - acc: 0.7811\n",
      "Epoch 57/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.6495 - acc: 0.7840\n",
      "Epoch 58/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6486 - acc: 0.7840\n",
      "Epoch 59/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.6478 - acc: 0.7840\n",
      "Epoch 60/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6469 - acc: 0.7840\n",
      "Epoch 61/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6460 - acc: 0.7840\n",
      "Epoch 62/300\n",
      "13/13 [==============================] - 0s 106us/step - loss: 0.6451 - acc: 0.7840\n",
      "Epoch 63/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6442 - acc: 0.7899\n",
      "Epoch 64/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.6433 - acc: 0.7899\n",
      "Epoch 65/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.6425 - acc: 0.7929\n",
      "Epoch 66/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.6416 - acc: 0.7929\n",
      "Epoch 67/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.6407 - acc: 0.7959\n",
      "Epoch 68/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6398 - acc: 0.7959\n",
      "Epoch 69/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.6389 - acc: 0.7959\n",
      "Epoch 70/300\n",
      "13/13 [==============================] - 0s 68us/step - loss: 0.6380 - acc: 0.8018\n",
      "Epoch 71/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.6371 - acc: 0.8018\n",
      "Epoch 72/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6361 - acc: 0.8018\n",
      "Epoch 73/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.6352 - acc: 0.8018\n",
      "Epoch 74/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.6343 - acc: 0.8018\n",
      "Epoch 75/300\n",
      "13/13 [==============================] - 0s 93us/step - loss: 0.6334 - acc: 0.8018\n",
      "Epoch 76/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.6325 - acc: 0.8018\n",
      "Epoch 77/300\n",
      "13/13 [==============================] - 0s 107us/step - loss: 0.6315 - acc: 0.8018\n",
      "Epoch 78/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.6306 - acc: 0.8077\n",
      "Epoch 79/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.6297 - acc: 0.8166\n",
      "Epoch 80/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.6287 - acc: 0.8166\n",
      "Epoch 81/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.6278 - acc: 0.8107\n",
      "Epoch 82/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.6268 - acc: 0.8107\n",
      "Epoch 83/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.6259 - acc: 0.8107\n",
      "Epoch 84/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6249 - acc: 0.8166\n",
      "Epoch 85/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6240 - acc: 0.8166\n",
      "Epoch 86/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.6230 - acc: 0.8166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.6220 - acc: 0.8166\n",
      "Epoch 88/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.6211 - acc: 0.8195\n",
      "Epoch 89/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.6201 - acc: 0.8195\n",
      "Epoch 90/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6191 - acc: 0.8254\n",
      "Epoch 91/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.6181 - acc: 0.8254\n",
      "Epoch 92/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.6171 - acc: 0.8254\n",
      "Epoch 93/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.6161 - acc: 0.8284\n",
      "Epoch 94/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.6152 - acc: 0.8284\n",
      "Epoch 95/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6142 - acc: 0.8284\n",
      "Epoch 96/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.6132 - acc: 0.8343\n",
      "Epoch 97/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.6121 - acc: 0.8343\n",
      "Epoch 98/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.6111 - acc: 0.8343\n",
      "Epoch 99/300\n",
      "13/13 [==============================] - 0s 69us/step - loss: 0.6101 - acc: 0.8343\n",
      "Epoch 100/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.6091 - acc: 0.8373\n",
      "Epoch 101/300\n",
      "13/13 [==============================] - 0s 67us/step - loss: 0.6081 - acc: 0.8373\n",
      "Epoch 102/300\n",
      "13/13 [==============================] - 0s 64us/step - loss: 0.6070 - acc: 0.8373\n",
      "Epoch 103/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.6060 - acc: 0.8432\n",
      "Epoch 104/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.6049 - acc: 0.8432\n",
      "Epoch 105/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.6039 - acc: 0.8432\n",
      "Epoch 106/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.6028 - acc: 0.8462\n",
      "Epoch 107/300\n",
      "13/13 [==============================] - 0s 88us/step - loss: 0.6017 - acc: 0.8462\n",
      "Epoch 108/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.6007 - acc: 0.8491\n",
      "Epoch 109/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.5996 - acc: 0.8491\n",
      "Epoch 110/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.5985 - acc: 0.8491\n",
      "Epoch 111/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.5974 - acc: 0.8491\n",
      "Epoch 112/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.5963 - acc: 0.8491\n",
      "Epoch 113/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.5952 - acc: 0.8491\n",
      "Epoch 114/300\n",
      "13/13 [==============================] - 0s 88us/step - loss: 0.5941 - acc: 0.8521\n",
      "Epoch 115/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.5929 - acc: 0.8609\n",
      "Epoch 116/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.5918 - acc: 0.8669\n",
      "Epoch 117/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.5907 - acc: 0.8669\n",
      "Epoch 118/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.5895 - acc: 0.8698\n",
      "Epoch 119/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.5884 - acc: 0.8757\n",
      "Epoch 120/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.5872 - acc: 0.8757\n",
      "Epoch 121/300\n",
      "13/13 [==============================] - 0s 69us/step - loss: 0.5861 - acc: 0.8787\n",
      "Epoch 122/300\n",
      "13/13 [==============================] - 0s 69us/step - loss: 0.5849 - acc: 0.8787\n",
      "Epoch 123/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.5837 - acc: 0.8817\n",
      "Epoch 124/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5826 - acc: 0.8876\n",
      "Epoch 125/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.5814 - acc: 0.8876\n",
      "Epoch 126/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5802 - acc: 0.8876\n",
      "Epoch 127/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.5790 - acc: 0.8905\n",
      "Epoch 128/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.5778 - acc: 0.8905\n",
      "Epoch 129/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.5766 - acc: 0.8905\n",
      "Epoch 130/300\n",
      "13/13 [==============================] - 0s 68us/step - loss: 0.5754 - acc: 0.8905\n",
      "Epoch 131/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.5742 - acc: 0.8964\n",
      "Epoch 132/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.5730 - acc: 0.8964\n",
      "Epoch 133/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.5718 - acc: 0.8964\n",
      "Epoch 134/300\n",
      "13/13 [==============================] - 0s 66us/step - loss: 0.5706 - acc: 0.8964\n",
      "Epoch 135/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.5694 - acc: 0.8964\n",
      "Epoch 136/300\n",
      "13/13 [==============================] - 0s 97us/step - loss: 0.5681 - acc: 0.8964\n",
      "Epoch 137/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.5669 - acc: 0.9053\n",
      "Epoch 138/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.5657 - acc: 0.9053\n",
      "Epoch 139/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5644 - acc: 0.9053\n",
      "Epoch 140/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.5632 - acc: 0.9112\n",
      "Epoch 141/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.5619 - acc: 0.9112\n",
      "Epoch 142/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5606 - acc: 0.9142\n",
      "Epoch 143/300\n",
      "13/13 [==============================] - 0s 145us/step - loss: 0.5594 - acc: 0.9142\n",
      "Epoch 144/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.5581 - acc: 0.9142\n",
      "Epoch 145/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.5568 - acc: 0.9142\n",
      "Epoch 146/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.5555 - acc: 0.9142\n",
      "Epoch 147/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.5542 - acc: 0.9142\n",
      "Epoch 148/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.5528 - acc: 0.9201\n",
      "Epoch 149/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.5515 - acc: 0.9201\n",
      "Epoch 150/300\n",
      "13/13 [==============================] - 0s 109us/step - loss: 0.5502 - acc: 0.9201\n",
      "Epoch 151/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.5488 - acc: 0.9231\n",
      "Epoch 152/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.5475 - acc: 0.9260\n",
      "Epoch 153/300\n",
      "13/13 [==============================] - 0s 68us/step - loss: 0.5461 - acc: 0.9290\n",
      "Epoch 154/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.5448 - acc: 0.9349\n",
      "Epoch 155/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.5434 - acc: 0.9349\n",
      "Epoch 156/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.5421 - acc: 0.9349\n",
      "Epoch 157/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.5407 - acc: 0.9379\n",
      "Epoch 158/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.5393 - acc: 0.9379\n",
      "Epoch 159/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.5379 - acc: 0.9438\n",
      "Epoch 160/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5365 - acc: 0.9438\n",
      "Epoch 161/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.5351 - acc: 0.9438\n",
      "Epoch 162/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.5337 - acc: 0.9438\n",
      "Epoch 163/300\n",
      "13/13 [==============================] - 0s 91us/step - loss: 0.5323 - acc: 0.9438\n",
      "Epoch 164/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.5309 - acc: 0.9438\n",
      "Epoch 165/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5295 - acc: 0.9438\n",
      "Epoch 166/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.5281 - acc: 0.9438\n",
      "Epoch 167/300\n",
      "13/13 [==============================] - 0s 114us/step - loss: 0.5267 - acc: 0.9438\n",
      "Epoch 168/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.5253 - acc: 0.9438\n",
      "Epoch 169/300\n",
      "13/13 [==============================] - 0s 117us/step - loss: 0.5238 - acc: 0.9438\n",
      "Epoch 170/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.5224 - acc: 0.9438\n",
      "Epoch 171/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.5210 - acc: 0.9497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.5195 - acc: 0.9497\n",
      "Epoch 173/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.5181 - acc: 0.9497\n",
      "Epoch 174/300\n",
      "13/13 [==============================] - 0s 97us/step - loss: 0.5166 - acc: 0.9497\n",
      "Epoch 175/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.5152 - acc: 0.9497\n",
      "Epoch 176/300\n",
      "13/13 [==============================] - 0s 70us/step - loss: 0.5137 - acc: 0.9527\n",
      "Epoch 177/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.5123 - acc: 0.9527\n",
      "Epoch 178/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.5108 - acc: 0.9527\n",
      "Epoch 179/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.5093 - acc: 0.9527\n",
      "Epoch 180/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.5078 - acc: 0.9527\n",
      "Epoch 181/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.5063 - acc: 0.9586\n",
      "Epoch 182/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.5048 - acc: 0.9586\n",
      "Epoch 183/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.5033 - acc: 0.9586\n",
      "Epoch 184/300\n",
      "13/13 [==============================] - 0s 67us/step - loss: 0.5018 - acc: 0.9586\n",
      "Epoch 185/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.5003 - acc: 0.9586\n",
      "Epoch 186/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.4987 - acc: 0.9586\n",
      "Epoch 187/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.4972 - acc: 0.9586\n",
      "Epoch 188/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.4956 - acc: 0.9586\n",
      "Epoch 189/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.4940 - acc: 0.9586\n",
      "Epoch 190/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.4924 - acc: 0.9586\n",
      "Epoch 191/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4908 - acc: 0.9586\n",
      "Epoch 192/300\n",
      "13/13 [==============================] - 0s 67us/step - loss: 0.4892 - acc: 0.9586\n",
      "Epoch 193/300\n",
      "13/13 [==============================] - 0s 67us/step - loss: 0.4876 - acc: 0.9586\n",
      "Epoch 194/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4860 - acc: 0.9615\n",
      "Epoch 195/300\n",
      "13/13 [==============================] - 0s 70us/step - loss: 0.4844 - acc: 0.9615\n",
      "Epoch 196/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.4828 - acc: 0.9615\n",
      "Epoch 197/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.4812 - acc: 0.9615\n",
      "Epoch 198/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.4796 - acc: 0.9615\n",
      "Epoch 199/300\n",
      "13/13 [==============================] - 0s 90us/step - loss: 0.4779 - acc: 0.9615\n",
      "Epoch 200/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.4763 - acc: 0.9615\n",
      "Epoch 201/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.4747 - acc: 0.9615\n",
      "Epoch 202/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4731 - acc: 0.9615\n",
      "Epoch 203/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4715 - acc: 0.9615\n",
      "Epoch 204/300\n",
      "13/13 [==============================] - 0s 64us/step - loss: 0.4698 - acc: 0.9615\n",
      "Epoch 205/300\n",
      "13/13 [==============================] - 0s 63us/step - loss: 0.4682 - acc: 0.9615\n",
      "Epoch 206/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.4665 - acc: 0.9615\n",
      "Epoch 207/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.4649 - acc: 0.9615\n",
      "Epoch 208/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.4632 - acc: 0.9615\n",
      "Epoch 209/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.4616 - acc: 0.9615\n",
      "Epoch 210/300\n",
      "13/13 [==============================] - 0s 94us/step - loss: 0.4599 - acc: 0.9615\n",
      "Epoch 211/300\n",
      "13/13 [==============================] - 0s 102us/step - loss: 0.4583 - acc: 0.9615\n",
      "Epoch 212/300\n",
      "13/13 [==============================] - 0s 88us/step - loss: 0.4566 - acc: 0.9615\n",
      "Epoch 213/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.4550 - acc: 0.9615\n",
      "Epoch 214/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.4533 - acc: 0.9615\n",
      "Epoch 215/300\n",
      "13/13 [==============================] - 0s 90us/step - loss: 0.4516 - acc: 0.9615\n",
      "Epoch 216/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.4500 - acc: 0.9615\n",
      "Epoch 217/300\n",
      "13/13 [==============================] - 0s 103us/step - loss: 0.4483 - acc: 0.9615\n",
      "Epoch 218/300\n",
      "13/13 [==============================] - 0s 88us/step - loss: 0.4467 - acc: 0.9615\n",
      "Epoch 219/300\n",
      "13/13 [==============================] - 0s 91us/step - loss: 0.4450 - acc: 0.9615\n",
      "Epoch 220/300\n",
      "13/13 [==============================] - 0s 67us/step - loss: 0.4434 - acc: 0.9615\n",
      "Epoch 221/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.4418 - acc: 0.9615\n",
      "Epoch 222/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.4401 - acc: 0.9615\n",
      "Epoch 223/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.4385 - acc: 0.9615\n",
      "Epoch 224/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.4368 - acc: 0.9615\n",
      "Epoch 225/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.4352 - acc: 0.9615\n",
      "Epoch 226/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4336 - acc: 0.9615\n",
      "Epoch 227/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4319 - acc: 0.9615\n",
      "Epoch 228/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.4303 - acc: 0.9615\n",
      "Epoch 229/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.4287 - acc: 0.9615\n",
      "Epoch 230/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.4271 - acc: 0.9615\n",
      "Epoch 231/300\n",
      "13/13 [==============================] - 0s 82us/step - loss: 0.4254 - acc: 0.9615\n",
      "Epoch 232/300\n",
      "13/13 [==============================] - 0s 90us/step - loss: 0.4238 - acc: 0.9615\n",
      "Epoch 233/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.4222 - acc: 0.9615\n",
      "Epoch 234/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.4205 - acc: 0.9615\n",
      "Epoch 235/300\n",
      "13/13 [==============================] - 0s 65us/step - loss: 0.4189 - acc: 0.9615\n",
      "Epoch 236/300\n",
      "13/13 [==============================] - 0s 70us/step - loss: 0.4172 - acc: 0.9615\n",
      "Epoch 237/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.4156 - acc: 0.9615\n",
      "Epoch 238/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.4139 - acc: 0.9615\n",
      "Epoch 239/300\n",
      "13/13 [==============================] - 0s 93us/step - loss: 0.4123 - acc: 0.9615\n",
      "Epoch 240/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.4106 - acc: 0.9615\n",
      "Epoch 241/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.4089 - acc: 0.9615\n",
      "Epoch 242/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.4072 - acc: 0.9615\n",
      "Epoch 243/300\n",
      "13/13 [==============================] - 0s 88us/step - loss: 0.4055 - acc: 0.9615\n",
      "Epoch 244/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.4038 - acc: 0.9615\n",
      "Epoch 245/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.4021 - acc: 0.9615\n",
      "Epoch 246/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.4003 - acc: 0.9615\n",
      "Epoch 247/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.3986 - acc: 0.9615\n",
      "Epoch 248/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.3968 - acc: 0.9615\n",
      "Epoch 249/300\n",
      "13/13 [==============================] - 0s 89us/step - loss: 0.3950 - acc: 0.9615\n",
      "Epoch 250/300\n",
      "13/13 [==============================] - 0s 102us/step - loss: 0.3932 - acc: 0.9615\n",
      "Epoch 251/300\n",
      "13/13 [==============================] - 0s 130us/step - loss: 0.3913 - acc: 0.9615\n",
      "Epoch 252/300\n",
      "13/13 [==============================] - 0s 97us/step - loss: 0.3894 - acc: 0.9615\n",
      "Epoch 253/300\n",
      "13/13 [==============================] - 0s 128us/step - loss: 0.3875 - acc: 0.9615\n",
      "Epoch 254/300\n",
      "13/13 [==============================] - 0s 84us/step - loss: 0.3856 - acc: 0.9615\n",
      "Epoch 255/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.3837 - acc: 0.9615\n",
      "Epoch 256/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.3818 - acc: 0.9615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 257/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.3799 - acc: 0.9615\n",
      "Epoch 258/300\n",
      "13/13 [==============================] - 0s 93us/step - loss: 0.3781 - acc: 0.9615\n",
      "Epoch 259/300\n",
      "13/13 [==============================] - 0s 87us/step - loss: 0.3762 - acc: 0.9615\n",
      "Epoch 260/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.3743 - acc: 0.9615\n",
      "Epoch 261/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.3724 - acc: 0.9615\n",
      "Epoch 262/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.3705 - acc: 0.9615\n",
      "Epoch 263/300\n",
      "13/13 [==============================] - 0s 86us/step - loss: 0.3687 - acc: 0.9615\n",
      "Epoch 264/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.3668 - acc: 0.9615\n",
      "Epoch 265/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.3649 - acc: 0.9615\n",
      "Epoch 266/300\n",
      "13/13 [==============================] - 0s 90us/step - loss: 0.3631 - acc: 0.9615\n",
      "Epoch 267/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.3612 - acc: 0.9615\n",
      "Epoch 268/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.3593 - acc: 0.9615\n",
      "Epoch 269/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.3575 - acc: 0.9615\n",
      "Epoch 270/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.3556 - acc: 0.9615\n",
      "Epoch 271/300\n",
      "13/13 [==============================] - 0s 95us/step - loss: 0.3537 - acc: 0.9615\n",
      "Epoch 272/300\n",
      "13/13 [==============================] - 0s 96us/step - loss: 0.3519 - acc: 0.9615\n",
      "Epoch 273/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.3500 - acc: 0.9615\n",
      "Epoch 274/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.3482 - acc: 0.9615\n",
      "Epoch 275/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.3464 - acc: 0.9615\n",
      "Epoch 276/300\n",
      "13/13 [==============================] - 0s 79us/step - loss: 0.3445 - acc: 0.9615\n",
      "Epoch 277/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.3427 - acc: 0.9615\n",
      "Epoch 278/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.3409 - acc: 0.9615\n",
      "Epoch 279/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.3391 - acc: 0.9615\n",
      "Epoch 280/300\n",
      "13/13 [==============================] - 0s 83us/step - loss: 0.3374 - acc: 0.9615\n",
      "Epoch 281/300\n",
      "13/13 [==============================] - 0s 85us/step - loss: 0.3356 - acc: 0.9615\n",
      "Epoch 282/300\n",
      "13/13 [==============================] - 0s 98us/step - loss: 0.3338 - acc: 0.9615\n",
      "Epoch 283/300\n",
      "13/13 [==============================] - 0s 76us/step - loss: 0.3321 - acc: 0.9615\n",
      "Epoch 284/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.3304 - acc: 0.9615\n",
      "Epoch 285/300\n",
      "13/13 [==============================] - 0s 72us/step - loss: 0.3287 - acc: 0.9615\n",
      "Epoch 286/300\n",
      "13/13 [==============================] - 0s 70us/step - loss: 0.3270 - acc: 0.9615\n",
      "Epoch 287/300\n",
      "13/13 [==============================] - 0s 67us/step - loss: 0.3252 - acc: 0.9615\n",
      "Epoch 288/300\n",
      "13/13 [==============================] - 0s 70us/step - loss: 0.3235 - acc: 0.9615\n",
      "Epoch 289/300\n",
      "13/13 [==============================] - 0s 70us/step - loss: 0.3218 - acc: 0.9615\n",
      "Epoch 290/300\n",
      "13/13 [==============================] - 0s 81us/step - loss: 0.3202 - acc: 0.9615\n",
      "Epoch 291/300\n",
      "13/13 [==============================] - 0s 75us/step - loss: 0.3185 - acc: 0.9615\n",
      "Epoch 292/300\n",
      "13/13 [==============================] - 0s 74us/step - loss: 0.3168 - acc: 0.9615\n",
      "Epoch 293/300\n",
      "13/13 [==============================] - 0s 94us/step - loss: 0.3152 - acc: 0.9615\n",
      "Epoch 294/300\n",
      "13/13 [==============================] - 0s 78us/step - loss: 0.3135 - acc: 0.9615\n",
      "Epoch 295/300\n",
      "13/13 [==============================] - 0s 115us/step - loss: 0.3119 - acc: 0.9615\n",
      "Epoch 296/300\n",
      "13/13 [==============================] - 0s 71us/step - loss: 0.3102 - acc: 0.9615\n",
      "Epoch 297/300\n",
      "13/13 [==============================] - 0s 73us/step - loss: 0.3086 - acc: 0.9615\n",
      "Epoch 298/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.3069 - acc: 0.9615\n",
      "Epoch 299/300\n",
      "13/13 [==============================] - 0s 77us/step - loss: 0.3053 - acc: 0.9615\n",
      "Epoch 300/300\n",
      "13/13 [==============================] - 0s 80us/step - loss: 0.3036 - acc: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb27917550>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the output has 26 dimension, the accuracy is not the real accuracy for the systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial caeser function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to build a architechture for our deep learning model, we tried to use sigmoid function to build one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caeser funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, Caeser encryption function is $(n+3) mod 26$. key = 3 in this situation, but generally we can chose key as any integer number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit it by using sigmoid function, we can see in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    return(1/(1 + math.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_caeser_function(x):\n",
    "    s1 = sigmoid(-30*(x-22.5))\n",
    "    s2 = sigmoid(30*(x-22.5))\n",
    "    y = (x+3)*s1+(x-23)*s2\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.953457900066243e-06"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_caeser_function(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11e1b60b8>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0,26,.01), list(map(ai_caeser_function,np.arange(0,26,.01))),'-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = (x-3)sigmoid(-30(x-22.5))+(x-23)sigmoid(30(x-22.5))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try to build a similar structure for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function data_genelization in module myfun:\n",
      "\n",
      "data_genelization(sample_size=2, loops=1000, size=26, key=3, x_as_vector=False, y_as_vector=False)\n",
      "    data_genelization(sample_size = 2,loops = 1000, size = 26, key = 3, x_as_vector = False, y_as_vector = False):\n",
      "    return x_train, label with equual size, labels with 1 dimension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(data_genelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x_train, y_train, y_train_small = data_genelization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1000/1000 [==============================] - 0s 465us/step - loss: 0.6700 - acc: 0.6623\n",
      "Epoch 2/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.5888 - acc: 0.8702\n",
      "Epoch 3/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.4076 - acc: 0.9510\n",
      "Epoch 4/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.2146 - acc: 0.9615\n",
      "Epoch 5/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1666 - acc: 0.9615\n",
      "Epoch 6/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 7/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1595 - acc: 0.9615\n",
      "Epoch 8/200\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1579 - acc: 0.9615\n",
      "Epoch 9/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1563 - acc: 0.9615\n",
      "Epoch 10/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1548 - acc: 0.9615\n",
      "Epoch 11/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1530 - acc: 0.9615\n",
      "Epoch 12/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1512 - acc: 0.9615\n",
      "Epoch 13/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1491 - acc: 0.9615\n",
      "Epoch 14/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1468 - acc: 0.9615\n",
      "Epoch 15/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1443 - acc: 0.9615\n",
      "Epoch 16/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1415 - acc: 0.9615\n",
      "Epoch 17/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1386 - acc: 0.9615\n",
      "Epoch 18/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1355 - acc: 0.9615\n",
      "Epoch 19/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1321 - acc: 0.9615\n",
      "Epoch 20/200\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1284 - acc: 0.9615\n",
      "Epoch 21/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1245 - acc: 0.9616\n",
      "Epoch 22/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1205 - acc: 0.9617\n",
      "Epoch 23/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1164 - acc: 0.9618\n",
      "Epoch 24/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.1123 - acc: 0.9620\n",
      "Epoch 25/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1081 - acc: 0.9624\n",
      "Epoch 26/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1041 - acc: 0.9628\n",
      "Epoch 27/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1001 - acc: 0.9637\n",
      "Epoch 28/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0962 - acc: 0.9641\n",
      "Epoch 29/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0924 - acc: 0.9648\n",
      "Epoch 30/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0888 - acc: 0.9657\n",
      "Epoch 31/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0853 - acc: 0.9669\n",
      "Epoch 32/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0817 - acc: 0.9678\n",
      "Epoch 33/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0784 - acc: 0.9688\n",
      "Epoch 34/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0750 - acc: 0.9706\n",
      "Epoch 35/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0717 - acc: 0.9721\n",
      "Epoch 36/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0686 - acc: 0.9733\n",
      "Epoch 37/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0655 - acc: 0.9757\n",
      "Epoch 38/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0626 - acc: 0.9774\n",
      "Epoch 39/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0597 - acc: 0.9786\n",
      "Epoch 40/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0569 - acc: 0.9797\n",
      "Epoch 41/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0543 - acc: 0.9806\n",
      "Epoch 42/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0518 - acc: 0.9819\n",
      "Epoch 43/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0494 - acc: 0.9831\n",
      "Epoch 44/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0471 - acc: 0.9844\n",
      "Epoch 45/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0450 - acc: 0.9857\n",
      "Epoch 46/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0429 - acc: 0.9862\n",
      "Epoch 47/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0410 - acc: 0.9868\n",
      "Epoch 48/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0391 - acc: 0.9876\n",
      "Epoch 49/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0373 - acc: 0.9883\n",
      "Epoch 50/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0357 - acc: 0.9885\n",
      "Epoch 51/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0341 - acc: 0.9893\n",
      "Epoch 52/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0326 - acc: 0.9896\n",
      "Epoch 53/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0311 - acc: 0.9903\n",
      "Epoch 54/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0297 - acc: 0.9912\n",
      "Epoch 55/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0284 - acc: 0.9916\n",
      "Epoch 56/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0271 - acc: 0.9922\n",
      "Epoch 57/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0260 - acc: 0.9927\n",
      "Epoch 58/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0248 - acc: 0.9931\n",
      "Epoch 59/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0237 - acc: 0.9934\n",
      "Epoch 60/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0227 - acc: 0.9939\n",
      "Epoch 61/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0217 - acc: 0.9943\n",
      "Epoch 62/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0208 - acc: 0.9945\n",
      "Epoch 63/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0199 - acc: 0.9949\n",
      "Epoch 64/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0191 - acc: 0.9950\n",
      "Epoch 65/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0182 - acc: 0.9956\n",
      "Epoch 66/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0175 - acc: 0.9958\n",
      "Epoch 67/200\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0168 - acc: 0.9960\n",
      "Epoch 68/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0161 - acc: 0.9962\n",
      "Epoch 69/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0155 - acc: 0.9963\n",
      "Epoch 70/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0148 - acc: 0.9966\n",
      "Epoch 71/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0142 - acc: 0.9970\n",
      "Epoch 72/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0137 - acc: 0.9970\n",
      "Epoch 73/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0131 - acc: 0.9972\n",
      "Epoch 74/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0126 - acc: 0.9973\n",
      "Epoch 75/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0121 - acc: 0.9976\n",
      "Epoch 76/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0116 - acc: 0.9976\n",
      "Epoch 77/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 78/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0108 - acc: 0.9978\n",
      "Epoch 79/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0104 - acc: 0.9979\n",
      "Epoch 80/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0100 - acc: 0.9982\n",
      "Epoch 81/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0096 - acc: 0.9983\n",
      "Epoch 82/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0093 - acc: 0.9984\n",
      "Epoch 83/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0089 - acc: 0.9985\n",
      "Epoch 84/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0086 - acc: 0.9985\n",
      "Epoch 85/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0083 - acc: 0.9986\n",
      "Epoch 86/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0080 - acc: 0.9987\n",
      "Epoch 87/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0077 - acc: 0.9986\n",
      "Epoch 88/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0074 - acc: 0.9988\n",
      "Epoch 89/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0072 - acc: 0.9988\n",
      "Epoch 90/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0070 - acc: 0.9989\n",
      "Epoch 91/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0067 - acc: 0.9988\n",
      "Epoch 92/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0065 - acc: 0.9990\n",
      "Epoch 93/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0062 - acc: 0.9991\n",
      "Epoch 94/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0060 - acc: 0.9991\n",
      "Epoch 95/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0058 - acc: 0.9992\n",
      "Epoch 96/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0056 - acc: 0.9992\n",
      "Epoch 97/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0054 - acc: 0.9993\n",
      "Epoch 98/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0052 - acc: 0.9992\n",
      "Epoch 99/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0051 - acc: 0.9993\n",
      "Epoch 100/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0049 - acc: 0.9994\n",
      "Epoch 101/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0047 - acc: 0.9994\n",
      "Epoch 102/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0046 - acc: 0.9994\n",
      "Epoch 103/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0044 - acc: 0.9995\n",
      "Epoch 104/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0043 - acc: 0.9995\n",
      "Epoch 105/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0041 - acc: 0.9995\n",
      "Epoch 106/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 107/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0039 - acc: 0.9996\n",
      "Epoch 108/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0037 - acc: 0.9996\n",
      "Epoch 109/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 110/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0035 - acc: 0.9997\n",
      "Epoch 111/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 112/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 113/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0032 - acc: 0.9997\n",
      "Epoch 114/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0031 - acc: 0.9997\n",
      "Epoch 115/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0030 - acc: 0.9997\n",
      "Epoch 116/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 117/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 118/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 119/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 120/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 121/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 122/200\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 123/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0023 - acc: 0.9999\n",
      "Epoch 124/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0022 - acc: 0.9999\n",
      "Epoch 125/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 126/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 127/200\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0020 - acc: 0.9999\n",
      "Epoch 128/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 129/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 130/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0018 - acc: 0.9999\n",
      "Epoch 131/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 132/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 133/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 134/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0016 - acc: 0.9999\n",
      "Epoch 135/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 136/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 137/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0011 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0010 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 9.7236e-04 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 9.4125e-04 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 9.1808e-04 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 8.9147e-04 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 8.6359e-04 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 8.4166e-04 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 8.1558e-04 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 7.9493e-04 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 7.8806e-04 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.5468e-04 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.3276e-04 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 7.1028e-04 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.8649e-04 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.6916e-04 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.5236e-04 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.3536e-04 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 6.1699e-04 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 6.0138e-04 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.8287e-04 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.7002e-04 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.5388e-04 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.4054e-04 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.2514e-04 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 5.1190e-04 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.9908e-04 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.8500e-04 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.7333e-04 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.6115e-04 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.4938e-04 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.3618e-04 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.2750e-04 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.1621e-04 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 4.0465e-04 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.9477e-04 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.8559e-04 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 3.7617e-04 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.6605e-04 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 3.5802e-04 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.4792e-04 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.4036e-04 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 3.3260e-04 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.2386e-04 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.1661e-04 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.0922e-04 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 3.0159e-04 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.9404e-04 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 2.8667e-04 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.8076e-04 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.7379e-04 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.6671e-04 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 2.6116e-04 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2776f898>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2]])\n",
    "\n",
    "\n",
    "def predict_results_only_2(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2))\n",
    "    label_list = list(map(num2str, label1, label2))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2):   \n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        return(\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 100us/step\n",
      "\n",
      "acc: 99.62%\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, y_test_small = data_genelization()\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "prediction_list, label_list = predict_results_only_2(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UN UN\n",
      "YG YG\n",
      "LW LW\n",
      "KG KG\n",
      "IC IC\n",
      "DV DV\n",
      "IM IM\n",
      "CF CF\n",
      "SV SV\n",
      "VR VR\n",
      "EK EK\n",
      "DY DY\n",
      "PJ PJ\n",
      "NM NM\n",
      "FD FD\n",
      "GR GR\n",
      "CK CK\n",
      "FG FG\n",
      "VK VX\n",
      "RF RF\n",
      "FG FG\n",
      "WM QC\n",
      "IC IC\n",
      "WR WR\n",
      "AM AM\n",
      "VP VP\n",
      "LY LY\n",
      "UB UB\n",
      "IM IM\n",
      "WG WG\n",
      "GM GM\n",
      "JI JI\n",
      "BS BS\n",
      "EI EI\n",
      "AJ AJ\n",
      "MY MY\n",
      "JW JW\n",
      "LQ LQ\n",
      "LR LR\n",
      "AB AB\n",
      "WS WS\n",
      "KR KR\n",
      "JM JM\n",
      "MV MV\n",
      "SQ XQ\n",
      "LW LW\n",
      "GL GL\n",
      "VS VS\n",
      "AF AF\n",
      "HC HC\n",
      "CC CC\n",
      "FK FK\n",
      "RW RW\n",
      "ZU ZU\n",
      "FI FI\n",
      "KL KL\n",
      "VE VE\n",
      "DO DO\n",
      "GG GG\n",
      "JV JV\n",
      "SY SY\n",
      "SP SP\n",
      "NB NB\n",
      "KX KX\n",
      "CP CP\n",
      "TN TN\n",
      "LJ LJ\n",
      "TQ TQ\n",
      "GI GI\n",
      "NT NT\n",
      "SQ SQ\n",
      "YY YY\n",
      "LG LG\n",
      "NY NY\n",
      "FC FC\n",
      "EY EY\n",
      "OT OT\n",
      "WS WS\n",
      "BG BG\n",
      "OU OU\n",
      "MZ MZ\n",
      "KF KF\n",
      "IP IP\n",
      "FB FB\n",
      "YN YN\n",
      "FA FA\n",
      "RE RE\n",
      "XA XA\n",
      "XI XI\n",
      "EI EI\n",
      "RF RF\n",
      "AT AT\n",
      "CG CG\n",
      "GD GD\n",
      "WX WX\n",
      "ZX ZX\n",
      "QE QE\n",
      "XC XC\n",
      "YN YN\n",
      "XM XM\n",
      "CF CF\n",
      "NL NL\n",
      "HS HS\n",
      "NX NX\n",
      "EO EO\n",
      "IP IP\n",
      "SP SP\n",
      "LH LH\n",
      "MM MM\n",
      "CP CP\n",
      "LX LX\n",
      "ZU ZU\n",
      "XS XS\n",
      "GV GV\n",
      "TN TN\n",
      "NM NM\n",
      "VC VC\n",
      "FD FD\n",
      "CX CX\n",
      "PY PY\n",
      "XR XR\n",
      "AS AS\n",
      "ZK ZK\n",
      "FF FF\n",
      "ZP ZP\n",
      "UF UF\n",
      "MO MO\n",
      "HM HM\n",
      "FE EZ\n",
      "ZD ZD\n",
      "OQ OQ\n",
      "IP IP\n",
      "RU RU\n",
      "JY JY\n",
      "RP RP\n",
      "RI RI\n",
      "UE UE\n",
      "UK UK\n",
      "WK WK\n",
      "JQ JQ\n",
      "BS BS\n",
      "ES ES\n",
      "QS QS\n",
      "RT RT\n",
      "EA EA\n",
      "QJ QJ\n",
      "MO MO\n",
      "UO RX\n",
      "CK CK\n",
      "VD VD\n",
      "SQ SQ\n",
      "VB VB\n",
      "OU OL\n",
      "RL RL\n",
      "DD DD\n",
      "EV EV\n",
      "BW BW\n",
      "ZW ZW\n",
      "UN UN\n",
      "HC HC\n",
      "QH QH\n",
      "AE AE\n",
      "MU MU\n",
      "VP VP\n",
      "BO BO\n",
      "RM RM\n",
      "KC KC\n",
      "WU WU\n",
      "GM GM\n",
      "UO RX\n",
      "BG BG\n",
      "DL DL\n",
      "HK HK\n",
      "WI WI\n",
      "CW CW\n",
      "XB XB\n",
      "WW WW\n",
      "FN FN\n",
      "DH DH\n",
      "WQ WQ\n",
      "LR LR\n",
      "AP AP\n",
      "PH PH\n",
      "IT IT\n",
      "MW MW\n",
      "IP IP\n",
      "CC CC\n",
      "VK VX\n",
      "QL QL\n",
      "FE FE\n",
      "TG ZC\n",
      "AF AF\n",
      "MP MP\n",
      "VJ VJ\n",
      "RO RO\n",
      "IP IP\n",
      "RZ RZ\n",
      "VJ VJ\n",
      "KU KU\n",
      "NX NX\n",
      "ZI ZI\n",
      "BU BU\n",
      "LF LF\n",
      "LJ LJ\n",
      "BQ BQ\n",
      "PQ PQ\n",
      "ID ID\n",
      "VN VN\n",
      "YH YH\n",
      "NV NV\n",
      "GQ GQ\n",
      "HL HL\n",
      "BX BX\n",
      "NR NR\n",
      "CJ CJ\n",
      "OF OF\n",
      "EM EM\n",
      "BM BM\n",
      "AH AH\n",
      "XR XR\n",
      "LI LI\n",
      "TG TG\n",
      "OY OY\n",
      "MH MH\n",
      "NV NV\n",
      "TX TX\n",
      "SM SM\n",
      "RL RL\n",
      "CA CA\n",
      "FM FM\n",
      "CZ CZ\n",
      "PV PV\n",
      "EW EW\n",
      "QG QG\n",
      "YK YK\n",
      "PQ PQ\n",
      "LH LH\n",
      "VY VY\n",
      "YY YY\n",
      "JT JT\n",
      "DD DD\n",
      "BE BE\n",
      "IT IT\n",
      "ZA ZA\n",
      "OU OU\n",
      "WO WO\n",
      "PP PP\n",
      "EK EK\n",
      "XN XN\n",
      "EO EO\n",
      "FL FL\n",
      "BY BY\n",
      "KK KK\n",
      "UJ UJ\n",
      "QK QK\n",
      "LE LE\n",
      "TB TB\n",
      "NP NP\n",
      "VU VU\n",
      "TY TY\n",
      "LG LG\n",
      "PO PO\n",
      "OW OW\n",
      "VT YT\n",
      "AT AT\n",
      "UL UL\n",
      "QV QV\n",
      "CU CU\n",
      "PX PX\n",
      "CS CS\n",
      "IL IL\n",
      "YE YE\n",
      "UK UK\n",
      "AU AU\n",
      "VG VG\n",
      "FY FY\n",
      "NI NI\n",
      "TI TI\n",
      "GC GX\n",
      "HJ HJ\n",
      "SO ZO\n",
      "SC SC\n",
      "ZI ZI\n",
      "XJ XJ\n",
      "NX NX\n",
      "UN UN\n",
      "VZ VZ\n",
      "SW SW\n",
      "ON ON\n",
      "BV BV\n",
      "XB XB\n",
      "TX TX\n",
      "FA FA\n",
      "FZ FZ\n",
      "YU YU\n",
      "YE YE\n",
      "KZ KZ\n",
      "TP TP\n",
      "OD OD\n",
      "XH XH\n",
      "CB CB\n",
      "EF EF\n",
      "PE PE\n",
      "SY SY\n",
      "JY JY\n",
      "XZ XZ\n",
      "FM FM\n",
      "QQ QQ\n",
      "XS XS\n",
      "DU DU\n",
      "KW KW\n",
      "UG UG\n",
      "SU SU\n",
      "VA VA\n",
      "HR HR\n",
      "DB DB\n",
      "OI OI\n",
      "UG UG\n",
      "RR RR\n",
      "HQ HQ\n",
      "SJ SJ\n",
      "BB BB\n",
      "TL PK\n",
      "ZE ZE\n",
      "XD XD\n",
      "IX IX\n",
      "KQ KQ\n",
      "HZ HZ\n",
      "LO LO\n",
      "GH GH\n",
      "IZ IZ\n",
      "CD CD\n",
      "EO EO\n",
      "FX FX\n",
      "AM AM\n",
      "IP IP\n",
      "UH UH\n",
      "XI XI\n",
      "WM WM\n",
      "UK UK\n",
      "XV XV\n",
      "RE RE\n",
      "YV YV\n",
      "ZR ZR\n",
      "ZS ZS\n",
      "QQ QQ\n",
      "RJ RJ\n",
      "DG DG\n",
      "ME ME\n",
      "YM YM\n",
      "HY HY\n",
      "YG YG\n",
      "KZ KZ\n",
      "YQ YQ\n",
      "JI JI\n",
      "FT FT\n",
      "IL IL\n",
      "UW UW\n",
      "OB OB\n",
      "BV BV\n",
      "ZP ZP\n",
      "SX SX\n",
      "RP RP\n",
      "CT CT\n",
      "JQ JQ\n",
      "TJ TJ\n",
      "GY GY\n",
      "MM MM\n",
      "RJ RJ\n",
      "CJ CJ\n",
      "ZB ZB\n",
      "BQ BQ\n",
      "ID ID\n",
      "KI KI\n",
      "TK TK\n",
      "AQ AQ\n",
      "KX KX\n",
      "WW WW\n",
      "RV RV\n",
      "BL BL\n",
      "NG NG\n",
      "FJ FJ\n",
      "AG AG\n",
      "PF PF\n",
      "ZV ZV\n",
      "GE GE\n",
      "TW TW\n",
      "IN IN\n",
      "RE RE\n",
      "AW AW\n",
      "IF IF\n",
      "GC GX\n",
      "NK NK\n",
      "RH RH\n",
      "SR SR\n",
      "SA SA\n",
      "UT UT\n",
      "FZ FZ\n",
      "RW RW\n",
      "MG MG\n",
      "WN WN\n",
      "SW SW\n",
      "NE NE\n",
      "NG NG\n",
      "ZT ZT\n",
      "EO EO\n",
      "VI VI\n",
      "UH UH\n",
      "PA PA\n",
      "SJ SJ\n",
      "VO VO\n",
      "ZT ZT\n",
      "TS TS\n",
      "OF OF\n",
      "PO PO\n",
      "YJ YJ\n",
      "UV UV\n",
      "MQ MQ\n",
      "IF IF\n",
      "SY SY\n",
      "EY EY\n",
      "II II\n",
      "GD GD\n",
      "LL SL\n",
      "AR AR\n",
      "TZ TZ\n",
      "VK VX\n",
      "MA MA\n",
      "PP PP\n",
      "QB QB\n",
      "XV XV\n",
      "SV SV\n",
      "GW GW\n",
      "MF MF\n",
      "JB JB\n",
      "BE BE\n",
      "HE HE\n",
      "SX SX\n",
      "HZ HZ\n",
      "YB YB\n",
      "FP FP\n",
      "QG QG\n",
      "ZA ZA\n",
      "NU NU\n",
      "WO WO\n",
      "JF JF\n",
      "TA TA\n",
      "XF XF\n",
      "UD UD\n",
      "TQ TQ\n",
      "DB DB\n",
      "CC CC\n",
      "DX DX\n",
      "DD DD\n",
      "HL HU\n",
      "LQ LQ\n",
      "HV HV\n",
      "UO UO\n",
      "RF RF\n",
      "PU PU\n",
      "ZQ ZQ\n",
      "YR YR\n",
      "ST ST\n",
      "DK DK\n",
      "LT LT\n",
      "IM IM\n",
      "GR GR\n",
      "KO KO\n",
      "PE PE\n",
      "OZ OZ\n",
      "YF YF\n",
      "OG OS\n",
      "UE UE\n",
      "ID ID\n",
      "WL WL\n",
      "YP YP\n",
      "HL HU\n",
      "DN DN\n",
      "DI DI\n",
      "JH JH\n",
      "UP UP\n",
      "RS RS\n",
      "YM YM\n",
      "SN SN\n",
      "HC DS\n",
      "AN AN\n",
      "TV TV\n",
      "NX NX\n",
      "OI OI\n",
      "OW OW\n",
      "HG HG\n",
      "VM VM\n",
      "GB GB\n",
      "GD GD\n",
      "EF EF\n",
      "NR NR\n",
      "RO RO\n",
      "LN LN\n",
      "TM TM\n",
      "KI KI\n",
      "IT IT\n",
      "QU QU\n",
      "FV FV\n",
      "PR PR\n",
      "IV IV\n",
      "JD JD\n",
      "BS BS\n",
      "DK DK\n",
      "PL PL\n",
      "FJ FJ\n",
      "BT BT\n",
      "UP UP\n",
      "TY TY\n",
      "WF WF\n",
      "OT OT\n",
      "SF SF\n",
      "UG UG\n",
      "CE CE\n",
      "PC PC\n",
      "DZ DZ\n",
      "VZ VZ\n",
      "GF GF\n",
      "HD HD\n",
      "IK IK\n",
      "CH CH\n",
      "UE CQ\n",
      "EU EU\n",
      "UF UF\n",
      "QN QN\n",
      "QX QX\n",
      "JL JL\n",
      "GG GG\n",
      "YG YG\n",
      "UA UA\n",
      "IP IP\n",
      "BW BW\n",
      "UP UP\n",
      "YA YA\n",
      "US US\n",
      "XH XH\n",
      "TR TR\n",
      "JW JW\n",
      "CK CK\n",
      "HR HR\n",
      "HL HU\n",
      "LL LL\n",
      "GT GT\n",
      "RR RR\n",
      "LJ LJ\n",
      "BB BB\n",
      "NS NS\n",
      "RZ RZ\n",
      "HF HF\n",
      "TQ TQ\n",
      "NN NN\n",
      "RW RW\n",
      "IE IE\n",
      "JX JX\n",
      "FX FX\n",
      "OG OG\n",
      "DK DK\n",
      "KD KD\n",
      "GS GS\n",
      "UX UX\n",
      "YC YC\n",
      "OQ OQ\n",
      "UQ UQ\n",
      "BM BM\n",
      "FR FR\n",
      "HB HB\n",
      "WP WP\n",
      "MB MB\n",
      "BI BI\n",
      "FT FT\n",
      "HW HW\n",
      "JB JB\n",
      "EF EF\n",
      "UQ UQ\n",
      "JC JC\n",
      "PS PS\n",
      "IX IX\n",
      "VM VM\n",
      "CB CB\n",
      "BN BN\n",
      "WU WU\n",
      "UC UC\n",
      "NT NT\n",
      "VS VS\n",
      "UX UX\n",
      "DN DN\n",
      "UK UK\n",
      "VY VY\n",
      "VN VN\n",
      "YK YK\n",
      "OZ OZ\n",
      "GO GO\n",
      "WJ WJ\n",
      "XU XU\n",
      "HZ HZ\n",
      "XF XF\n",
      "EO EO\n",
      "UV UV\n",
      "AK AK\n",
      "SV SV\n",
      "PH PH\n",
      "QM QM\n",
      "GK GK\n",
      "TJ TJ\n",
      "ND ND\n",
      "HS HS\n",
      "ZA ZA\n",
      "EK EK\n",
      "EU EU\n",
      "QT QT\n",
      "HD WD\n",
      "LH LH\n",
      "AO AO\n",
      "TT TT\n",
      "GW GW\n",
      "NJ NJ\n",
      "XU XU\n",
      "LR LR\n",
      "RV RV\n",
      "UB UB\n",
      "OP OP\n",
      "KL KL\n",
      "QE QE\n",
      "HI HI\n",
      "HN HN\n",
      "FE EZ\n",
      "HI HI\n",
      "MM MM\n",
      "UJ UJ\n",
      "PM PM\n",
      "FS FS\n",
      "JD JD\n",
      "DT DT\n",
      "TZ TZ\n",
      "ZK ZK\n",
      "CS CS\n",
      "JZ JZ\n",
      "LH LH\n",
      "CJ CJ\n",
      "WA WA\n",
      "LM LM\n",
      "NB NB\n",
      "IQ IQ\n",
      "DT DT\n",
      "HH HH\n",
      "UC UC\n",
      "LU LU\n",
      "FD FD\n",
      "NE NE\n",
      "LC LC\n",
      "LI LI\n",
      "YX YX\n",
      "EF EF\n",
      "XA XA\n",
      "QK QK\n",
      "YM YM\n",
      "TP TP\n",
      "GF GF\n",
      "NG NG\n",
      "LS LS\n",
      "VP VP\n",
      "QS QS\n",
      "HH HH\n",
      "FC FC\n",
      "BF BF\n",
      "WT WT\n",
      "DE DE\n",
      "ZW ZW\n",
      "GG GG\n",
      "JL JL\n",
      "RW RW\n",
      "KL KL\n",
      "ZW ZW\n",
      "HL HL\n",
      "CG CG\n",
      "WF WF\n",
      "EH EH\n",
      "IF IF\n",
      "TE TE\n",
      "XO XO\n",
      "SA SH\n",
      "PX PX\n",
      "NU NU\n",
      "ZB ZB\n",
      "JD JD\n",
      "LN LN\n",
      "KJ KJ\n",
      "SJ SJ\n",
      "VA VA\n",
      "HD WD\n",
      "SY SY\n",
      "MW MW\n",
      "JC JC\n",
      "FY FY\n",
      "LU LU\n",
      "VK VK\n",
      "BV BV\n",
      "ZK ZK\n",
      "IJ IJ\n",
      "PB PB\n",
      "ST ST\n",
      "UD UD\n",
      "JQ JQ\n",
      "XK XK\n",
      "XC XC\n",
      "WK WK\n",
      "FL FL\n",
      "CU CU\n",
      "XL XL\n",
      "MQ MQ\n",
      "JM JM\n",
      "FW FW\n",
      "MK MK\n",
      "NE NE\n",
      "NQ NQ\n",
      "XU XU\n",
      "CO CO\n",
      "PU PU\n",
      "QL QL\n",
      "IA IA\n",
      "YP YP\n",
      "GL GL\n",
      "DK DK\n",
      "LL SL\n",
      "TO TO\n",
      "EP EP\n",
      "FY FY\n",
      "CD CD\n",
      "HT HT\n",
      "HK HK\n",
      "BO BO\n",
      "EB EB\n",
      "JH JH\n",
      "SD SD\n",
      "HQ HQ\n",
      "TK TK\n",
      "IF IF\n",
      "YE YE\n",
      "SW SW\n",
      "RJ RJ\n",
      "MR MR\n",
      "KU KU\n",
      "KO KO\n",
      "HA HA\n",
      "SD SD\n",
      "GC GX\n",
      "PZ PZ\n",
      "BL BL\n",
      "VU VU\n",
      "DW DW\n",
      "XL XL\n",
      "XZ XZ\n",
      "JD JD\n",
      "SP SP\n",
      "GK GK\n",
      "PL PL\n",
      "FG FG\n",
      "XW XW\n",
      "LY LY\n",
      "LF LF\n",
      "GW GW\n",
      "CA CA\n",
      "UE UE\n",
      "LM LM\n",
      "LQ LQ\n",
      "US US\n",
      "DR DR\n",
      "WM QC\n",
      "UP UP\n",
      "FV FV\n",
      "MJ MJ\n",
      "VB VB\n",
      "AX AX\n",
      "YE YE\n",
      "BV BV\n",
      "SQ SQ\n",
      "LN LN\n",
      "MK MK\n",
      "IO IO\n",
      "GR GR\n",
      "LT LT\n",
      "TI TI\n",
      "EU EU\n",
      "TJ TJ\n",
      "RR RR\n",
      "VF VF\n",
      "AJ AJ\n",
      "ZB ZB\n",
      "HE HE\n",
      "LI LI\n",
      "MP MP\n",
      "BS BS\n",
      "BO BO\n",
      "EM EM\n",
      "IG IG\n",
      "HM HM\n",
      "NO NO\n",
      "TQ TQ\n",
      "GM GM\n",
      "QS QS\n",
      "VZ VZ\n",
      "HI HI\n",
      "FD FD\n",
      "UO RX\n",
      "HT HT\n",
      "AV AV\n",
      "TT TT\n",
      "VK VK\n",
      "XK XK\n",
      "TF TF\n",
      "YR YR\n",
      "SZ SZ\n",
      "RS RS\n",
      "CF CF\n",
      "QH QH\n",
      "UN UN\n",
      "EE EE\n",
      "AH AH\n",
      "WO WO\n",
      "AK AK\n",
      "VE VE\n",
      "LM LM\n",
      "GO GO\n",
      "JF JF\n",
      "YQ YQ\n",
      "RB RB\n",
      "OU OL\n",
      "LS LS\n",
      "EF EF\n",
      "GC GX\n",
      "ZZ ZZ\n",
      "VY VY\n",
      "NU NU\n",
      "UG UG\n",
      "MT MT\n",
      "AY AY\n",
      "GG GG\n",
      "IG IG\n",
      "QJ QJ\n",
      "WL WL\n",
      "WJ WJ\n",
      "NV NV\n",
      "EH EH\n",
      "XM XM\n",
      "XF XF\n",
      "RK RK\n",
      "RW RW\n",
      "RI RI\n",
      "GC GX\n",
      "BT BT\n",
      "RL RL\n",
      "TJ TJ\n",
      "RH RH\n",
      "WL WL\n",
      "QN QN\n",
      "LV LV\n",
      "QY QY\n",
      "HT HT\n",
      "CF CF\n",
      "XI XI\n",
      "JB JB\n",
      "DH DH\n",
      "QY QY\n",
      "WN WN\n",
      "QQ QQ\n",
      "ZU ZU\n",
      "ZR ZR\n",
      "VM VM\n",
      "AD AD\n",
      "TT TT\n",
      "YZ YZ\n",
      "NN NN\n",
      "VC VC\n",
      "LS LS\n",
      "VE VE\n",
      "ET ET\n",
      "IP IP\n",
      "RQ RQ\n",
      "NC NC\n",
      "XU XU\n",
      "UV UV\n",
      "ZM ZM\n",
      "RL RL\n",
      "JG JG\n",
      "SY SY\n",
      "GV GV\n",
      "TZ TZ\n",
      "JC JC\n",
      "DO DO\n",
      "FW FW\n",
      "ST ST\n",
      "UJ UJ\n",
      "SU SU\n",
      "XV XV\n",
      "AB AB\n",
      "SF SF\n",
      "GV GV\n",
      "BH BH\n",
      "QH QH\n",
      "VZ VZ\n",
      "GP GP\n",
      "IY IY\n",
      "PD PD\n",
      "TX TX\n",
      "OY OY\n",
      "OZ OZ\n",
      "WU WU\n",
      "OO OO\n",
      "YU YU\n",
      "NY NY\n",
      "BL BL\n",
      "ZW ZW\n",
      "FD FD\n",
      "YV YV\n",
      "MC MC\n",
      "OU OL\n",
      "NP NP\n",
      "BQ BQ\n",
      "AE AE\n",
      "BS BS\n",
      "LE LE\n",
      "TK TK\n",
      "AL AL\n",
      "DK DK\n",
      "SU SU\n",
      "XO XO\n",
      "MV MV\n",
      "TU TU\n",
      "DN DN\n",
      "YN YN\n",
      "WG WG\n",
      "NP NP\n",
      "FU FU\n",
      "MQ MQ\n",
      "TV TV\n",
      "FI FI\n",
      "WZ WZ\n",
      "NO NO\n",
      "RW RW\n",
      "YL YL\n",
      "OI OI\n",
      "CH CH\n",
      "LH LH\n",
      "PH PH\n",
      "NZ NZ\n",
      "FB FB\n",
      "MT MT\n",
      "VL VL\n",
      "PX PX\n",
      "NK NK\n",
      "IK IK\n",
      "UO UO\n",
      "KC KC\n",
      "GO GO\n",
      "KV KV\n",
      "BK BK\n",
      "AO AO\n",
      "WY WY\n",
      "VC VC\n",
      "HZ HZ\n",
      "QT QT\n",
      "ZP ZP\n",
      "EL EL\n",
      "HW HW\n",
      "UK UK\n",
      "LJ LJ\n",
      "VI VI\n",
      "IN IN\n",
      "XS XS\n",
      "CD CD\n",
      "FN FN\n",
      "UE UE\n",
      "MR MR\n",
      "OU OU\n",
      "JO JO\n",
      "RC RC\n",
      "OH OH\n",
      "YC YC\n",
      "LF LF\n",
      "IL IL\n",
      "RE RE\n",
      "EN EN\n",
      "ES ES\n",
      "KR KR\n",
      "PZ PZ\n",
      "OD OD\n",
      "FL FL\n",
      "XV XV\n",
      "ET ET\n",
      "QZ QZ\n",
      "GO GO\n",
      "KA KA\n",
      "BE BE\n",
      "PX PX\n",
      "OT OT\n",
      "EY EY\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prediction_list)):\n",
    "    print(\"\".join(prediction_list[i]), \"\".join(label_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " ('VK', 'VX'),\n",
       " ('WM', 'QC'),\n",
       " ('SQ', 'XQ'),\n",
       " ('FE', 'EZ'),\n",
       " ('UO', 'RX'),\n",
       " ('OU', 'OL'),\n",
       " ('TG', 'ZC'),\n",
       " ('VT', 'YT'),\n",
       " ('GC', 'GX'),\n",
       " ('SO', 'ZO'),\n",
       " ('TL', 'PK'),\n",
       " ('LL', 'SL'),\n",
       " ('HL', 'HU'),\n",
       " ('OG', 'OS'),\n",
       " ('HC', 'DS'),\n",
       " ('UE', 'CQ'),\n",
       " ('HD', 'WD'),\n",
       " ('SA', 'SH')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n",
    "output = []\n",
    "for x in diff:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick an error see if the model can predict it well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [\"SA\"]\n",
    "for i in range(1000):\n",
    "    temp.append(\"SA\")\n",
    "x_test, y_test, y_test_small = data_test(temp)\n",
    "prediction_list, label_list = predict_results_only_2(model, x_test, y_test)\n",
    "diff = list(map(find_diff, prediction_list, label_list))\n",
    "output = []\n",
    "for x in diff:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 3 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As 3 letters have 17576 different combinations, we set the for loop times to 20000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x_train, y_train, y_train_small = data_genelization(sample_size = 3, loops = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 1s 42us/step - loss: 0.2133 - acc: 0.9441\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.1243 - acc: 0.9621\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 1s 28us/step - loss: 0.0866 - acc: 0.9695\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0629 - acc: 0.9783\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.0479 - acc: 0.9841\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 0.0368 - acc: 0.9882\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0202 - acc: 0.9941\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0140 - acc: 0.9963\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0090 - acc: 0.9979\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0055 - acc: 0.9990\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 1s 30us/step - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0019 - acc: 0.9999\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 7.2790e-04 - acc: 1.0000\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 4.6762e-04 - acc: 1.0000\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 3.0626e-04 - acc: 1.0000\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 2.0392e-04 - acc: 1.0000\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 1s 27us/step - loss: 1.3773e-04 - acc: 1.0000\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 1s 29us/step - loss: 9.3850e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb26751be0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, index3, size = 26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2], I2L[index3]])\n",
    "\n",
    "\n",
    "def predict_results_only_3(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    index3 = np.argmax(predictions[:, 52:-1], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "    label3 = np.argmax(y_train[:, 52:-1], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2, index3))\n",
    "    label_list = list(map(num2str, label1, label2, label3))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2):   \n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        return(\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 81us/step\n",
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test, y_test_small = data_genelization(sample_size = 3)\n",
    "prediction_list, label_list = predict_results_only_3(model, x_test, y_test)\n",
    "#prediction_list, label_list = predict_results_only_3(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 6 predictions:\n",
      "WTG WTG\n",
      "WHL WHL\n",
      "QUE QUE\n",
      "PHU PHU\n",
      "IJY IJY\n",
      "XTE XTE\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 6 predictions:\")\n",
    "for i in range(6):\n",
    "    print(\"\".join(prediction_list[i]), \"\".join(label_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show some errors:\n",
      "('MQD', 'MQA')\n",
      "('KSO', 'KSA')\n",
      "('NSO', 'NSA')\n",
      "('DGC', 'DGA')\n",
      "('ZMC', 'ZMA')\n",
      "('TUO', 'TUA')\n",
      "The num of total errors: 36\n",
      "The accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n",
    "output = []\n",
    "for x in diff:\n",
    "    if x not in output:\n",
    "        output.append(x)\n",
    "print(\"Show some errors:\")\n",
    "output.remove(True)\n",
    "for i in range(6):\n",
    "    print(output[i])\n",
    "print(\"The num of total errors:\", len(output))\n",
    "print(\"The accuracy:\", 1-len(output)/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 4 letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = data_genelization(sample_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 0s 486us/step - loss: 0.6649 - acc: 0.7153\n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.5365 - acc: 0.9304\n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.2811 - acc: 0.9611\n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1722 - acc: 0.9615\n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1655 - acc: 0.9615\n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1636 - acc: 0.9615\n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1626 - acc: 0.9615\n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1612 - acc: 0.9615\n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1607 - acc: 0.9615\n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1601 - acc: 0.9615\n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1596 - acc: 0.9615\n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1590 - acc: 0.9615\n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1583 - acc: 0.9615\n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.1576 - acc: 0.9615\n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1568 - acc: 0.9615\n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1559 - acc: 0.9615\n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1549 - acc: 0.9615\n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1538 - acc: 0.9615\n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.1526 - acc: 0.9615\n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1512 - acc: 0.9615\n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1497 - acc: 0.9615\n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1481 - acc: 0.9615\n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1463 - acc: 0.9615\n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1445 - acc: 0.9615\n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1425 - acc: 0.9615\n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1405 - acc: 0.9615\n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1385 - acc: 0.9615\n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1363 - acc: 0.9615\n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1342 - acc: 0.9615\n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1320 - acc: 0.9615\n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1299 - acc: 0.9616\n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1278 - acc: 0.9616\n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1256 - acc: 0.9617\n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1235 - acc: 0.9618\n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1214 - acc: 0.9619\n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.1194 - acc: 0.9621\n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1173 - acc: 0.9624\n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.1153 - acc: 0.9627\n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.1133 - acc: 0.9631\n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.1113 - acc: 0.9636\n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.1094 - acc: 0.9638\n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.1075 - acc: 0.9643\n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.1057 - acc: 0.9646\n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.1039 - acc: 0.9650\n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1022 - acc: 0.9655\n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.1005 - acc: 0.9659\n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0988 - acc: 0.9663\n",
      "Epoch 49/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0973 - acc: 0.9669\n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0958 - acc: 0.9673\n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0942 - acc: 0.9678\n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0929 - acc: 0.9682\n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0915 - acc: 0.9686\n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0901 - acc: 0.9690\n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0888 - acc: 0.9694\n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0875 - acc: 0.9697\n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0862 - acc: 0.9701\n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0850 - acc: 0.9705\n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0838 - acc: 0.9708\n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0826 - acc: 0.9711\n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0815 - acc: 0.9716\n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0804 - acc: 0.9720\n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0793 - acc: 0.9725\n",
      "Epoch 64/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0783 - acc: 0.9728\n",
      "Epoch 65/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0772 - acc: 0.9732\n",
      "Epoch 66/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0761 - acc: 0.9735\n",
      "Epoch 67/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0751 - acc: 0.9740\n",
      "Epoch 68/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0741 - acc: 0.9742\n",
      "Epoch 69/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0731 - acc: 0.9747\n",
      "Epoch 70/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0721 - acc: 0.9750\n",
      "Epoch 71/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0713 - acc: 0.9754\n",
      "Epoch 72/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0704 - acc: 0.9758\n",
      "Epoch 73/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0695 - acc: 0.9763\n",
      "Epoch 74/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0686 - acc: 0.9765\n",
      "Epoch 75/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0678 - acc: 0.9768\n",
      "Epoch 76/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0670 - acc: 0.9771\n",
      "Epoch 77/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0662 - acc: 0.9774\n",
      "Epoch 78/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0654 - acc: 0.9779\n",
      "Epoch 79/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0646 - acc: 0.9780\n",
      "Epoch 80/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0639 - acc: 0.9783\n",
      "Epoch 81/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0631 - acc: 0.9787\n",
      "Epoch 82/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0625 - acc: 0.9788\n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0618 - acc: 0.9792\n",
      "Epoch 84/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0611 - acc: 0.9794\n",
      "Epoch 85/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0603 - acc: 0.9796\n",
      "Epoch 86/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0597 - acc: 0.9800\n",
      "Epoch 87/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0589 - acc: 0.9802\n",
      "Epoch 88/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0582 - acc: 0.9805\n",
      "Epoch 89/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0576 - acc: 0.9805\n",
      "Epoch 90/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0570 - acc: 0.9809\n",
      "Epoch 91/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0563 - acc: 0.9812\n",
      "Epoch 92/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0557 - acc: 0.9814\n",
      "Epoch 93/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0551 - acc: 0.9815\n",
      "Epoch 94/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0544 - acc: 0.9819\n",
      "Epoch 95/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0538 - acc: 0.9820\n",
      "Epoch 96/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0532 - acc: 0.9821\n",
      "Epoch 97/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0527 - acc: 0.9823\n",
      "Epoch 98/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0521 - acc: 0.9827\n",
      "Epoch 99/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0516 - acc: 0.9829\n",
      "Epoch 100/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0510 - acc: 0.9827\n",
      "Epoch 101/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0504 - acc: 0.9832\n",
      "Epoch 102/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0499 - acc: 0.9834\n",
      "Epoch 103/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0494 - acc: 0.9835\n",
      "Epoch 104/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0489 - acc: 0.9837\n",
      "Epoch 105/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0484 - acc: 0.9841\n",
      "Epoch 106/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0479 - acc: 0.9842\n",
      "Epoch 107/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0474 - acc: 0.9843\n",
      "Epoch 108/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0470 - acc: 0.9845\n",
      "Epoch 109/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0464 - acc: 0.9846\n",
      "Epoch 110/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0461 - acc: 0.9849\n",
      "Epoch 111/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0456 - acc: 0.9851\n",
      "Epoch 112/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0451 - acc: 0.9851\n",
      "Epoch 113/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0447 - acc: 0.9854\n",
      "Epoch 114/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0442 - acc: 0.9856\n",
      "Epoch 115/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0438 - acc: 0.9857\n",
      "Epoch 116/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0434 - acc: 0.9859\n",
      "Epoch 117/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0430 - acc: 0.9860\n",
      "Epoch 118/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0427 - acc: 0.9863\n",
      "Epoch 119/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0421 - acc: 0.9864\n",
      "Epoch 120/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0417 - acc: 0.9865\n",
      "Epoch 121/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0413 - acc: 0.9867\n",
      "Epoch 122/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0409 - acc: 0.9869\n",
      "Epoch 123/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0406 - acc: 0.9869\n",
      "Epoch 124/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0401 - acc: 0.9872\n",
      "Epoch 125/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0399 - acc: 0.9873\n",
      "Epoch 126/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0395 - acc: 0.9873\n",
      "Epoch 127/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0390 - acc: 0.9875\n",
      "Epoch 128/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0387 - acc: 0.9875\n",
      "Epoch 129/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0383 - acc: 0.9877\n",
      "Epoch 130/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0379 - acc: 0.9881\n",
      "Epoch 131/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0376 - acc: 0.9881\n",
      "Epoch 132/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0373 - acc: 0.9879\n",
      "Epoch 133/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0370 - acc: 0.9882\n",
      "Epoch 134/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0365 - acc: 0.9883\n",
      "Epoch 135/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0363 - acc: 0.9886\n",
      "Epoch 136/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0359 - acc: 0.9885\n",
      "Epoch 137/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0356 - acc: 0.9886\n",
      "Epoch 138/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0353 - acc: 0.9888\n",
      "Epoch 139/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0350 - acc: 0.9890\n",
      "Epoch 140/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0346 - acc: 0.9891\n",
      "Epoch 141/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0343 - acc: 0.9891\n",
      "Epoch 142/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0340 - acc: 0.9893\n",
      "Epoch 143/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0337 - acc: 0.9895\n",
      "Epoch 144/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0333 - acc: 0.9895\n",
      "Epoch 145/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0331 - acc: 0.9897\n",
      "Epoch 146/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0328 - acc: 0.9898\n",
      "Epoch 147/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0325 - acc: 0.9899\n",
      "Epoch 148/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0322 - acc: 0.9900\n",
      "Epoch 149/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0319 - acc: 0.9901\n",
      "Epoch 150/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0315 - acc: 0.9901\n",
      "Epoch 151/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0314 - acc: 0.9903\n",
      "Epoch 152/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0310 - acc: 0.9903\n",
      "Epoch 153/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0308 - acc: 0.9904\n",
      "Epoch 154/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0305 - acc: 0.9903\n",
      "Epoch 155/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0302 - acc: 0.9909\n",
      "Epoch 156/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0300 - acc: 0.9907\n",
      "Epoch 157/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0298 - acc: 0.9906\n",
      "Epoch 158/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0295 - acc: 0.9909\n",
      "Epoch 159/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0293 - acc: 0.9908\n",
      "Epoch 160/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0289 - acc: 0.9911\n",
      "Epoch 161/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0288 - acc: 0.9909\n",
      "Epoch 162/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0285 - acc: 0.9911\n",
      "Epoch 163/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0283 - acc: 0.9913\n",
      "Epoch 164/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0280 - acc: 0.9913\n",
      "Epoch 165/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0278 - acc: 0.9914\n",
      "Epoch 166/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0275 - acc: 0.9915\n",
      "Epoch 167/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0274 - acc: 0.9913\n",
      "Epoch 168/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0272 - acc: 0.9916\n",
      "Epoch 169/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0270 - acc: 0.9917\n",
      "Epoch 170/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0267 - acc: 0.9918\n",
      "Epoch 171/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0265 - acc: 0.9920\n",
      "Epoch 172/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0262 - acc: 0.9919\n",
      "Epoch 173/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0261 - acc: 0.9922\n",
      "Epoch 174/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0258 - acc: 0.9921\n",
      "Epoch 175/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0257 - acc: 0.9923\n",
      "Epoch 176/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0254 - acc: 0.9925\n",
      "Epoch 177/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0252 - acc: 0.9924\n",
      "Epoch 178/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0250 - acc: 0.9924\n",
      "Epoch 179/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0249 - acc: 0.9925\n",
      "Epoch 180/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0246 - acc: 0.9924\n",
      "Epoch 181/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0244 - acc: 0.9927\n",
      "Epoch 182/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0242 - acc: 0.9928\n",
      "Epoch 183/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0240 - acc: 0.9928\n",
      "Epoch 184/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0239 - acc: 0.9929\n",
      "Epoch 185/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0236 - acc: 0.9930\n",
      "Epoch 186/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0235 - acc: 0.9931\n",
      "Epoch 187/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0233 - acc: 0.9931\n",
      "Epoch 188/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0231 - acc: 0.9934\n",
      "Epoch 189/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0229 - acc: 0.9935\n",
      "Epoch 190/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0227 - acc: 0.9934\n",
      "Epoch 191/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0225 - acc: 0.9935\n",
      "Epoch 192/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0224 - acc: 0.9935\n",
      "Epoch 193/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0223 - acc: 0.9935\n",
      "Epoch 194/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0220 - acc: 0.9935\n",
      "Epoch 195/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0219 - acc: 0.9938\n",
      "Epoch 196/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0216 - acc: 0.9937\n",
      "Epoch 197/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0216 - acc: 0.9939\n",
      "Epoch 198/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0214 - acc: 0.9940\n",
      "Epoch 199/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0212 - acc: 0.9940\n",
      "Epoch 200/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0210 - acc: 0.9941\n",
      "Epoch 201/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0209 - acc: 0.9941\n",
      "Epoch 202/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0206 - acc: 0.9942\n",
      "Epoch 203/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0205 - acc: 0.9943\n",
      "Epoch 204/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0203 - acc: 0.9941\n",
      "Epoch 205/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0202 - acc: 0.9943\n",
      "Epoch 206/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0201 - acc: 0.9944\n",
      "Epoch 207/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0199 - acc: 0.9945\n",
      "Epoch 208/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0197 - acc: 0.9946\n",
      "Epoch 209/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0196 - acc: 0.9947\n",
      "Epoch 210/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0195 - acc: 0.9945\n",
      "Epoch 211/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0193 - acc: 0.9947\n",
      "Epoch 212/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0192 - acc: 0.9947\n",
      "Epoch 213/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0191 - acc: 0.9948\n",
      "Epoch 214/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0189 - acc: 0.9947\n",
      "Epoch 215/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0187 - acc: 0.9950\n",
      "Epoch 216/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0186 - acc: 0.9950\n",
      "Epoch 217/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0185 - acc: 0.9949\n",
      "Epoch 218/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0184 - acc: 0.9951\n",
      "Epoch 219/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0182 - acc: 0.9949\n",
      "Epoch 220/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0181 - acc: 0.9951\n",
      "Epoch 221/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0180 - acc: 0.9953\n",
      "Epoch 222/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0179 - acc: 0.9952\n",
      "Epoch 223/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0177 - acc: 0.9952\n",
      "Epoch 224/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0176 - acc: 0.9953\n",
      "Epoch 225/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0175 - acc: 0.9952\n",
      "Epoch 226/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0173 - acc: 0.9956\n",
      "Epoch 227/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0172 - acc: 0.9953\n",
      "Epoch 228/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0171 - acc: 0.9954\n",
      "Epoch 229/300\n",
      "1000/1000 [==============================] - 0s 39us/step - loss: 0.0170 - acc: 0.9955\n",
      "Epoch 230/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0168 - acc: 0.9956\n",
      "Epoch 231/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0167 - acc: 0.9957\n",
      "Epoch 232/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0167 - acc: 0.9956\n",
      "Epoch 233/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0165 - acc: 0.9956\n",
      "Epoch 234/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0164 - acc: 0.9956\n",
      "Epoch 235/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0163 - acc: 0.9955\n",
      "Epoch 236/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0161 - acc: 0.9959\n",
      "Epoch 237/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0160 - acc: 0.9959\n",
      "Epoch 238/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0160 - acc: 0.9958\n",
      "Epoch 239/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0158 - acc: 0.9960\n",
      "Epoch 240/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0157 - acc: 0.9959\n",
      "Epoch 241/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0156 - acc: 0.9961\n",
      "Epoch 242/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0156 - acc: 0.9960\n",
      "Epoch 243/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0154 - acc: 0.9960\n",
      "Epoch 244/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0153 - acc: 0.9962\n",
      "Epoch 245/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0152 - acc: 0.9962\n",
      "Epoch 246/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0151 - acc: 0.9962\n",
      "Epoch 247/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0150 - acc: 0.9961\n",
      "Epoch 248/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0148 - acc: 0.9963\n",
      "Epoch 249/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0147 - acc: 0.9964\n",
      "Epoch 250/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0147 - acc: 0.9963\n",
      "Epoch 251/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0146 - acc: 0.9964\n",
      "Epoch 252/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0144 - acc: 0.9967\n",
      "Epoch 253/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0144 - acc: 0.9964\n",
      "Epoch 254/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0143 - acc: 0.9965\n",
      "Epoch 255/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0142 - acc: 0.9965\n",
      "Epoch 256/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0141 - acc: 0.9966\n",
      "Epoch 257/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0140 - acc: 0.9967\n",
      "Epoch 258/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0139 - acc: 0.9967\n",
      "Epoch 259/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0139 - acc: 0.9966\n",
      "Epoch 260/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0137 - acc: 0.9967\n",
      "Epoch 261/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0136 - acc: 0.9967\n",
      "Epoch 262/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0135 - acc: 0.9969\n",
      "Epoch 263/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0135 - acc: 0.9969\n",
      "Epoch 264/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0134 - acc: 0.9969\n",
      "Epoch 265/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0133 - acc: 0.9971\n",
      "Epoch 266/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0132 - acc: 0.9970\n",
      "Epoch 267/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0131 - acc: 0.9970\n",
      "Epoch 268/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0130 - acc: 0.9970\n",
      "Epoch 269/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0130 - acc: 0.9970\n",
      "Epoch 270/300\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0128 - acc: 0.9971\n",
      "Epoch 271/300\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 0.0128 - acc: 0.9971\n",
      "Epoch 272/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0128 - acc: 0.9970\n",
      "Epoch 273/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0127 - acc: 0.9970\n",
      "Epoch 274/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 275/300\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 276/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0123 - acc: 0.9973\n",
      "Epoch 277/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0123 - acc: 0.9971\n",
      "Epoch 278/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0122 - acc: 0.9973\n",
      "Epoch 279/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 280/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0121 - acc: 0.9973\n",
      "Epoch 281/300\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 0.0120 - acc: 0.9972\n",
      "Epoch 282/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0119 - acc: 0.9976\n",
      "Epoch 283/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0119 - acc: 0.9975\n",
      "Epoch 284/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0117 - acc: 0.9974\n",
      "Epoch 285/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0117 - acc: 0.9973\n",
      "Epoch 286/300\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 0.0116 - acc: 0.9976\n",
      "Epoch 287/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0115 - acc: 0.9976\n",
      "Epoch 288/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0114 - acc: 0.9976\n",
      "Epoch 289/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0114 - acc: 0.9977\n",
      "Epoch 290/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0113 - acc: 0.9976\n",
      "Epoch 291/300\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0112 - acc: 0.9977\n",
      "Epoch 292/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0112 - acc: 0.9976\n",
      "Epoch 293/300\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 294/300\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 0.0110 - acc: 0.9977\n",
      "Epoch 295/300\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 0.0110 - acc: 0.9977\n",
      "Epoch 296/300\n",
      "1000/1000 [==============================] - 0s 40us/step - loss: 0.0108 - acc: 0.9979\n",
      "Epoch 297/300\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 0.0108 - acc: 0.9978\n",
      "Epoch 298/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0108 - acc: 0.9977\n",
      "Epoch 299/300\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 0.0107 - acc: 0.9979\n",
      "Epoch 300/300\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 0.0106 - acc: 0.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb276fff98>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we should not trust the accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning the deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import data_test\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let strat with the simpliest model, say our alphabet has there letter ABC and the \"caeser\" function is to shift one place, ie. shift ABC to BCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = data_genelization(sample_size = 2,loops = 1000, size = 3, key = 1, x_as_vector = False, y_as_vector = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=x_train.shape[1], activation='relu', name = \"input\"))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid', name = \"layer1\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1000/1000 [==============================] - 0s 404us/step - loss: 0.6844 - acc: 0.5315\n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6607 - acc: 0.6298\n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6399 - acc: 0.6413\n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.6211 - acc: 0.6878\n",
      "Epoch 5/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.6035 - acc: 0.7145\n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.5869 - acc: 0.7213\n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.5693 - acc: 0.7372\n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.5501 - acc: 0.7745\n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.5325 - acc: 0.7772\n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.5151 - acc: 0.7772\n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.4953 - acc: 0.7778\n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4762 - acc: 0.8033\n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.4584 - acc: 0.8290\n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4419 - acc: 0.8453\n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.4261 - acc: 0.8580\n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.4107 - acc: 0.8520\n",
      "Epoch 17/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3957 - acc: 0.8827\n",
      "Epoch 18/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3807 - acc: 0.8807\n",
      "Epoch 19/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3658 - acc: 0.8938\n",
      "Epoch 20/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3510 - acc: 0.8987\n",
      "Epoch 21/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3362 - acc: 0.9167\n",
      "Epoch 22/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.3215 - acc: 0.9277\n",
      "Epoch 23/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.3069 - acc: 0.9518\n",
      "Epoch 24/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2928 - acc: 0.9583\n",
      "Epoch 25/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2787 - acc: 0.9583\n",
      "Epoch 26/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2646 - acc: 0.9783\n",
      "Epoch 27/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2507 - acc: 0.9833\n",
      "Epoch 28/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2369 - acc: 1.0000\n",
      "Epoch 29/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.2234 - acc: 1.0000\n",
      "Epoch 30/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.2103 - acc: 1.0000\n",
      "Epoch 31/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1970 - acc: 1.0000\n",
      "Epoch 32/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.1841 - acc: 1.0000\n",
      "Epoch 33/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1722 - acc: 1.0000\n",
      "Epoch 34/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1612 - acc: 1.0000\n",
      "Epoch 35/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1512 - acc: 1.0000\n",
      "Epoch 36/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1421 - acc: 1.0000\n",
      "Epoch 37/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1340 - acc: 1.0000\n",
      "Epoch 38/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1267 - acc: 1.0000\n",
      "Epoch 39/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1202 - acc: 1.0000\n",
      "Epoch 40/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1144 - acc: 1.0000\n",
      "Epoch 41/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1091 - acc: 1.0000\n",
      "Epoch 42/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.1043 - acc: 1.0000\n",
      "Epoch 43/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.1000 - acc: 1.0000\n",
      "Epoch 44/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0959 - acc: 1.0000\n",
      "Epoch 45/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0922 - acc: 1.0000\n",
      "Epoch 46/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0888 - acc: 1.0000\n",
      "Epoch 47/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0856 - acc: 1.0000\n",
      "Epoch 48/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0827 - acc: 1.0000\n",
      "Epoch 49/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0799 - acc: 1.0000\n",
      "Epoch 50/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0773 - acc: 1.0000\n",
      "Epoch 51/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0749 - acc: 1.0000\n",
      "Epoch 52/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0726 - acc: 1.0000\n",
      "Epoch 53/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0704 - acc: 1.0000\n",
      "Epoch 54/150\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0684 - acc: 1.0000\n",
      "Epoch 55/150\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 0.0665 - acc: 1.0000\n",
      "Epoch 56/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0647 - acc: 1.0000\n",
      "Epoch 57/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0629 - acc: 1.0000\n",
      "Epoch 58/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0613 - acc: 1.0000\n",
      "Epoch 59/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0597 - acc: 1.0000\n",
      "Epoch 60/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0582 - acc: 1.0000\n",
      "Epoch 61/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0568 - acc: 1.0000\n",
      "Epoch 62/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0554 - acc: 1.0000\n",
      "Epoch 63/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0541 - acc: 1.0000\n",
      "Epoch 64/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0529 - acc: 1.0000\n",
      "Epoch 65/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0517 - acc: 1.0000\n",
      "Epoch 66/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0505 - acc: 1.0000\n",
      "Epoch 67/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0494 - acc: 1.0000\n",
      "Epoch 68/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0483 - acc: 1.0000\n",
      "Epoch 69/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0473 - acc: 1.0000\n",
      "Epoch 70/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0463 - acc: 1.0000\n",
      "Epoch 71/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0453 - acc: 1.0000\n",
      "Epoch 72/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0444 - acc: 1.0000\n",
      "Epoch 73/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0435 - acc: 1.0000\n",
      "Epoch 74/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 75/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0417 - acc: 1.0000\n",
      "Epoch 76/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 77/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0401 - acc: 1.0000\n",
      "Epoch 78/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0394 - acc: 1.0000\n",
      "Epoch 79/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0386 - acc: 1.0000\n",
      "Epoch 80/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0379 - acc: 1.0000\n",
      "Epoch 81/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0372 - acc: 1.0000\n",
      "Epoch 82/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0365 - acc: 1.0000\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0358 - acc: 1.0000\n",
      "Epoch 84/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0352 - acc: 1.0000\n",
      "Epoch 85/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0345 - acc: 1.0000\n",
      "Epoch 86/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0339 - acc: 1.0000\n",
      "Epoch 87/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0333 - acc: 1.0000\n",
      "Epoch 88/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0327 - acc: 1.0000\n",
      "Epoch 89/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0321 - acc: 1.0000\n",
      "Epoch 90/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0316 - acc: 1.0000\n",
      "Epoch 91/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0311 - acc: 1.0000\n",
      "Epoch 92/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0305 - acc: 1.0000\n",
      "Epoch 93/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 94/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0295 - acc: 1.0000\n",
      "Epoch 95/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 96/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 97/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0281 - acc: 1.0000\n",
      "Epoch 98/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 99/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0271 - acc: 1.0000\n",
      "Epoch 100/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0267 - acc: 1.0000\n",
      "Epoch 101/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 102/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0258 - acc: 1.0000\n",
      "Epoch 103/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 104/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0250 - acc: 1.0000\n",
      "Epoch 105/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 106/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0242 - acc: 1.0000\n",
      "Epoch 107/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0238 - acc: 1.0000\n",
      "Epoch 108/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0234 - acc: 1.0000\n",
      "Epoch 109/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0231 - acc: 1.0000\n",
      "Epoch 110/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0227 - acc: 1.0000\n",
      "Epoch 111/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0223 - acc: 1.0000\n",
      "Epoch 112/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 113/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0216 - acc: 1.0000\n",
      "Epoch 114/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 115/150\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.0210 - acc: 1.0000\n",
      "Epoch 116/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 117/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0203 - acc: 1.0000\n",
      "Epoch 118/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0200 - acc: 1.0000\n",
      "Epoch 119/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0197 - acc: 1.0000\n",
      "Epoch 120/150\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 121/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 122/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 123/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 124/150\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 0.0183 - acc: 1.0000\n",
      "Epoch 125/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 126/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 127/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0174 - acc: 1.0000\n",
      "Epoch 128/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0172 - acc: 1.0000\n",
      "Epoch 129/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 130/150\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 0.0167 - acc: 1.0000\n",
      "Epoch 131/150\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 132/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 133/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0159 - acc: 1.0000\n",
      "Epoch 134/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 135/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0155 - acc: 1.0000\n",
      "Epoch 136/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 137/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 138/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0148 - acc: 1.0000\n",
      "Epoch 139/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 140/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0144 - acc: 1.0000\n",
      "Epoch 141/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 142/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 143/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 144/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 145/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0134 - acc: 1.0000\n",
      "Epoch 146/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0132 - acc: 1.0000\n",
      "Epoch 147/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0130 - acc: 1.0000\n",
      "Epoch 148/150\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 149/150\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 150/150\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 0.0124 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2cb62940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what did the model learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Dense)                (None, 5)                 35        \n",
      "_________________________________________________________________\n",
      "layer1 (Dense)               (None, 6)                 36        \n",
      "=================================================================\n",
      "Total params: 71\n",
      "Trainable params: 71\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that we have 71 parameters. In the first layer, we have 30 weights and 5 biases. In the second layer, we have 30 weights and 6 biases which show as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.5230295 , -2.3722856 , -0.91483843,  0.9904725 , -2.0497599 ],\n",
       "        [ 0.5239528 , -2.0380375 ,  2.315268  ,  0.71013767,  2.8065956 ],\n",
       "        [ 0.49922538,  1.9143298 , -0.91098964,  0.9905842 ,  0.5758578 ],\n",
       "        [-1.188174  ,  1.0506359 , -0.2596239 , -2.4396398 ,  0.8528697 ],\n",
       "        [-1.1856221 ,  0.7440861 , -0.53387815,  1.9751233 ,  2.1526575 ],\n",
       "        [ 2.0391617 ,  1.6623969 ,  2.4390411 ,  0.5798911 , -0.12870102]],\n",
       "       dtype=float32),\n",
       " array([0.6622595, 1.3193262, 1.170595 , 1.4491769, 1.1961169],\n",
       "       dtype=float32),\n",
       " array([[-0.1914282 ,  0.06342135,  0.9865053 , -1.7767843 ,  2.4452999 ,\n",
       "         -0.830269  ],\n",
       "        [-2.1160614 ,  2.7547426 , -2.2330601 , -1.1951566 ,  0.20668276,\n",
       "          0.33688462],\n",
       "        [ 1.3490559 , -1.2889181 , -1.0776172 , -1.7162324 ,  1.3834927 ,\n",
       "         -0.3855466 ],\n",
       "        [-1.6822165 , -0.6808207 ,  1.5137229 ,  2.424756  , -0.26901057,\n",
       "         -2.9711552 ],\n",
       "        [ 1.9215815 , -0.06922591, -2.0719502 ,  0.56272125, -1.7760575 ,\n",
       "          0.83846855]], dtype=float32),\n",
       " array([-2.2662177, -2.2439988,  2.5550244, -2.4377017, -2.3810773,\n",
       "         2.371451 ], dtype=float32)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.layers[0].get_weights()[0]\n",
    "biases = model.layers[0].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# Set one layer as output we can get every layers' information given the input\n",
    "\n",
    "dense1_layer_model = Model(inputs=model.input,\n",
    "                                     outputs=model.get_layer('input').output)\n",
    "\n",
    "dense1_output = dense1_layer_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 4.2842922e+00, 0.0000000e+00, 1.2123585e-04,\n",
       "       2.6248446e+00], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2_layer_model = Model(inputs=model.input, outputs=model.get_layer('layer1').output)\n",
    "\n",
    "dense2_output = dense2_layer_model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.8543303e-03, 9.9991524e-01, 3.9637089e-06, 2.2812188e-03,\n",
       "       2.1130741e-03, 9.9756479e-01], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense2_output[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, after calculating by two layers weights, the training data reduce the 1 value in the origin place and increase the value corresponding to the label place with value 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bad activation function and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we choose a bad activation function what would happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(5, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 0s 33us/step - loss: 1.3923 - acc: 0.5190\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 1.3921 - acc: 0.5730\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3920 - acc: 0.5770\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3918 - acc: 0.6490\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3917 - acc: 0.5380\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3915 - acc: 0.5770\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3914 - acc: 0.5210\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3912 - acc: 0.5180\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3911 - acc: 0.5170\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3910 - acc: 0.5410\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3908 - acc: 0.5460\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3907 - acc: 0.5910\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3906 - acc: 0.5480\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3905 - acc: 0.4830\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3904 - acc: 0.5360\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3903 - acc: 0.5970\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3901 - acc: 0.4690\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3900 - acc: 0.4650\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3900 - acc: 0.5570\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3899 - acc: 0.5250\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3898 - acc: 0.5220\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3897 - acc: 0.5380\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3896 - acc: 0.4790\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 1.3895 - acc: 0.4760\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 0s 32us/step - loss: 1.3894 - acc: 0.4440\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 1.3894 - acc: 0.4900\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 0s 30us/step - loss: 1.3893 - acc: 0.5160\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3892 - acc: 0.5580\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3891 - acc: 0.5450\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3891 - acc: 0.5070\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3890 - acc: 0.5340\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3889 - acc: 0.5390\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3889 - acc: 0.5210\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3888 - acc: 0.5180\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3888 - acc: 0.4880\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3887 - acc: 0.4890\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3886 - acc: 0.5210\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3886 - acc: 0.5010\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3885 - acc: 0.5300\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3885 - acc: 0.5290\n",
      "Epoch 41/100\n",
      "1000/1000 [==============================] - 0s 29us/step - loss: 1.3884 - acc: 0.4870\n",
      "Epoch 42/100\n",
      "1000/1000 [==============================] - 0s 28us/step - loss: 1.3884 - acc: 0.5300\n",
      "Epoch 43/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3883 - acc: 0.5440\n",
      "Epoch 44/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3883 - acc: 0.4310\n",
      "Epoch 45/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3882 - acc: 0.5140\n",
      "Epoch 46/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3882 - acc: 0.4920\n",
      "Epoch 47/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3882 - acc: 0.4780\n",
      "Epoch 48/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3881 - acc: 0.4940\n",
      "Epoch 49/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3881 - acc: 0.4520\n",
      "Epoch 50/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3880 - acc: 0.4830\n",
      "Epoch 51/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3880 - acc: 0.4680\n",
      "Epoch 52/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3880 - acc: 0.4890\n",
      "Epoch 53/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3879 - acc: 0.5150\n",
      "Epoch 54/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3879 - acc: 0.4190\n",
      "Epoch 55/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3879 - acc: 0.5080\n",
      "Epoch 56/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3878 - acc: 0.3890\n",
      "Epoch 57/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3878 - acc: 0.4620\n",
      "Epoch 58/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3878 - acc: 0.5020\n",
      "Epoch 59/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3877 - acc: 0.4380\n",
      "Epoch 60/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3877 - acc: 0.4660\n",
      "Epoch 61/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3877 - acc: 0.4190\n",
      "Epoch 62/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3876 - acc: 0.5080\n",
      "Epoch 63/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3876 - acc: 0.4940\n",
      "Epoch 64/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3876 - acc: 0.4630\n",
      "Epoch 65/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3876 - acc: 0.4730\n",
      "Epoch 66/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3875 - acc: 0.5370\n",
      "Epoch 67/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3875 - acc: 0.4600\n",
      "Epoch 68/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3875 - acc: 0.5110\n",
      "Epoch 69/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3875 - acc: 0.4180\n",
      "Epoch 70/100\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 1.3874 - acc: 0.5110\n",
      "Epoch 71/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3874 - acc: 0.4470\n",
      "Epoch 72/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3874 - acc: 0.4640\n",
      "Epoch 73/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3874 - acc: 0.4890\n",
      "Epoch 74/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.5020\n",
      "Epoch 75/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.5310\n",
      "Epoch 76/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3873 - acc: 0.4530\n",
      "Epoch 77/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.4820\n",
      "Epoch 78/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3873 - acc: 0.5280\n",
      "Epoch 79/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3872 - acc: 0.5160\n",
      "Epoch 80/100\n",
      "1000/1000 [==============================] - 0s 24us/step - loss: 1.3872 - acc: 0.4530\n",
      "Epoch 81/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3872 - acc: 0.4610\n",
      "Epoch 82/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3872 - acc: 0.4610\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3872 - acc: 0.5130\n",
      "Epoch 84/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3871 - acc: 0.5250\n",
      "Epoch 85/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.5140\n",
      "Epoch 86/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.5170\n",
      "Epoch 87/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.4790\n",
      "Epoch 88/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3871 - acc: 0.4990\n",
      "Epoch 89/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3871 - acc: 0.4380\n",
      "Epoch 90/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3870 - acc: 0.5080\n",
      "Epoch 91/100\n",
      "1000/1000 [==============================] - 0s 25us/step - loss: 1.3870 - acc: 0.4770\n",
      "Epoch 92/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3870 - acc: 0.4990\n",
      "Epoch 93/100\n",
      "1000/1000 [==============================] - 0s 27us/step - loss: 1.3870 - acc: 0.4960\n",
      "Epoch 94/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3870 - acc: 0.4700\n",
      "Epoch 95/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3870 - acc: 0.4960\n",
      "Epoch 96/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3870 - acc: 0.4990\n",
      "Epoch 97/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3869 - acc: 0.4740\n",
      "Epoch 98/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3869 - acc: 0.5200\n",
      "Epoch 99/100\n",
      "1000/1000 [==============================] - 0s 22us/step - loss: 1.3869 - acc: 0.4620\n",
      "Epoch 100/100\n",
      "1000/1000 [==============================] - 0s 23us/step - loss: 1.3869 - acc: 0.4610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2de30400>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the accuracy cannot converge to 1 because the softmax and categorical cross entropy treat the output as a distribution, however, it is not a classfication problem. This time our output has two value should be 1, so it is not suitable to use softmax and categorical crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.17837621e-05, 4.99764025e-01, 5.15444517e-05, 1.03189086e-04,\n",
       "       1.24050755e-04, 4.99925375e-01], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.999999968251359"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.predict(x_train)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 变量 loops, error probs 似乎loops越多 纠错性越好，画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import misslabeled_data_genelization\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, size=26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2]])\n",
    "\n",
    "\n",
    "def predict_results_only_2(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2))\n",
    "    label_list = list(map(num2str, label1, label2))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2, ifprint = False):\n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        if ifprint == True:\n",
    "            print(\"\".join(list1), \"\".join(list2))\n",
    "        return (\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return (True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function misslabeled_data_genelization in module myfun:\n",
      "\n",
      "misslabeled_data_genelization(sample_size=2, loops=1000, size=26, key=3, prob=0.1, x_as_vector=False, y_as_vector=False)\n",
      "    data_genelization(sample_size = 2,loops = 1000, size = 26, key = 3, x_as_vector = False, y_as_vector = False):\n",
      "    return x_train, label with equual size, labels with 1 dimension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(misslabeled_data_genelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=0)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.2733 - acc: 0.9253\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1459 - acc: 0.9615\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0816 - acc: 0.9690\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0575 - acc: 0.9792\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0397 - acc: 0.9866\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0267 - acc: 0.9922\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0180 - acc: 0.9955\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0081 - acc: 0.9987\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0054 - acc: 0.9994\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 8.6940e-04 - acc: 1.0000\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.4030e-04 - acc: 1.0000\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.7999e-04 - acc: 1.0000\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 3.6524e-04 - acc: 1.0000\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 2.8181e-04 - acc: 1.0000\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 2.1865e-04 - acc: 1.0000\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.7182e-04 - acc: 1.0000\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.3591e-04 - acc: 1.0000\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0813e-04 - acc: 1.0000\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 8.6493e-05 - acc: 1.0000\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.9610e-05 - acc: 1.0000\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 5.6196e-05 - acc: 1.0000\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.5588e-05 - acc: 1.0000\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.7033e-05 - acc: 1.0000\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.0210e-05 - acc: 1.0000\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.4694e-05 - acc: 1.0000\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 2.0271e-05 - acc: 1.0000\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.6651e-05 - acc: 1.0000\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.3710e-05 - acc: 1.0000\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.1309e-05 - acc: 1.0000\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 9.3405e-06 - acc: 1.0000\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 7.7300e-06 - acc: 1.0000\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.4087e-06 - acc: 1.0000\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.3226e-06 - acc: 1.0000\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.4261e-06 - acc: 1.0000\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 3.6833e-06 - acc: 1.0000\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 3.0772e-06 - acc: 1.0000\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.5746e-06 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.1571e-06 - acc: 1.0000\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.8133e-06 - acc: 1.0000\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.5278e-06 - acc: 1.0000\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.2922e-06 - acc: 1.0000\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0960e-06 - acc: 1.0000\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 9.3311e-07 - acc: 1.0000\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 7.9692e-07 - acc: 1.0000\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 6.8385e-07 - acc: 1.0000\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.8999e-07 - acc: 1.0000\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.1103e-07 - acc: 1.0000\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 4.4580e-07 - acc: 1.0000\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.9107e-07 - acc: 1.0000\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.4543e-07 - acc: 1.0000\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.0730e-07 - acc: 1.0000\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.7532e-07 - acc: 1.0000\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.4854e-07 - acc: 1.0000\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.2619e-07 - acc: 1.0000\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.0733e-07 - acc: 1.0000\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.9153e-07 - acc: 1.0000\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.7835e-07 - acc: 1.0000\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.6708e-07 - acc: 1.0000\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.5768e-07 - acc: 1.0000\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.4971e-07 - acc: 1.0000\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.4296e-07 - acc: 1.0000\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 1.3726e-07 - acc: 1.0000\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.3242e-07 - acc: 1.0000\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.2837e-07 - acc: 1.0000\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.2486e-07 - acc: 1.0000\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.2192e-07 - acc: 1.0000\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1938e-07 - acc: 1.0000\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1717e-07 - acc: 1.0000\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1528e-07 - acc: 1.0000\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.1367e-07 - acc: 1.0000\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.1229e-07 - acc: 1.0000\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.1110e-07 - acc: 1.0000\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.1002e-07 - acc: 1.0000\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0915e-07 - acc: 1.0000\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0832e-07 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0761e-07 - acc: 1.0000\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0700e-07 - acc: 1.0000\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0646e-07 - acc: 1.0000\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0597e-07 - acc: 1.0000\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0552e-07 - acc: 1.0000\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0514e-07 - acc: 1.0000\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0483e-07 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0452e-07 - acc: 1.0000\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0427e-07 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0401e-07 - acc: 1.0000\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0378e-07 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0358e-07 - acc: 1.0000\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0341e-07 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0325e-07 - acc: 1.0000\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0310e-07 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0296e-07 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0284e-07 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0272e-07 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0261e-07 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0252e-07 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0242e-07 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0233e-07 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0225e-07 - acc: 1.0000\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0217e-07 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0212e-07 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0204e-07 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0198e-07 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0193e-07 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0188e-07 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0182e-07 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0179e-07 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0175e-07 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0171e-07 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0166e-07 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0164e-07 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0160e-07 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0157e-07 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0154e-07 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0150e-07 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0150e-07 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0147e-07 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0145e-07 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0141e-07 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0140e-07 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0137e-07 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0136e-07 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0134e-07 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0132e-07 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.0130e-07 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0128e-07 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0126e-07 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0125e-07 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0124e-07 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0121e-07 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0120e-07 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0119e-07 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0117e-07 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0117e-07 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0116e-07 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0113e-07 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0112e-07 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0113e-07 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0112e-07 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0110e-07 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0110e-07 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0106e-07 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0105e-07 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0104e-07 - acc: 1.0000\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0104e-07 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0103e-07 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0103e-07 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0102e-07 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0102e-07 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0101e-07 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0100e-07 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0100e-07 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0099e-07 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0097e-07 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0096e-07 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0097e-07 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0093e-07 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0093e-07 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0087e-07 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33e8a898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0087433304306614e-07, 1.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.2882 - acc: 0.9191\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1466 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1199 - acc: 0.9620\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0943 - acc: 0.9665\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0768 - acc: 0.9749\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0671 - acc: 0.9809\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0617 - acc: 0.9842\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0583 - acc: 0.9859\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0559 - acc: 0.9873\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0540 - acc: 0.9882\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0524 - acc: 0.9889\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0510 - acc: 0.9896\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0499 - acc: 0.9901\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0489 - acc: 0.9906\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0481 - acc: 0.9910\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0473 - acc: 0.9914\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0467 - acc: 0.9917\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0462 - acc: 0.9918\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0456 - acc: 0.9919\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0452 - acc: 0.9921\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0448 - acc: 0.9922\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0445 - acc: 0.9923\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.0442 - acc: 0.9924\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0439 - acc: 0.9924\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0436 - acc: 0.9925\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0434 - acc: 0.9925\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0431 - acc: 0.9926\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0429 - acc: 0.9926\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0427 - acc: 0.9926\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0425 - acc: 0.9926\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0424 - acc: 0.9926\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0422 - acc: 0.9926\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0420 - acc: 0.9926\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0419 - acc: 0.9927\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0417 - acc: 0.9926\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0416 - acc: 0.9927\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0414 - acc: 0.9927\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0413 - acc: 0.9927\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0412 - acc: 0.9927\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0411 - acc: 0.9927\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0410 - acc: 0.9927\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0409 - acc: 0.9927\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0407 - acc: 0.9927\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0405 - acc: 0.9927\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0404 - acc: 0.9927\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0403 - acc: 0.9927\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0402 - acc: 0.9927\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0401 - acc: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb34edfac8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=0.1)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010234667018055916, 0.9998576957702636]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 0.2886 - acc: 0.9219\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1595 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1543 - acc: 0.9615\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1480 - acc: 0.9615\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1427 - acc: 0.9615\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1390 - acc: 0.9616\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1362 - acc: 0.9616\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1342 - acc: 0.9617\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1325 - acc: 0.9618\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1311 - acc: 0.9619\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1301 - acc: 0.9621\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1293 - acc: 0.9622\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1287 - acc: 0.9623\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1282 - acc: 0.9623\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1278 - acc: 0.9624\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1273 - acc: 0.9624\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1270 - acc: 0.9625\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1266 - acc: 0.9626\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1263 - acc: 0.9627\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1260 - acc: 0.9627\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1257 - acc: 0.9629\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1255 - acc: 0.9628\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1253 - acc: 0.9627\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1251 - acc: 0.9629\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1249 - acc: 0.9629\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1247 - acc: 0.9629\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1246 - acc: 0.9630\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1244 - acc: 0.9629\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1242 - acc: 0.9631\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1240 - acc: 0.9632\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1240 - acc: 0.9631\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1238 - acc: 0.9631\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1236 - acc: 0.9633\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1235 - acc: 0.9632\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1234 - acc: 0.9631\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1233 - acc: 0.9632\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1231 - acc: 0.9632\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9634\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9634\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1228 - acc: 0.9634\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1227 - acc: 0.9635\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1227 - acc: 0.9635\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1226 - acc: 0.9633\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1225 - acc: 0.9634\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1225 - acc: 0.9635\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1224 - acc: 0.9635\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1223 - acc: 0.9634\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1223 - acc: 0.9635\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1222 - acc: 0.9634\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1222 - acc: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb350bccc0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000,prob=0.5)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05628207671642303, 0.9727115383148194]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 0.2870 - acc: 0.9283\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1636 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1634 - acc: 0.9615\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1633 - acc: 0.9615\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1632 - acc: 0.9615\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1631 - acc: 0.9615\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1631 - acc: 0.9615\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1630 - acc: 0.9615\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1629 - acc: 0.9615\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1628 - acc: 0.9615\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1628 - acc: 0.9615\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1626 - acc: 0.9615\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1625 - acc: 0.9615\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1625 - acc: 0.9615\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1622 - acc: 0.9615\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1622 - acc: 0.9615\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1614 - acc: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb356bd9e8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000,prob=1)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16452448892593383, 0.9615384340286255]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)\n",
    "# 他这个accuracy不太对 是计算整体的好像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04283839, 0.03006983, 0.03108829, 0.03086495, 0.03821969,\n",
       "       0.04318464, 0.04275879, 0.02881029, 0.02591237, 0.05666709,\n",
       "       0.04924551, 0.04192016, 0.04666957, 0.04626572, 0.04118401,\n",
       "       0.03622824, 0.02944967, 0.03765562, 0.03575623, 0.03924397,\n",
       "       0.03410533, 0.03530636, 0.04261568, 0.03204012, 0.04352927,\n",
       "       0.03008407, 0.05662912, 0.04116714, 0.04893467, 0.05282199,\n",
       "       0.03043225, 0.04488924, 0.05681634, 0.04138765, 0.05441827,\n",
       "       0.02774709, 0.03407055, 0.04218432, 0.03569707, 0.03512883,\n",
       "       0.04342568, 0.05268621, 0.04676977, 0.02773789, 0.02489308,\n",
       "       0.0423353 , 0.04129213, 0.03324315, 0.03560084, 0.02898619,\n",
       "       0.03714329, 0.036331  ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes hours to get the curves, the python file is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubostibility of deep learning with large size of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "with open('../data/noise_size_500.pickle', 'rb') as f:\n",
    "    info500 = pickle.load(f)\n",
    "with open('../data/noise_size_1000.pickle', 'rb') as f:\n",
    "    info1000 = pickle.load(f)\n",
    "with open('../data/noise_size_10000.pickle', 'rb') as f:\n",
    "    info10000 = pickle.load(f)\n",
    "with open('../data/noise_size_20000.pickle', 'rb') as f:\n",
    "    info20000 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Write some analyse about the rubostiblity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "l2, = plt.plot(np.arange(0,1,.01),info500, label = 'data size 500')\n",
    "l3, = plt.plot(np.arange(0,1,.01),info1000, label = 'data size 1000')\n",
    "l4, = plt.plot(np.arange(0,1,.01),info10000, label = 'data size 10000')\n",
    "l5, = plt.plot(np.arange(0,1,.01),info20000, label = 'data size 20000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Write some analyse about the system error maybe add some codes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
