{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 变量 loops, error probs 似乎loops越多 纠错性越好，画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import misslabeled_data_genelization\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, size=26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2]])\n",
    "\n",
    "\n",
    "def predict_results_only_2(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2))\n",
    "    label_list = list(map(num2str, label1, label2))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2, ifprint = False):\n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        if ifprint == True:\n",
    "            print(\"\".join(list1), \"\".join(list2))\n",
    "        return (\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return (True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function misslabeled_data_genelization in module myfun:\n",
      "\n",
      "misslabeled_data_genelization(sample_size=2, loops=1000, size=26, key=3, prob=0.1, x_as_vector=False, y_as_vector=False)\n",
      "    data_genelization(sample_size = 2,loops = 1000, size = 26, key = 3, x_as_vector = False, y_as_vector = False):\n",
      "    return x_train, label with equual size, labels with 1 dimension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(misslabeled_data_genelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=0)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.2733 - acc: 0.9253\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1459 - acc: 0.9615\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0816 - acc: 0.9690\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0575 - acc: 0.9792\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0397 - acc: 0.9866\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0267 - acc: 0.9922\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0180 - acc: 0.9955\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0081 - acc: 0.9987\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0054 - acc: 0.9994\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 8.6940e-04 - acc: 1.0000\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.4030e-04 - acc: 1.0000\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.7999e-04 - acc: 1.0000\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 3.6524e-04 - acc: 1.0000\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 2.8181e-04 - acc: 1.0000\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 2.1865e-04 - acc: 1.0000\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.7182e-04 - acc: 1.0000\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.3591e-04 - acc: 1.0000\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0813e-04 - acc: 1.0000\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 8.6493e-05 - acc: 1.0000\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.9610e-05 - acc: 1.0000\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 5.6196e-05 - acc: 1.0000\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.5588e-05 - acc: 1.0000\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.7033e-05 - acc: 1.0000\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.0210e-05 - acc: 1.0000\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.4694e-05 - acc: 1.0000\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 2.0271e-05 - acc: 1.0000\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.6651e-05 - acc: 1.0000\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.3710e-05 - acc: 1.0000\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.1309e-05 - acc: 1.0000\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 9.3405e-06 - acc: 1.0000\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 7.7300e-06 - acc: 1.0000\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.4087e-06 - acc: 1.0000\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.3226e-06 - acc: 1.0000\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.4261e-06 - acc: 1.0000\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 3.6833e-06 - acc: 1.0000\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 3.0772e-06 - acc: 1.0000\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.5746e-06 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.1571e-06 - acc: 1.0000\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.8133e-06 - acc: 1.0000\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.5278e-06 - acc: 1.0000\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.2922e-06 - acc: 1.0000\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0960e-06 - acc: 1.0000\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 9.3311e-07 - acc: 1.0000\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 7.9692e-07 - acc: 1.0000\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 6.8385e-07 - acc: 1.0000\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.8999e-07 - acc: 1.0000\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.1103e-07 - acc: 1.0000\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 4.4580e-07 - acc: 1.0000\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.9107e-07 - acc: 1.0000\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.4543e-07 - acc: 1.0000\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.0730e-07 - acc: 1.0000\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.7532e-07 - acc: 1.0000\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.4854e-07 - acc: 1.0000\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.2619e-07 - acc: 1.0000\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.0733e-07 - acc: 1.0000\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.9153e-07 - acc: 1.0000\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.7835e-07 - acc: 1.0000\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.6708e-07 - acc: 1.0000\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.5768e-07 - acc: 1.0000\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.4971e-07 - acc: 1.0000\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.4296e-07 - acc: 1.0000\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 1.3726e-07 - acc: 1.0000\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.3242e-07 - acc: 1.0000\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.2837e-07 - acc: 1.0000\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.2486e-07 - acc: 1.0000\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.2192e-07 - acc: 1.0000\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1938e-07 - acc: 1.0000\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1717e-07 - acc: 1.0000\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1528e-07 - acc: 1.0000\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.1367e-07 - acc: 1.0000\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.1229e-07 - acc: 1.0000\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.1110e-07 - acc: 1.0000\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.1002e-07 - acc: 1.0000\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0915e-07 - acc: 1.0000\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0832e-07 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0761e-07 - acc: 1.0000\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0700e-07 - acc: 1.0000\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0646e-07 - acc: 1.0000\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0597e-07 - acc: 1.0000\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0552e-07 - acc: 1.0000\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0514e-07 - acc: 1.0000\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0483e-07 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0452e-07 - acc: 1.0000\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0427e-07 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0401e-07 - acc: 1.0000\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0378e-07 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0358e-07 - acc: 1.0000\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0341e-07 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0325e-07 - acc: 1.0000\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0310e-07 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0296e-07 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0284e-07 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0272e-07 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0261e-07 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0252e-07 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0242e-07 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0233e-07 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0225e-07 - acc: 1.0000\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0217e-07 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0212e-07 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0204e-07 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0198e-07 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0193e-07 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0188e-07 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0182e-07 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0179e-07 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0175e-07 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0171e-07 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0166e-07 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0164e-07 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0160e-07 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0157e-07 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0154e-07 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0150e-07 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0150e-07 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0147e-07 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0145e-07 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0141e-07 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0140e-07 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0137e-07 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0136e-07 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0134e-07 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0132e-07 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.0130e-07 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0128e-07 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0126e-07 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0125e-07 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0124e-07 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0121e-07 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0120e-07 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0119e-07 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0117e-07 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0117e-07 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0116e-07 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0113e-07 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0112e-07 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0113e-07 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0112e-07 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0110e-07 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0110e-07 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0106e-07 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0105e-07 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0104e-07 - acc: 1.0000\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0104e-07 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0103e-07 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0103e-07 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0102e-07 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0102e-07 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0101e-07 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0100e-07 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0100e-07 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0099e-07 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0097e-07 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0096e-07 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0097e-07 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0093e-07 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0093e-07 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0087e-07 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33e8a898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0087433304306614e-07, 1.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.2882 - acc: 0.9191\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1466 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1199 - acc: 0.9620\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0943 - acc: 0.9665\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0768 - acc: 0.9749\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0671 - acc: 0.9809\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0617 - acc: 0.9842\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0583 - acc: 0.9859\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0559 - acc: 0.9873\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0540 - acc: 0.9882\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0524 - acc: 0.9889\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0510 - acc: 0.9896\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0499 - acc: 0.9901\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0489 - acc: 0.9906\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0481 - acc: 0.9910\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0473 - acc: 0.9914\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0467 - acc: 0.9917\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0462 - acc: 0.9918\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0456 - acc: 0.9919\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0452 - acc: 0.9921\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0448 - acc: 0.9922\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0445 - acc: 0.9923\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.0442 - acc: 0.9924\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0439 - acc: 0.9924\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0436 - acc: 0.9925\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0434 - acc: 0.9925\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0431 - acc: 0.9926\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0429 - acc: 0.9926\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0427 - acc: 0.9926\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0425 - acc: 0.9926\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0424 - acc: 0.9926\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0422 - acc: 0.9926\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0420 - acc: 0.9926\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0419 - acc: 0.9927\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0417 - acc: 0.9926\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0416 - acc: 0.9927\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0414 - acc: 0.9927\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0413 - acc: 0.9927\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0412 - acc: 0.9927\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0411 - acc: 0.9927\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0410 - acc: 0.9927\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0409 - acc: 0.9927\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0407 - acc: 0.9927\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0405 - acc: 0.9927\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0404 - acc: 0.9927\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0403 - acc: 0.9927\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0402 - acc: 0.9927\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0401 - acc: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb34edfac8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=0.1)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010234667018055916, 0.9998576957702636]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 0.2886 - acc: 0.9219\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1595 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1543 - acc: 0.9615\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1480 - acc: 0.9615\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1427 - acc: 0.9615\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1390 - acc: 0.9616\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1362 - acc: 0.9616\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1342 - acc: 0.9617\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1325 - acc: 0.9618\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1311 - acc: 0.9619\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1301 - acc: 0.9621\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1293 - acc: 0.9622\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1287 - acc: 0.9623\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1282 - acc: 0.9623\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1278 - acc: 0.9624\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1273 - acc: 0.9624\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1270 - acc: 0.9625\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1266 - acc: 0.9626\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1263 - acc: 0.9627\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1260 - acc: 0.9627\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1257 - acc: 0.9629\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1255 - acc: 0.9628\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1253 - acc: 0.9627\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1251 - acc: 0.9629\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1249 - acc: 0.9629\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1247 - acc: 0.9629\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1246 - acc: 0.9630\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1244 - acc: 0.9629\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1242 - acc: 0.9631\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1240 - acc: 0.9632\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1240 - acc: 0.9631\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1238 - acc: 0.9631\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1236 - acc: 0.9633\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1235 - acc: 0.9632\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1234 - acc: 0.9631\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1233 - acc: 0.9632\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1231 - acc: 0.9632\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9634\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9634\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1228 - acc: 0.9634\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1227 - acc: 0.9635\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1227 - acc: 0.9635\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1226 - acc: 0.9633\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1225 - acc: 0.9634\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1225 - acc: 0.9635\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1224 - acc: 0.9635\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1223 - acc: 0.9634\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1223 - acc: 0.9635\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1222 - acc: 0.9634\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1222 - acc: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb350bccc0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000,prob=0.5)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05628207671642303, 0.9727115383148194]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 0.2870 - acc: 0.9283\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1636 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1634 - acc: 0.9615\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1633 - acc: 0.9615\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1632 - acc: 0.9615\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1631 - acc: 0.9615\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1631 - acc: 0.9615\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1630 - acc: 0.9615\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1629 - acc: 0.9615\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1628 - acc: 0.9615\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1628 - acc: 0.9615\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1626 - acc: 0.9615\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1625 - acc: 0.9615\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1625 - acc: 0.9615\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1622 - acc: 0.9615\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1622 - acc: 0.9615\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1614 - acc: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb356bd9e8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000,prob=1)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16452448892593383, 0.9615384340286255]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)\n",
    "# 他这个accuracy不太对 是计算整体的好像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04283839, 0.03006983, 0.03108829, 0.03086495, 0.03821969,\n",
       "       0.04318464, 0.04275879, 0.02881029, 0.02591237, 0.05666709,\n",
       "       0.04924551, 0.04192016, 0.04666957, 0.04626572, 0.04118401,\n",
       "       0.03622824, 0.02944967, 0.03765562, 0.03575623, 0.03924397,\n",
       "       0.03410533, 0.03530636, 0.04261568, 0.03204012, 0.04352927,\n",
       "       0.03008407, 0.05662912, 0.04116714, 0.04893467, 0.05282199,\n",
       "       0.03043225, 0.04488924, 0.05681634, 0.04138765, 0.05441827,\n",
       "       0.02774709, 0.03407055, 0.04218432, 0.03569707, 0.03512883,\n",
       "       0.04342568, 0.05268621, 0.04676977, 0.02773789, 0.02489308,\n",
       "       0.0423353 , 0.04129213, 0.03324315, 0.03560084, 0.02898619,\n",
       "       0.03714329, 0.036331  ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_prob_curve(prob):\n",
    "    x_train, y_train, y_train_small = misslabeled_data_genelization(loops=1000, prob=prob)\n",
    "    # assume that we have got our data x_train y_train\n",
    "    # now we are going to train it in our model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    # change loss from possion to binary it works well :P\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train,epochs = 200)\n",
    "    \n",
    "    \n",
    "    x_train, y_train, y_train_small = data_genelization()\n",
    "    model.evaluate(x_train, y_train)\n",
    "\n",
    "    prediction_list, label_list = predict_results_only_2(model, x_train, y_train)\n",
    "    diff = list(map(find_diff, prediction_list, label_list))\n",
    "    count = diff.count(True)/len(diff)\n",
    "    return(count)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It takes hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%time\n",
    "plotinfo = list(map(plot_acc_prob_curve,np.arange(0,1,.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/noise_size_1000.pickle', 'wb') as f:\n",
    "    pickle.dump(plotinfo, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.97,\n",
       " 0.961,\n",
       " 0.954,\n",
       " 0.96,\n",
       " 0.934,\n",
       " 0.952,\n",
       " 0.939,\n",
       " 0.922,\n",
       " 0.912,\n",
       " 0.931,\n",
       " 0.931,\n",
       " 0.911,\n",
       " 0.909,\n",
       " 0.914,\n",
       " 0.908,\n",
       " 0.903,\n",
       " 0.865,\n",
       " 0.896,\n",
       " 0.913,\n",
       " 0.876,\n",
       " 0.935,\n",
       " 0.892,\n",
       " 0.904,\n",
       " 0.889,\n",
       " 0.898,\n",
       " 0.905,\n",
       " 0.905,\n",
       " 0.883,\n",
       " 0.869,\n",
       " 0.887,\n",
       " 0.89,\n",
       " 0.895,\n",
       " 0.871,\n",
       " 0.864,\n",
       " 0.843,\n",
       " 0.82,\n",
       " 0.831,\n",
       " 0.886,\n",
       " 0.845,\n",
       " 0.833,\n",
       " 0.834,\n",
       " 0.876,\n",
       " 0.85,\n",
       " 0.819,\n",
       " 0.812,\n",
       " 0.796,\n",
       " 0.809,\n",
       " 0.791,\n",
       " 0.746,\n",
       " 0.803,\n",
       " 0.777,\n",
       " 0.754,\n",
       " 0.712,\n",
       " 0.727,\n",
       " 0.769,\n",
       " 0.701,\n",
       " 0.702,\n",
       " 0.676,\n",
       " 0.659,\n",
       " 0.722,\n",
       " 0.657,\n",
       " 0.676,\n",
       " 0.627,\n",
       " 0.657,\n",
       " 0.565,\n",
       " 0.567,\n",
       " 0.634,\n",
       " 0.543,\n",
       " 0.542,\n",
       " 0.478,\n",
       " 0.508,\n",
       " 0.483,\n",
       " 0.459,\n",
       " 0.45,\n",
       " 0.463,\n",
       " 0.32,\n",
       " 0.38,\n",
       " 0.38,\n",
       " 0.343,\n",
       " 0.265,\n",
       " 0.287,\n",
       " 0.307,\n",
       " 0.312,\n",
       " 0.211,\n",
       " 0.18,\n",
       " 0.171,\n",
       " 0.138,\n",
       " 0.128,\n",
       " 0.119,\n",
       " 0.08,\n",
       " 0.095,\n",
       " 0.055,\n",
       " 0.027,\n",
       " 0.041,\n",
       " 0.019,\n",
       " 0.016,\n",
       " 0.009,\n",
       " 0.014,\n",
       " 0.005,\n",
       " 0.0]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb78505390>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8VFX+//HXmUkjvRfSA6GEDqFJVVCwgQUVXHWtWFd/637X1e/uui7bXb+urrIq7lpXRaygYkeKUkMn1BBCCumBhPQy5/fHDDGNZALJTGbm83w8eJi592Tmcwm8PZx77jlKa40QQgjnYrB3AUIIIXqehLsQQjghCXchhHBCEu5CCOGEJNyFEMIJSbgLIYQTknAXQggnJOEuhBBOSMJdCCGckFtXDZRSrwBXAEVa6+EdnFfAs8BlQDVwq9Z6R1fvGxoaqhMSErpdsBBCuLLt27eXaK3DumrXZbgDrwHPA2+c5fylQLLl10TgBct/O5WQkEBaWpoVHy+EEOIMpdRxa9p1OSyjtV4PlHXSZD7whjbbDAQqpaKsK1MIIURv6Ikx92ggp8XrXMsxIYQQdtIT4a46ONbhUpNKqcVKqTSlVFpxcXEPfLQQQoiO9ES45wKxLV7HACc6aqi1Xqa1TtVap4aFdXk/QAghxDnqiXBfBdyizCYB5Vrr/B54XyGEEOfImqmQ7wAzgVClVC7wO8AdQGv9IrAa8zTIDMxTIW/rrWKFEEJYp8tw11ov6uK8Bu7vsYqEEEKcN4d7QjWj6DRPf3WImvome5cihBB9lsOF+7cHivjnmgxmP72OL/YVIHvACiFEew4X7nfPGMDyxZPw9XTjnv9u55ZXtrL9eOtnrHLKqnl3Wza1DdK7F0K4JmuWH+hzJiWF8OmDU3lj03GeW3OEa1/YxPiEIK4Y2Z9vDhSy4UgJAHkna3j4ksF2rlYIIWzP4XruZ7gbDdwxNZGNj17E765M4cSpWn63Kp3M4ip+PnsQl6RE8NL6THLKqpu/p+h0LT99ZSsf7cy1Y+VCCNH7lL3GrFNTU3VPLhzW0GTiaHElyeF+GA2K/PIaLnpqHTMHh/HCTeOobWhi4bLN7Mo5BcBDs5L5f7OTMS9qKYQQjkEptV1rndpVO4ftubflbjQwJNIfo8Ec1lEB/bh35gA+31fAxowSHnl/D7tyTvHcojFcOzaGZ789wsMrdlPXKOPyQgjn45Bj7tZaPD2Jd7flsPjN7VTWNfLLOYO5clR/rhgZRXyIN09/fZjq+kb+9ZNxzf9T6Kt255wiPsSbQG8Pe5cihHAATtNz74iXu5FfXz6UyrpGrhkTzX0zBwCglOLBWck8fkUKX6YX8vtP0s86pfLbA4Us+WR/8681BwtteQkAVNU1ct2Lm/jz6gM2/2whhGNy6p47wGUjovj0Z1MZHOnXbnz99qmJ5JfX8PKGY83DOC1tzCjhrjfS8HAz4G4w0GAy8fqmLN5dPInUhGCbXcO2rDLqm0x8vreAJfOH4+VutNlnCyEck1P33M8YHh2Au7HjS33s0qFcOao/f/viIP/ekInJZO7B552q4YF3djIgzJe031zM3t/PYeuvZxMT1I8H3t5JWVW9zerfdLQUgNN1jaw9VGSzzxVCOC6XCPfOGAyKp64byeyh4fzxswMsfHkzhwpOc8+b22loNPHSzePw9TT/A8ffy52lN46lrKqeh1fsav4fQUta6x5/eGpTZinj4oMI9fVk5a4OV1MWQohWnH5YxhqebkZeviWV99Jy+cNn+5nzzHoA/vPTVJLCfFu1HR4dwG+vTOG3H+/jNyv3ceHgcOKCvamsa2T13nw+35tPcWUdlwyL5MYJcUxOCsFg5c3arJIqHvlgD49eOoSxcUEAlNc0sC+vnJ9dlMyI6Abe3prN6doG/Lzce/Y3QQjhVCTcLZRSXD8+lumDwvjL5wcYFRPIrKERHba9aWIce3JO8faWbN7ekt183MNoYPqgUPoH9mPV7hN8tief2OB+XD6iP5ePiGJ4tP9Z59WXVNbx01e3cry0mufXZPDKreMB2HqsDJOGyQNC8HAz8NrGLL5ML2TBuBgATCaNUnQ6X7+sqp4D+RXNr+OCvYkN9u7275EQwnFIuLcRGeDFswvHdNpGKcWTC0by2GVDySmrJtvyFOyMwWH4W3rU/3vZUL5ML+D97bm8vCGTF9cdZXCEH8sXTyLIp/V0xur6Ru54bRuFFbVckhLB1wcKyT1ZTUyQNxuPluDpZmBMXCAeRgOxwf1YuSuPBeNiOJBfwX1v7SAqwIuXb0nFx7P1jzOnrJp/b8jk3bQcahtMzcc93Ax88oD5JrMQwjlJuJ8jpRTBPh4E+3gwKjaw3XkvdyPzR0czf3Q0J6vq+WTPCR5fmc5723NYPP3HWTlNJs39b+1gb145L92cSkp/f745UMg7W7P55ZwhbDpaSmpCEJ5u5hky80dF86+1GbyxKYu/rD6It4eR7LJqbnt1G6/eNh4fTzdKK+t48otDvL8jF4OCq0ZHc9WYaNyNBhqaTDy0fCcPr9jFR/dNwcPN+tsuJpPmWGkVSaE+8mSvEH2cy99QtYUgHw9umZxAanwQy7fmtJpTv3JXHt8dKuaJecO4OCWC6MB+XDQknHe35VJYUcvBgtNcMCC0uf380f0xaXh8ZTrD+vvz+UPTeHbhaLZnn+TWV7fyyvfHuPCptXywI5dbJsez/pEL+ft1o5gyMJQJicFMGRjKn68eQfqJCp5bc6TL2msbmticWcoTq9KZ/NdvmfV/63hxXWav/D4JIXqO9NxtaNGEOH7x3m62HCtjUlIITSbN899lMCTSj5snxTe3+8nEeL45sI0ln+4HzKtgnpEc4cc1Y6MJ8vbgV3OH4OFm4IqR/VEoHly+k21ZJ5k6MJQn5g1jYLhvuxoALhkWyYJxMSz9LoOLhoQzxnLz9ozi03U8/fUhdmaf4khRJU0mjYebgZmDwjhV08Az3xzmshGRxIf4nPPvRXZpNXWNTSRHyNCQEL1Bwt2GLhsRxROfpPPO1mwmJYXw+b58MourWHrj2FbDHNMHhRET1I/P9uTj42FkZExAq/d5+vrR7d778pFRBPm4U13XxKyh4V0Omzx+ZQqbjpby8IrdrLh7MmF+noC5p37nG2kczK9gUlIIs4dGMCImgCkDQ/H1dKOgvJbZT6/jNx/v443bJ5zz8MzDK3aRd6qG7391UZ9f+kEIRyTDMjbUz8PINWOi+XxvAaWVdTy/JoOB4b5cOjyyVTujQbFoQhwA4xODz/oAVlsXDAhldkqEVYHr7+XOMwtHU1Bey/UvbSL3ZDVaa/7nvd3syT3FPxeN4fXbJ/A/cwYzZ1hk81z/yAAvfjlnMBuOlJzznPvymgZ2ZJ8kv7yWzZmlVn1PbUMTaVllXTcUQgAS7ja3aGIc9U0mHlq+i4MFp7n/wgEdzoO/PjUWX0+3s07H7AnjE4L5750TKK2sY8ELm/jNx/v4dE8+j8wZwpxhkWf9vpsmxTM6NpA/fLqfotO13f7cjRklmDQoBR/ssG5t/WXrM1nw4iY2HCnu9ucJ4Yok3G1sSKQ/o2MD+T6jhPgQb64c2b/DdmF+nmx67CJ+YunB95Zx8cG8e/dkmrTmrS3ZXDs2hntmJHX6PUaD4i/XjOB0bSMznlzL7z9JJ+9UjdWfuf5ICb6eblw7NoYv9hVQVdfYaXutNR/tzANgySf7aWgyddpeCCHhbhc3WgL7/pkDcetkyMXPy93qp1vPx9Aofz645wIeu3QIf75muFXDOkOj/PnswalcOiKSNzcdZ/qT37F8a3aX36e1Zv3hYi4YEMIN42Oprm/ii30FnX7P7txyjpVUMWdYBEeKKnlr83Grr00IVyXhbgfXjovhlVtTm58y7QviQry5e8aA5vn01kiO8OPp60ez/pELmZQUzOOr0jlYUNHp92SVVpN3qoZpg8JIjQ8iNrgfH3ax7eHHO/PwcDPw9+tGMXVgKE9/fdimC7cJ4Ygk3O3AaFBcNCTCJr1yW+gf2I9nF47B38udB9/Z2enCaWfGzKcnh6KU4poxMWw8WsqJswzrNDSZ+GT3CS4eGoG/lzuPX5lCVX0TT399qFeuRQhnIeEuekSoryf/d/0oDhdWdrqpyPrDxcQFezfPkb9mbDRaw8e78jps//2REkqr6rlqTDQAgyLMzwS8vSWbI4Wne/5ChHASEu6ix8wYFMYdUxN5Y9Nxvtnffseq+kYTm46WMi35xydu40N8GJ8QxGs/ZLH/RPshnY935RHo7c6MQWHNxx6clYyXu5Gl32X0zoUI4QQk3EWPemTuYIZE+vHYR3spr25odW5n9kmq6puYlhzW6vjvrhyGQSmueeEHVrbowVfWNfJlegFXjIxqtQZOsI8HN02KZ9XuExwrqerdCxLCQUm4ix7l6WbkqetGUVZVzx8+29/q3IYjJRgNigsGhrQ6Pjw6gE9+NpWR0YE8tHwXd76ext1vprFo2WZqG0xcbRmSaenOaYm4Gw28sFZ670J0RMJd9Ljh0QHcMyOJ97fnsu6w+QbqmoOF/HfLccbFBTUvi9xSmJ8nb901kTumJrL/hHnqo5e7gZsmxTVvXNJSuJ8XiybE8eGOPHIsSy4LIX6kWq5QaEupqak6LS3NLp8tel9tQxNXPPc91XWNXDIsktc2ZjE0yp+lN45pt7vVucovr2H6k99xfWosf7p6RI+8pxB9nVJqu9Y6tat20nMXvcLL3ciTC0aSX1HLaxuzuPWCBD6674IeC3aAqIB+LBgXy3tpuazclUejPLkqRDOreu5KqbnAs4AR+LfW+q9tzscBrwOBljaPaq1Xd/ae0nN3DZ/tycfXy63VbJeelF9ewy3/2cqRokpig/tx17QkbpwQ1+mTv0I4Mmt77l2Gu1LKCBwGLgZygW3AIq31/hZtlgE7tdYvKKVSgNVa64TO3lfCXfQUk0nzzYFCXlx3lB3Zp5g/uj//uH5080Ni5TUNPP3VIeaN7s+4+GA7VyvE+enJYZkJQIbWOlNrXQ8sB+a3aaMBf8vXAcC5rQUrxDkwGBSXDIvkg3sv4JG5g1m56wS/XbkPrTU5ZdUseGEjr286zu2vpXU6dTItq4z0E+U2rFyI3mPNZh3RQE6L17nAxDZtngC+Ukr9DPABZvdIdUJ0g1KK+2YO5HRtIy+sPUpNQxPrD5dQ39jEU9eN4k+f7eeO17fx0b1TCPBuPWNHa819b+1AKVjzi5ntNhsXwtFY03PvaAGUtmM5i4DXtNYxwGXAm0qpdu+tlFqslEpTSqUVF8u63KJ3PDJnMDdPiufDHXn08zDw4X1TWDAuhpduTiWnrJr7397RbtngjKJKik7XUVhRxwtrj9qpciF6jjXhngvEtngdQ/thlzuAFQBa602AFxDapg1a62Va61StdWpYWO/cYBNCKcXv5w3juUVj+Pi+Kc17yU5IDObPV4/g+4wSlq1vvcn39xklAExOCmHZhkyyS2XuvHBs1oT7NiBZKZWolPIAFgKr2rTJBmYBKKWGYg536ZoLuzEYFFeO6k+Ir2er49elxjIpKZgPd+TScjLBD5bNU/5xw2iMSnW6+JkQjqDLcNdaNwIPAF8CB4AVWut0pdQSpdQ8S7NfAHcppXYD7wC3ans9HSVEF64Y2Z+jxVUcsqwq2dhkYnNmGVMGhhIZ4MX9Fw7gi/QCNlp680I4IqsmA2utV2utB2mtB2it/2Q59rjWepXl6/1a6yla61Fa69Fa6696s2ghzsfc4ZEYFKzekw+Yd3qqrGtk6kDzSOKd05KIDe7H46vSqWs8+9r0QvRl8qSHcDmhvp5MHhDCp3vz0VrzQ0YJSpnH28H8dO0f5g8no6iS5761bmGywopa3t6SjfyDVfQVEu7CJV02IorM4ioOFpzm+4wShvX3J8jHo/n8zMHhXDs2hhfWHWVfXtdz35/99gj/+9FesuRGrOgjJNyFS5o7zDw0815aLjuzTzJlYLvJXfz2iqEE+3jwy/f3tJs62VJdYxOfWYZ4dhw/2Ws1C9EdEu7CJYX4enLBgFDe3JxFQ5NmyoD24R7o7cEfrxrOgfwKXuxk7vvaQ8WU15g3JtmZI+Eu+gYJd+GyLh8ZRUOTxsNoYHxCx2vOzBkWyZxhESxbn3nWjb8/3plHqK8Hk5KC2XH8VG+WLITVJNyFy5ozLBKjQTEuPoh+Hsaztrt5UgKn6xpZc7Co3bnymga+PVDElaP6MyEhmIMFFVTVNfZm2UJYRcJduKxgHw+WzB/Gzy8e1Gm7yQNCCPfz5KOdee3Ofb43n/om81aAY+KDMGnYnSu9d2F/Eu7Cpf1kYjwTEjtfBthoUMwf3Z+1h4o4WVXf6txHO/NICvNhRHQAY2IDAdiZLeEu7E/CXQgrXDUmmoYmzWd785uP5Z2qYcuxMq4eHY1SikBvD5LCfNiZLTdVhf1JuAthhZQofwZF+PKxZWhGa83za44AMH90dHO7sXFB7Mg+JQ8zCbuTcBfCCkoprhoTTdrxk2SXVvP7T/bzztYc7pqWSFyId3O7sXFBlFXVc7wbDzPV1Dcx95n1fHeo/Q1bIc6VhLsQVjrTQ7/5lS28tjGLO6cm8r+XDW3VZmy8edx9RzeGZnZkn+RgwenmtW6E6AkS7kJYKTqwHxMTgzleWs1d0xL59eVDUar1XjbJ4X74erp166bqlmNlAKTJ062iB8leYkJ0w5L5w9mTe4oF42LaBTuYZ9aMig3oVs99myXcj5VUUVJZR2ibNeiFOBfScxeiGwZH+nFdamyHwX7G2LggDuRXcO0LG5n99DpmP72OrLNszF3faGJnzklGxQQAkJYlvXfRMyTchehhl4+MYnRsIJ5uBpLDfck/VcMfP9vfYdu9eeXUNpi4fWoiHm4Gth8vs3G1wlnJsIwQPWxIpD8f3jel+fWL647y188Psu5wMTMGtd47eFuWOcwvGBDKqJgAtknPXfQQ6bkL0ctum5JAQog3Sz5Jb7d08LZjZSSF+RDm58m4+GDST5SfdYEyIbpDwl2IXubpZuQ3l6dwtLiKNzYdbz5uMmm2ZZUxwbIi5fiEIBqaNLtzZPkCcf4k3IWwgVlDw5k+KIxnvjlMUUUtAIcKT1NR29i8ts24+CDgxymRWmve3HycPbIQmTgHEu5C2IBSisevSKGxSXPH62lU1TU2j7efWUs+0NuDgeG+pFmOv701m99+vI+nvjpst7qF45JwF8JGBob78vyNY0g/Uc59b+1gY0YpUQFexAT1a24zPiGI7cdPkpZVxhOr0vF0M7DpaAmnaxvsWLlwRBLuQtjQrKER/OnqEaw7XMwX6QVMSAxuNWd+XHwwFbWN3PbqNvoH9mPpjWNpaNKsO1xsx6qFI5JwF8LGFk2I48FZyQBMTAxpdW58gnncvdGkeenmcVw4JJxgHw++3l/Yqt2bm4/z4Y5c2xQsHJLMcxfCDn4+O5nxCUHt9m6NC/bmhtRYZqdEMCTSH4CLhoTzVXoBDU0m3I0Gcsqq+f2qdJIj/LhmbIw9yhcOQHruQtiBUoppyWF4uRvbHf/bgpFcnBLRfOzilAgqahvZalmD5l9rM2g0aY4WV9JkknXjRcck3IXo46Ylh+LpZuDr/YXklFXzXloukf5e1DeayCmzft144Vok3IXo47w93Jg6MJSv9xfy/JoMDAbFE/OGAXCkqNLO1Ym+SsJdCAdwcUoEeadqWLE9hxsnxDFloPlG7JGi02f9nld/OMbl/9wgW/65KAl3IRzArKERKAUeRgP3zhyAn5c7UQFeZBSevef+ZXoB6ScqKKmst2Gloq+Q2TJCOIAwP09uSI0lLsSbCH8vwPxQ1NmGZRqaTOzOKQfgSOFpwvxkAxBXIz13IRzEX68dyX0zBza/Tg73I6OoElMHM2YO5p+mxrK6pIzLuyarwl0pNVcpdUgplaGUevQsba5XSu1XSqUrpd7u2TKFEG0lR/hS09BE3qmadufObPPnblSdjssL59XlsIxSyggsBS4GcoFtSqlVWuv9LdokA48BU7TWJ5VS4b1VsBDCLDncF4CMokpig71bndt+/CSR/l5EB/XjcCfj8sJ5WdNznwBkaK0ztdb1wHJgfps2dwFLtdYnAbTWRT1bphCirYGWcO+oZ74j+yRj4wMZFOFLhgzLuCRrwj0ayGnxOtdyrKVBwCCl1A9Kqc1Kqbk9VaAQomOB3h6E+XlypE3PvLCiltyTNYyNC2JguB9lVfWUVNbZqUphL9bMlulom/e2d3DcgGRgJhADbFBKDddat9plQCm1GFgMEBcX1+1ihRCtJXcwY2aHZbOPcfFBVNY1AnCksJJQX5kx40qs6bnnArEtXscAJzpos1Jr3aC1PgYcwhz2rWitl2mtU7XWqWFhYW1PCyG6KTncPOzS8kGlHdkn8XAzMKx/AMnhfkDnDzsJ52RNuG8DkpVSiUopD2AhsKpNm4+BCwGUUqGYh2kye7JQIUR7AyP8qKxrpMCydR+Yb6aOjA7Aw81AhL8nfp5u7YZuhPPrMty11o3AA8CXwAFghdY6XSm1RCk1z9LsS6BUKbUf+A74pda6tLeKFkKYnZkxcya86xqb2JdXwVjLfqxKKZIjfDlcKD13V2PVE6pa69XA6jbHHm/xtQYetvwSQthIc7gXVTJ9UBj78iqobzIxNi6oRRs/vjlQeLa3EE5KnlAVwoGF+HoS7ONBel45hwpOs2pXHgBj4wOb2yRH+FJaVU+pzJhxKbK2jBAObmC4Lx/uzOPDneZgHxzhR7ifV/P55IgzN1UrCZEZMy5Dwl0IB/fYpUPYlFlKbJA3ccHeJEf4tjo/KOLHoZtJSSEdvYVwQhLuQji4MXFBjGkxxt5WpL8Xvp5uHJGbqi5FxtyFcHJKKfPywDId0qVIz10IFzAowpcv0wt5c1MW/v3ciQ/xYXRsYJffJxyXhLsQLmDKwFDe257Lb1emNx/b8MiF7VaTFM5Dwl0IFzB/dDRzh0dSXtNA+okKbnt1G1uPlUm4OzEZcxfCRXi6GQn382JGchj+Xm6kHS+zd0miF0m4C+FiDAZFakIw27JO2rsU0Ysk3IVwQakJQWQUVVJWVW/vUkQvkXAXwgWNTwgGIC1LhmaclYS7EC5oRHQAHkYDacd/HJrRWrMr5xQmU9u9eIQjknAXwgV5uRsZFRvA1mM/9tzf2ZrDVUt/4JUfjtmxMtFTJNyFcFGpCcHsyyunpr6J+kYTS7/LAOCZb45Q2GLzD+GYJNyFcFHjE4JoNJmHYt7bnkPeqRqWzB9GfZOJP68+YO/yxHmScBfCRY2LC0Yp2Hi0hKVrMhgTF8jNk+K5Z8YAVu46wcajJfYuUZwHCXchXFSAtzuDI/x4eUMmJ8pr+fnsQSiluG/mAGKD+/H4ynQamkz2LlOcIwl3IVxYakIQtQ0mxsUHMS05FDDfbH3iymFkFFXy7w1yc9VRSbgL4cKmDDAH+sMXm3vtZ8waGsGcYRE8++1hskur7VWeOA8S7kK4sLnDI/n2FzOYMjC03bnfzxuOm8HArz/ei9Yy993RSLgL4cKUUgwI8+3wXGSAF7+cM5gNR0pYtftEh23KquplXL6PknAXQpzVTZPiGR0byJJP9lNSWdfq3Bf78pn8l295fk2GnaoTnZFwF0KcldGg+PPVIzhd28gl/1jP8q3ZmEyal9dncu9bO6hrNHGspMreZYoOyGYdQohOpfT3Z+UDU/jdynQe/XAvz63JIO9UDZePiCL3VA0F8jRrnyQ9dyFEl4ZG+fPu3ZN45obRuBnNc+GfWzSG+GBvWaqgj5KeuxDCKkoprhoTzVVjopuPRQZ48UV6LVrrVlMphf1Jz10Icc4i/L2obzRxqrrB3qWINiTchRDnLNLfC0DG3fsgCXchxDmLDPAEJNz7Igl3IcQ5i7D03AvLJdz7Ggl3IcQ5C/eTYZm+yqpwV0rNVUodUkplKKUe7aTdAqWUVkql9lyJQoi+ysPNQKivh0yH7IO6DHellBFYClwKpACLlFIpHbTzAx4EtvR0kUKIvivC34sCGZbpc6zpuU8AMrTWmVrremA5ML+Ddn8AngTkpyyEC4n096Kgoq7rhsKmrAn3aCCnxetcy7FmSqkxQKzW+tPO3kgptVgplaaUSisuLu52sUKIviciwEuGZfoga8K9o8fOmhd3VkoZgH8Av+jqjbTWy7TWqVrr1LCwMOurFEL0WZH+XpRV1VPX2GTvUkQL1oR7LhDb4nUM0HJxZz9gOLBWKZUFTAJWyU1VIVzDmQeZimRopk+xZm2ZbUCyUioRyAMWAjeeOam1Lgeat3FRSq0F/kdrndazpQoh+qKIgB+nQ8YGewOQX17DkcLK5jaDI/2a58QL2+gy3LXWjUqpB4AvASPwitY6XSm1BEjTWq/q7SKFEH1XhL/lKdUWM2Zue3UbBwtON79ODvflq59Pl8XFbMiqVSG11quB1W2OPX6WtjPPvywhhKM4Myxz5qZq8ek6Dhac5rYpCVwxMopNR0t56qvDbDlWxqSkEHuW6lLkCVUhxHkJ6OeOp5uhuee+ObMUgPmjoxkXH8wdU5Pw93LjrS3Z9izT5Ui4CyHOi1KKyACv5iUINmeW4uvpxvD+/gD08zBy7bgYvtiXT/FpuelqKxLuQojzFuH/41z3zZmljE8Iws34Y7z8ZGI8DU2a97bnnO0tRA+TcBdCnDfzU6q1FFXUcrS4iskDWo+tDwz3ZVJSMG9vMW+wLXqfhLsQ4rxFBnhRWFHHJst4e0c3Tn8yMZ7ckzWsOyJPp9uChLsQ4ryd2W7vi30F+Hm6kRLl367NnGGRhPp68NZmubFqCxLuQojzdmY65LcHi5iQGNxqvP0MDzcDc4dHsuloiQzN2ICEuxDivJ3Zbq++0dTpXPYR0QFU1TdxvKzaVqW5LAl3IcR5a7m0QNubqS0N6x8AwL688l6vydVJuAshztuZ7fb8vNwY2sF4+xmDIvxwNyrST1TYqjSXZdXyA0II0RkPNwMR/p6MiA7EaDj7+jEebgYGRfiRfkJ67r1Nwl0I0SOW3ji2uQffmWH9/fnmQBFa6y4XEssoOk1WSTWzUyJ6qkyXIeEuhOgRqQnBVrUbHh3AirRc8str6R/Yr9358poGXvn+GKv35nOkyLxm4EdWAAANdklEQVRs8Nc/n05yhF+P1uvsZMxdCGFTwyxrznQ07t5k0tz/1g7+ueYIQT4e3DUtEYCjxZXt2orOSbgLIWxqaJQ/SnU8Y+b5NRl8n1HCn68ewYq7J/PQ7EEAHC2usnWZDk/CXQhhU94ebiSF+rTruW/MKOGZbw9z9ZhoFo437+zp6+lGuJ8nx0ok3LtLwl0IYXPDowNazZgpOl3Lg8t3kRTqwx+vGt7qRmtSmI+E+zmQcBdC2Nzw/gHkl9dSWlmH1ppfvreHyroG/vWTcfh4tp7nkRjqS6aMuXebzJYRQthcy5uqOSerWXe4mCXzhzE4sv2MmKRQH05WN3Cyqp4gHw9bl+qwJNyFEDZ3ZhmC1XvzWbX7BNOSQ7lpYnyHbZPCfAA4Vlol4d4NMiwjhLC5AG93YoL6sXxbDkaD4skFIzGc5cnWxFBLuMuMmW6RcBdC2MVwS+99yfxhRAW0f5jpjNhgb4wGRWaJjLt3hwzLCCHs4vapiYyKDeSq0dGdtnM3GogL9pYZM90k4S6EsIsJicFMSLRuyYKkUB8yZVimW2RYRgjR5yWG+pBVWiU7OHWDhLsQos9LDPOhtsFEfkWtvUtxGBLuQog+LynUF5AZM90h4S6E6POa57rLjBmrSbgLIfq8cD9PvD2MZMqMGatJuAsh+jylFIkyY6ZbJNyFEA4hKcxX5rp3g1XhrpSaq5Q6pJTKUEo92sH5h5VS+5VSe5RS3yqlOl4kQgghzlFiqA+5J6upa2yydykOoctwV0oZgaXApUAKsEgpldKm2U4gVWs9EngfeLKnCxVCuLakUB9MGhmasZI1PfcJQIbWOlNrXQ8sB+a3bKC1/k5rXW15uRmI6dkyhRCublJSCO5Gxdtbsu1dikOwJtyjgZwWr3Mtx87mDuDz8ylKCCHaigzwYsG4GN5Ny6FQHmbqkjXh3tE6nB0+A6yUuglIBf5+lvOLlVJpSqm04uJi66sUQgjg3hkDaTJplq3PtHcpfZ414Z4LxLZ4HQOcaNtIKTUb+DUwT2td19Ebaa2Xaa1TtdapYWFh51KvEMKFxYV4c9XoaN7acpySyg5jRlhYE+7bgGSlVKJSygNYCKxq2UApNQZ4CXOwF/V8mUIIYXbfhQOoazTxn++P2buUPq3LcNdaNwIPAF8CB4AVWut0pdQSpdQ8S7O/A77Ae0qpXUqpVWd5OyGEOC8Dwny5YmR/3tiYRe7J6q6/wUUpre2zhGZqaqpOS0uzy2cLIRzbkcLTzHv+BwB+Nmsgd0xNxNPNaOeqbEMptV1rndpVO3lCVQjhcJIj/Pjq59OZPiiUJ784xNxnNnC8VOa/tyThLoRwSLHB3rx0cyqv3z6BwopanvnmiL1L6lMk3IUQDm3GoDAWjo/jk90nOHGqxt7l9BkS7kIIh3f71AQ08IrMoGkm4S6EcHgxQd5cMTKKd7ZmU17TYO9y+gQJdyGEU1g8PYmq+iZZe8ZCwl0I4RSG9Q9g6sBQXv3hmCwLjIS7EMKJLJ6eRNHpOn63Mp3q+kZ7l2NXEu5CCKcxLTmUO6cmsnxbDnOf2cCmo6X2LsluJNyFEE5DKcVvrkhhxd2TMShY9PJmHv1gD2VV9fYuzeYk3IUQTmdCYjCfPzSdxdOTeH97Lhc+tZY3Nx+nyWSf5VbsQcJdCOGU+nkY+d/LhvL5Q9NIifLntx/v429fHLR3WTYj4S6EcGrJEX68fddELh8RxfKt2dQ2uMZMGgl3IYTTU0px48Q4Kmob+TK9wN7l2ISEuxDCJUxOCiE2uB/vbsvpurETkHAXQrgEg0Fx/bhYNh4tJbv0x00+ahuanHJOvIS7EMJlLEiNwaDgve3m3ntWSRUXPrWWu95wvo2DJNyFEC4jKqAf0weF8f72XI4WV7Jw2Wbyy2v5IaOUYyXOtdmHhLsQwqXckBpLfnkt8577nvomE6/fPgGDgve3O9dYvIS7EMKlzBoaQaivB57uRt6+ayIzBoUxY1AYH2zPc6qHnCTchRAuxcPNwIq7J7P6wWkMifQH4LrUWAoqavkho8TO1fUcCXchhMtJCvMlMsCr+fWsoeEEervz3vZcO1bVsyTchRAuz9PNyPxR/fkyvcBpdnKScBdCCGDBuFjqG018svuEvUvpERLuQggBDI/2Z0ikH29tycbkBDdWJdyFEALz+jOLpydxIL+Cz/bm27uc8ybhLoQQFvNHRzMk0o+nvjpEfaOp+Xhjk4lT1Y614YeEuxBCWBgNil9dOoTjpdW8uy0bgPLqBm5Ytplpf/uOnLLqLt6h75BwF0KIFmYOCmNiYjDPfnuErJIqbli2iT25pzBpzS9W7HaYB50k3IUQogWlFI9eOoSSynoueWY92WXVvHLreJ6YN4ytWWW88v0xe5doFQl3IYRoY0xcEPNG9aefu5H/3jmRaclhLBgXw8UpEfz9q0McLjxt7xK7pLS2zz8xUlNTdVqa8y2zKYRwDo1NJuqbTHh7uDUfK6msY84/1hPQz51rxkYzPDqAkTGBBPt42KwupdR2rXVqV+2s6rkrpeYqpQ4ppTKUUo92cN5TKfWu5fwWpVRC90sWQoi+w81oaBXsAKG+njyzcDQaeOqrw9z66jbG/+kbHlq+k/0nKgAoq6rnna3ZPPrBHrYfL7ND5WZd9tyVUkbgMHAxkAtsAxZprfe3aHMfMFJrfY9SaiFwtdb6hs7eV3ruQghHVlHbQHpeBd8eKOSdrdlU1TcxOMKPjOJKmkwaDzcD9Y0mrh0bw6OXDiHMz7NHPtfanrs14T4ZeEJrPcfy+jEArfVfWrT50tJmk1LKDSgAwnQnby7hLoRwFuXVDfx3y3HWHipiYmIIl46IJCHEh+e/y+DfGzLxcjNy5ej+XD4iiomJwbgZz/12p7Xh7tZVAyAaaLmKfS4w8WxttNaNSqlyIARwnvUzhRDiLAK83bn/woHcf+HAVsd/NXcIC8bF8Mw3R/hoRx5vb8kmxMeDx69MYf7o6F6tyZpwVx0ca9sjt6YNSqnFwGKAuLg4Kz5aCCEc24AwX55bNIaa+ibWHipi9b4CogL69frnWhPuuUBsi9cxQNtl0860ybUMywQA7e4kaK2XAcvAPCxzLgULIYQj6udh5NIRUVw6Isomn2fNwM82IFkplaiU8gAWAqvatFkF/NTy9QJgTWfj7UIIIXpXlz13yxj6A8CXgBF4RWudrpRaAqRprVcB/wHeVEplYO6xL+zNooUQQnTOmmEZtNargdVtjj3e4uta4LqeLU0IIcS5kuUHhBDCCUm4CyGEE5JwF0IIJyThLoQQTkjCXQghnJDdlvxVShUDx8/x20NxzaUNXPG6XfGawTWv2xWvGbp/3fFa67CuGtkt3M+HUirNmoVznI0rXrcrXjO45nW74jVD7123DMsIIYQTknAXQggn5KjhvszeBdiJK163K14zuOZ1u+I1Qy9dt0OOuQshhOico/bchRBCdKJPh7srbsxtxTU/rJTar5Tao5T6VikVb486e1pX192i3QKllFZKOfysCmuuWSl1veXnna6UetvWNfYGK/6MxymlvlNK7bT8Ob/MHnX2JKXUK0qpIqXUvrOcV0qpf1p+T/Yopcae94dqrfvkL8zLCx8FkgAPYDeQ0qbNfcCLlq8XAu/au24bXPOFgLfl63sd/ZqtvW5LOz9gPbAZSLV33Tb4WScDO4Egy+twe9dto+teBtxr+ToFyLJ33T1w3dOBscC+s5y/DPgc8652k4At5/uZfbnnPgHI0Fpnaq3rgeXA/DZt5gOvW75+H5illOpoyz9H0eU1a62/01pXW15uxrwzlqOz5mcN8AfgSaDWlsX1Emuu+S5gqdb6JIDWusjGNfYGa65bA/6WrwNov/Obw9Far6eD3elamA+8oc02A4FKqfPasqkvh3tHG3O33VG21cbcwJmNuR2VNdfc0h2Y/2/v6Lq8bqXUGCBWa/2pLQvrRdb8rAcBg5RSPyilNiul5tqsut5jzXU/AdyklMrFvI/Ez2xTml119+9+l6zarMNOemxjbgdi9fUopW4CUoEZvVqRbXR63UopA/AP4FZbFWQD1vys3TAPzczE/C+0DUqp4VrrU71cW2+y5roXAa9prf9PKTUZ8y5vw7XWpt4vz256PMv6cs+9Oxtz09nG3A7EmmtGKTUb+DUwT2tdZ6PaelNX1+0HDAfWKqWyMI9JrnLwm6rW/vleqbVu0FofAw5hDntHZs113wGsANBabwK8MK+/4sys+rvfHX053F1xY+4ur9kyPPES5mB3hjFY6OK6tdblWutQrXWC1joB872GeVrrNPuU2yOs+fP9MeYb6CilQjEP02TatMqeZ811ZwOzAJRSQzGHe7FNq7S9VcAtllkzk4ByrXX+eb2jve8id3GH+TLgMOa767+2HFuC+S82mH/o7wEZwFYgyd412+CavwEKgV2WX6vsXbMtrrtN27U4+GwZK3/WCnga2A/sBRbau2YbXXcK8APmmTS7gEvsXXMPXPM7QD7QgLmXfgdwD3BPi5/1Usvvyd6e+PMtT6gKIYQT6svDMkIIIc6RhLsQQjghCXchhHBCEu5CCOGEJNyFEMIJSbgLIYQTknAXQggnJOEuhBBO6P8Dlk2yTRca7aUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(0,1,.01),plotinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL algorithms are quite robust to random errors in the training set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
