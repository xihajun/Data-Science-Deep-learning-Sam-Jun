{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 变量 loops, error probs 似乎loops越多 纠错性越好，画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from myfun import caeserde\n",
    "from myfun import data_genelization\n",
    "from myfun import misslabeled_data_genelization\n",
    "key = 3\n",
    "size = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num2str(index1, index2, size=26):\n",
    "    I2L = dict(zip(range(size), \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\n",
    "    return ([I2L[index1], I2L[index2]])\n",
    "\n",
    "\n",
    "def predict_results_only_2(model, x_train, y_train):\n",
    "    predictions = model.predict(x_train)\n",
    "    # index1, index2, label1, label2 represent two predictions and two labels respectively\n",
    "    index1 = np.argmax(predictions[:, 0:26], axis=1)\n",
    "    index2 = np.argmax(predictions[:, 26:52], axis=1)\n",
    "    label1 = np.argmax(y_train[:, 0:26], axis=1)\n",
    "    label2 = np.argmax(y_train[:, 26:52], axis=1)\n",
    "\n",
    "    # Change number to string and do an output\n",
    "    prediction_list = list(map(num2str, index1, index2))\n",
    "    label_list = list(map(num2str, label1, label2))\n",
    "\n",
    "    return (prediction_list, label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_diff(list1, list2, ifprint = False):\n",
    "    if \"\".join(list1) != \"\".join(list2):\n",
    "        if ifprint == True:\n",
    "            print(\"\".join(list1), \"\".join(list2))\n",
    "        return (\"\".join(list1), \"\".join(list2))\n",
    "    else:\n",
    "        return (True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function misslabeled_data_genelization in module myfun:\n",
      "\n",
      "misslabeled_data_genelization(sample_size=2, loops=1000, size=26, key=3, prob=0.1, x_as_vector=False, y_as_vector=False)\n",
      "    data_genelization(sample_size = 2,loops = 1000, size = 26, key = 3, x_as_vector = False, y_as_vector = False):\n",
      "    return x_train, label with equual size, labels with 1 dimension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(misslabeled_data_genelization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=0)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 1s 58us/step - loss: 0.2733 - acc: 0.9253\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1459 - acc: 0.9615\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1144 - acc: 0.9622\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0816 - acc: 0.9690\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0575 - acc: 0.9792\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0397 - acc: 0.9866\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0267 - acc: 0.9922\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0180 - acc: 0.9955\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0121 - acc: 0.9974\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0081 - acc: 0.9987\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0054 - acc: 0.9994\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0036 - acc: 0.9997\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 8.6940e-04 - acc: 1.0000\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.4030e-04 - acc: 1.0000\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.7999e-04 - acc: 1.0000\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 3.6524e-04 - acc: 1.0000\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 2.8181e-04 - acc: 1.0000\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 2.1865e-04 - acc: 1.0000\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.7182e-04 - acc: 1.0000\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.3591e-04 - acc: 1.0000\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0813e-04 - acc: 1.0000\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 8.6493e-05 - acc: 1.0000\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.9610e-05 - acc: 1.0000\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 5.6196e-05 - acc: 1.0000\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.5588e-05 - acc: 1.0000\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.7033e-05 - acc: 1.0000\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.0210e-05 - acc: 1.0000\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.4694e-05 - acc: 1.0000\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 2.0271e-05 - acc: 1.0000\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.6651e-05 - acc: 1.0000\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.3710e-05 - acc: 1.0000\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.1309e-05 - acc: 1.0000\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 9.3405e-06 - acc: 1.0000\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 7.7300e-06 - acc: 1.0000\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 6.4087e-06 - acc: 1.0000\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.3226e-06 - acc: 1.0000\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 4.4261e-06 - acc: 1.0000\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 3.6833e-06 - acc: 1.0000\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 3.0772e-06 - acc: 1.0000\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.5746e-06 - acc: 1.0000\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.1571e-06 - acc: 1.0000\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.8133e-06 - acc: 1.0000\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.5278e-06 - acc: 1.0000\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.2922e-06 - acc: 1.0000\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0960e-06 - acc: 1.0000\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 9.3311e-07 - acc: 1.0000\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 7.9692e-07 - acc: 1.0000\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 6.8385e-07 - acc: 1.0000\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.8999e-07 - acc: 1.0000\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 5.1103e-07 - acc: 1.0000\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 4.4580e-07 - acc: 1.0000\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.9107e-07 - acc: 1.0000\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.4543e-07 - acc: 1.0000\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 3.0730e-07 - acc: 1.0000\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.7532e-07 - acc: 1.0000\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.4854e-07 - acc: 1.0000\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.2619e-07 - acc: 1.0000\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 2.0733e-07 - acc: 1.0000\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.9153e-07 - acc: 1.0000\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.7835e-07 - acc: 1.0000\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.6708e-07 - acc: 1.0000\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.5768e-07 - acc: 1.0000\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.4971e-07 - acc: 1.0000\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.4296e-07 - acc: 1.0000\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 1.3726e-07 - acc: 1.0000\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.3242e-07 - acc: 1.0000\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.2837e-07 - acc: 1.0000\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.2486e-07 - acc: 1.0000\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.2192e-07 - acc: 1.0000\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1938e-07 - acc: 1.0000\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1717e-07 - acc: 1.0000\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.1528e-07 - acc: 1.0000\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.1367e-07 - acc: 1.0000\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.1229e-07 - acc: 1.0000\n",
      "Epoch 78/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.1110e-07 - acc: 1.0000\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.1002e-07 - acc: 1.0000\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0915e-07 - acc: 1.0000\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0832e-07 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0761e-07 - acc: 1.0000\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0700e-07 - acc: 1.0000\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0646e-07 - acc: 1.0000\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0597e-07 - acc: 1.0000\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0552e-07 - acc: 1.0000\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0514e-07 - acc: 1.0000\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0483e-07 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0452e-07 - acc: 1.0000\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0427e-07 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0401e-07 - acc: 1.0000\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0378e-07 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0358e-07 - acc: 1.0000\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0341e-07 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0325e-07 - acc: 1.0000\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0310e-07 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0296e-07 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0284e-07 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0272e-07 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0261e-07 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0252e-07 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0242e-07 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0233e-07 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0225e-07 - acc: 1.0000\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0217e-07 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0212e-07 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0204e-07 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0198e-07 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0193e-07 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0188e-07 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0182e-07 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0179e-07 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0175e-07 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0171e-07 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0166e-07 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0164e-07 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0160e-07 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0157e-07 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0154e-07 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0150e-07 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0150e-07 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0147e-07 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0145e-07 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0141e-07 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0140e-07 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0137e-07 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0136e-07 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0134e-07 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0132e-07 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 0s 33us/step - loss: 1.0130e-07 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0128e-07 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0126e-07 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0125e-07 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0124e-07 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0121e-07 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0120e-07 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0119e-07 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0117e-07 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0117e-07 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0116e-07 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0113e-07 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0112e-07 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0113e-07 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0112e-07 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0110e-07 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0110e-07 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0108e-07 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0106e-07 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0105e-07 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0104e-07 - acc: 1.0000\n",
      "Epoch 154/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0104e-07 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0103e-07 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0103e-07 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 1.0102e-07 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0102e-07 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0101e-07 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 1.0100e-07 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0100e-07 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0099e-07 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0098e-07 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0097e-07 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0096e-07 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0097e-07 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0095e-07 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0094e-07 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0093e-07 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0093e-07 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 1.0092e-07 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0091e-07 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0090e-07 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0089e-07 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 1.0088e-07 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 1.0087e-07 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb33e8a898>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 99us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0087433304306614e-07, 1.0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 0.2882 - acc: 0.9191\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1466 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1199 - acc: 0.9620\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0943 - acc: 0.9665\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0768 - acc: 0.9749\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0671 - acc: 0.9809\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0617 - acc: 0.9842\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0583 - acc: 0.9859\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0559 - acc: 0.9873\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0540 - acc: 0.9882\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0524 - acc: 0.9889\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0510 - acc: 0.9896\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0499 - acc: 0.9901\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0489 - acc: 0.9906\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0481 - acc: 0.9910\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0473 - acc: 0.9914\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0467 - acc: 0.9917\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0462 - acc: 0.9918\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0456 - acc: 0.9919\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0452 - acc: 0.9921\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0448 - acc: 0.9922\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0445 - acc: 0.9923\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.0442 - acc: 0.9924\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0439 - acc: 0.9924\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0436 - acc: 0.9925\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0434 - acc: 0.9925\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0431 - acc: 0.9926\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0429 - acc: 0.9926\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0427 - acc: 0.9926\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0425 - acc: 0.9926\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0424 - acc: 0.9926\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0422 - acc: 0.9926\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0420 - acc: 0.9926\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0419 - acc: 0.9927\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0417 - acc: 0.9926\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0416 - acc: 0.9927\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0414 - acc: 0.9927\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0413 - acc: 0.9927\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0412 - acc: 0.9927\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0411 - acc: 0.9927\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0410 - acc: 0.9927\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0409 - acc: 0.9927\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0407 - acc: 0.9927\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.0406 - acc: 0.9927\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0405 - acc: 0.9927\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0404 - acc: 0.9927\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.0403 - acc: 0.9927\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0402 - acc: 0.9927\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.0401 - acc: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb34edfac8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=0.1)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 12us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.010234667018055916, 0.9998576957702636]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 65us/step - loss: 0.2886 - acc: 0.9219\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1595 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1543 - acc: 0.9615\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1480 - acc: 0.9615\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1427 - acc: 0.9615\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1390 - acc: 0.9616\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1362 - acc: 0.9616\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1342 - acc: 0.9617\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1325 - acc: 0.9618\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1311 - acc: 0.9619\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1301 - acc: 0.9621\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1293 - acc: 0.9622\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1287 - acc: 0.9623\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1282 - acc: 0.9623\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1278 - acc: 0.9624\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1273 - acc: 0.9624\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1270 - acc: 0.9625\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1266 - acc: 0.9626\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1263 - acc: 0.9627\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1260 - acc: 0.9627\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1257 - acc: 0.9629\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1255 - acc: 0.9628\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1253 - acc: 0.9627\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1251 - acc: 0.9629\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1249 - acc: 0.9629\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1247 - acc: 0.9629\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1246 - acc: 0.9630\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1244 - acc: 0.9629\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1242 - acc: 0.9631\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1240 - acc: 0.9632\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1240 - acc: 0.9631\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1238 - acc: 0.9631\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1236 - acc: 0.9633\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1235 - acc: 0.9632\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1234 - acc: 0.9631\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1233 - acc: 0.9632\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1231 - acc: 0.9632\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9634\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9634\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1228 - acc: 0.9634\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1227 - acc: 0.9635\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1227 - acc: 0.9635\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1226 - acc: 0.9633\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1225 - acc: 0.9634\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1225 - acc: 0.9635\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1224 - acc: 0.9635\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1223 - acc: 0.9634\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1223 - acc: 0.9635\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1222 - acc: 0.9634\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1222 - acc: 0.9635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb350bccc0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000,prob=0.5)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 13us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.05628207671642303, 0.9727115383148194]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100% noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 1s 69us/step - loss: 0.2870 - acc: 0.9283\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1636 - acc: 0.9615\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1634 - acc: 0.9615\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1633 - acc: 0.9615\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1632 - acc: 0.9615\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1631 - acc: 0.9615\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1631 - acc: 0.9615\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1630 - acc: 0.9615\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1629 - acc: 0.9615\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1628 - acc: 0.9615\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1628 - acc: 0.9615\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1627 - acc: 0.9615\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 0s 26us/step - loss: 0.1626 - acc: 0.9615\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1625 - acc: 0.9615\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1625 - acc: 0.9615\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1624 - acc: 0.9615\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 0s 31us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1622 - acc: 0.9615\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1622 - acc: 0.9615\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1621 - acc: 0.9615\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 0.9615\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1619 - acc: 0.9615\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9615\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1617 - acc: 0.9615\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 0s 27us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1616 - acc: 0.9615\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 0s 28us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 0s 29us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 0s 30us/step - loss: 0.1614 - acc: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb356bd9e8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000,prob=1)\n",
    "# assume that we have got our data x_train y_train\n",
    "# now we are going to train it in our model\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "# change loss from possion to binary it works well :P\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 153us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.16452448892593383, 0.9615384340286255]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train, y_train_small = data_genelization()\n",
    "model.evaluate(x_train, y_train)\n",
    "# 他这个accuracy不太对 是计算整体的好像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04283839, 0.03006983, 0.03108829, 0.03086495, 0.03821969,\n",
       "       0.04318464, 0.04275879, 0.02881029, 0.02591237, 0.05666709,\n",
       "       0.04924551, 0.04192016, 0.04666957, 0.04626572, 0.04118401,\n",
       "       0.03622824, 0.02944967, 0.03765562, 0.03575623, 0.03924397,\n",
       "       0.03410533, 0.03530636, 0.04261568, 0.03204012, 0.04352927,\n",
       "       0.03008407, 0.05662912, 0.04116714, 0.04893467, 0.05282199,\n",
       "       0.03043225, 0.04488924, 0.05681634, 0.04138765, 0.05441827,\n",
       "       0.02774709, 0.03407055, 0.04218432, 0.03569707, 0.03512883,\n",
       "       0.04342568, 0.05268621, 0.04676977, 0.02773789, 0.02489308,\n",
       "       0.0423353 , 0.04129213, 0.03324315, 0.03560084, 0.02898619,\n",
       "       0.03714329, 0.036331  ], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "prediction_list, label_list = predict_results_only_2(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = list(map(find_diff, prediction_list, label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff.count(True)/len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_prob_curve(prob):\n",
    "    x_train, y_train, y_train_small = misslabeled_data_genelization(loops=1000, prob=prob)\n",
    "    # assume that we have got our data x_train y_train\n",
    "    # now we are going to train it in our model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    # change loss from possion to binary it works well :P\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train,epochs = 200)\n",
    "    \n",
    "    \n",
    "    x_train, y_train, y_train_small = data_genelization()\n",
    "    model.evaluate(x_train, y_train)\n",
    "\n",
    "    prediction_list, label_list = predict_results_only_2(model, x_train, y_train)\n",
    "    diff = list(map(find_diff, prediction_list, label_list))\n",
    "    count = diff.count(True)/len(diff)\n",
    "    return(count)\n",
    "\n",
    "def plot_acc_prob_curve_10000(prob):\n",
    "    x_train, y_train, y_train_small = misslabeled_data_genelization(loops=10000, prob=prob)\n",
    "    # assume that we have got our data x_train y_train\n",
    "    # now we are going to train it in our model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=x_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(y_train.shape[1], activation='sigmoid'))\n",
    "    # change loss from possion to binary it works well :P\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(x_train,y_train,epochs = 200, batch_size = 1000)\n",
    "    \n",
    "    \n",
    "    x_train, y_train, y_train_small = data_genelization()\n",
    "    model.evaluate(x_train, y_train)\n",
    "\n",
    "    prediction_list, label_list = predict_results_only_2(model, x_train, y_train)\n",
    "    diff = list(map(find_diff, prediction_list, label_list))\n",
    "    count = diff.count(True)/len(diff)\n",
    "    return(count)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It takes hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## %%time\n",
    "plotinfo = list(map(plot_acc_prob_curve,np.arange(0,1,.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/noise_size_1000.pickle', 'wb') as f:\n",
    "    pickle.dump(plotinfo, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb78aeb2b0>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(0,1,.01),plotinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL algorithms are quite robust to random errors in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10000/10000 [==============================] - 6s 582us/step - loss: 0.6814 - acc: 0.5963\n",
      "Epoch 2/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.6629 - acc: 0.7075\n",
      "Epoch 3/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.6390 - acc: 0.7939\n",
      "Epoch 4/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.6060 - acc: 0.8548\n",
      "Epoch 5/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.5605 - acc: 0.9007\n",
      "Epoch 6/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.4999 - acc: 0.9360\n",
      "Epoch 7/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.4253 - acc: 0.9557\n",
      "Epoch 8/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.3443 - acc: 0.9612\n",
      "Epoch 9/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.2709 - acc: 0.9615\n",
      "Epoch 10/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.2177 - acc: 0.9615\n",
      "Epoch 11/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1879 - acc: 0.9615\n",
      "Epoch 12/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1748 - acc: 0.9615\n",
      "Epoch 13/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1697 - acc: 0.9615\n",
      "Epoch 14/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1674 - acc: 0.9615\n",
      "Epoch 15/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1658 - acc: 0.9615\n",
      "Epoch 16/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1644 - acc: 0.9615\n",
      "Epoch 17/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1633 - acc: 0.9615\n",
      "Epoch 18/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1623 - acc: 0.9615\n",
      "Epoch 19/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1615 - acc: 0.9615\n",
      "Epoch 20/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1608 - acc: 0.9615\n",
      "Epoch 21/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1601 - acc: 0.9615\n",
      "Epoch 22/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1595 - acc: 0.9615\n",
      "Epoch 23/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1589 - acc: 0.9615\n",
      "Epoch 24/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1583 - acc: 0.9615\n",
      "Epoch 25/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1577 - acc: 0.9615\n",
      "Epoch 26/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1571 - acc: 0.9615\n",
      "Epoch 27/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1565 - acc: 0.9615\n",
      "Epoch 28/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1558 - acc: 0.9615\n",
      "Epoch 29/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1552 - acc: 0.9615\n",
      "Epoch 30/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1545 - acc: 0.9615\n",
      "Epoch 31/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1539 - acc: 0.9615\n",
      "Epoch 32/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1531 - acc: 0.9615\n",
      "Epoch 33/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1524 - acc: 0.9615\n",
      "Epoch 34/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1516 - acc: 0.9615\n",
      "Epoch 35/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1508 - acc: 0.9615\n",
      "Epoch 36/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1499 - acc: 0.9615\n",
      "Epoch 37/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1490 - acc: 0.9615\n",
      "Epoch 38/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1480 - acc: 0.9615\n",
      "Epoch 39/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1470 - acc: 0.9615\n",
      "Epoch 40/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1460 - acc: 0.9615\n",
      "Epoch 41/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1449 - acc: 0.9615\n",
      "Epoch 42/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1438 - acc: 0.9615\n",
      "Epoch 43/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1426 - acc: 0.9615\n",
      "Epoch 44/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1413 - acc: 0.9615\n",
      "Epoch 45/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1400 - acc: 0.9615\n",
      "Epoch 46/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1387 - acc: 0.9615\n",
      "Epoch 47/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1373 - acc: 0.9615\n",
      "Epoch 48/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1359 - acc: 0.9615\n",
      "Epoch 49/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1344 - acc: 0.9615\n",
      "Epoch 50/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1330 - acc: 0.9615\n",
      "Epoch 51/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1315 - acc: 0.9615\n",
      "Epoch 52/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1299 - acc: 0.9615\n",
      "Epoch 53/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1284 - acc: 0.9615\n",
      "Epoch 54/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1269 - acc: 0.9615\n",
      "Epoch 55/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1253 - acc: 0.9615\n",
      "Epoch 56/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1238 - acc: 0.9615\n",
      "Epoch 57/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1223 - acc: 0.9615\n",
      "Epoch 58/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1208 - acc: 0.9615\n",
      "Epoch 59/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1193 - acc: 0.9615\n",
      "Epoch 60/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1178 - acc: 0.9615\n",
      "Epoch 61/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1163 - acc: 0.9616\n",
      "Epoch 62/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1148 - acc: 0.9617\n",
      "Epoch 63/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1134 - acc: 0.9619\n",
      "Epoch 64/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1120 - acc: 0.9620\n",
      "Epoch 65/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1106 - acc: 0.9622\n",
      "Epoch 66/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1092 - acc: 0.9623\n",
      "Epoch 67/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1078 - acc: 0.9625\n",
      "Epoch 68/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1065 - acc: 0.9627\n",
      "Epoch 69/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1051 - acc: 0.9629\n",
      "Epoch 70/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1038 - acc: 0.9631\n",
      "Epoch 71/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1025 - acc: 0.9633\n",
      "Epoch 72/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.1012 - acc: 0.9635\n",
      "Epoch 73/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0999 - acc: 0.9638\n",
      "Epoch 74/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0986 - acc: 0.9641\n",
      "Epoch 75/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0973 - acc: 0.9643\n",
      "Epoch 76/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0960 - acc: 0.9646\n",
      "Epoch 77/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0948 - acc: 0.9649\n",
      "Epoch 78/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0935 - acc: 0.9653\n",
      "Epoch 79/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0923 - acc: 0.9658\n",
      "Epoch 80/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0910 - acc: 0.9663\n",
      "Epoch 81/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0898 - acc: 0.9667\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0886 - acc: 0.9672\n",
      "Epoch 83/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0873 - acc: 0.9677\n",
      "Epoch 84/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0861 - acc: 0.9680\n",
      "Epoch 85/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0849 - acc: 0.9685\n",
      "Epoch 86/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0836 - acc: 0.9687\n",
      "Epoch 87/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0824 - acc: 0.9691\n",
      "Epoch 88/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0812 - acc: 0.9696\n",
      "Epoch 89/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0800 - acc: 0.9700\n",
      "Epoch 90/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0787 - acc: 0.9704\n",
      "Epoch 91/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0775 - acc: 0.9709\n",
      "Epoch 92/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0763 - acc: 0.9713\n",
      "Epoch 93/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0751 - acc: 0.9718\n",
      "Epoch 94/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0738 - acc: 0.9724\n",
      "Epoch 95/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0726 - acc: 0.9730\n",
      "Epoch 96/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0713 - acc: 0.9735\n",
      "Epoch 97/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0699 - acc: 0.9741\n",
      "Epoch 98/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0685 - acc: 0.9751\n",
      "Epoch 99/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0671 - acc: 0.9757\n",
      "Epoch 100/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0657 - acc: 0.9763\n",
      "Epoch 101/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0643 - acc: 0.9768\n",
      "Epoch 102/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0629 - acc: 0.9772\n",
      "Epoch 103/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0616 - acc: 0.9778\n",
      "Epoch 104/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0602 - acc: 0.9783\n",
      "Epoch 105/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0589 - acc: 0.9789\n",
      "Epoch 106/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0576 - acc: 0.9795\n",
      "Epoch 107/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0564 - acc: 0.9801\n",
      "Epoch 108/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0552 - acc: 0.9807\n",
      "Epoch 109/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0541 - acc: 0.9812\n",
      "Epoch 110/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0530 - acc: 0.9818\n",
      "Epoch 111/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0519 - acc: 0.9823\n",
      "Epoch 112/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0508 - acc: 0.9828\n",
      "Epoch 113/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0498 - acc: 0.9832\n",
      "Epoch 114/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0489 - acc: 0.9835\n",
      "Epoch 115/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0479 - acc: 0.9842\n",
      "Epoch 116/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0470 - acc: 0.9845\n",
      "Epoch 117/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0461 - acc: 0.9848\n",
      "Epoch 118/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0453 - acc: 0.9853\n",
      "Epoch 119/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0444 - acc: 0.9856\n",
      "Epoch 120/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0436 - acc: 0.9859\n",
      "Epoch 121/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0428 - acc: 0.9863\n",
      "Epoch 122/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0420 - acc: 0.9866\n",
      "Epoch 123/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0413 - acc: 0.9870\n",
      "Epoch 124/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0405 - acc: 0.9873\n",
      "Epoch 125/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0398 - acc: 0.9875\n",
      "Epoch 126/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0391 - acc: 0.9877\n",
      "Epoch 127/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0384 - acc: 0.9880\n",
      "Epoch 128/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0377 - acc: 0.9883\n",
      "Epoch 129/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0370 - acc: 0.9885\n",
      "Epoch 130/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0363 - acc: 0.9889\n",
      "Epoch 131/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0357 - acc: 0.9891\n",
      "Epoch 132/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0350 - acc: 0.9895\n",
      "Epoch 133/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0344 - acc: 0.9899\n",
      "Epoch 134/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0338 - acc: 0.9902\n",
      "Epoch 135/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0332 - acc: 0.9903\n",
      "Epoch 136/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0326 - acc: 0.9905\n",
      "Epoch 137/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0320 - acc: 0.9906\n",
      "Epoch 138/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0314 - acc: 0.9909\n",
      "Epoch 139/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0308 - acc: 0.9912\n",
      "Epoch 140/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0302 - acc: 0.9914\n",
      "Epoch 141/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0297 - acc: 0.9917\n",
      "Epoch 142/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0291 - acc: 0.9918\n",
      "Epoch 143/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0286 - acc: 0.9920\n",
      "Epoch 144/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0280 - acc: 0.9922\n",
      "Epoch 145/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0275 - acc: 0.9924\n",
      "Epoch 146/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0269 - acc: 0.9926\n",
      "Epoch 147/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0264 - acc: 0.9929\n",
      "Epoch 148/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0259 - acc: 0.9931\n",
      "Epoch 149/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0254 - acc: 0.9933\n",
      "Epoch 150/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0249 - acc: 0.9934\n",
      "Epoch 151/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0244 - acc: 0.9936\n",
      "Epoch 152/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0239 - acc: 0.9938\n",
      "Epoch 153/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0234 - acc: 0.9939\n",
      "Epoch 154/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0229 - acc: 0.9941\n",
      "Epoch 155/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0225 - acc: 0.9943\n",
      "Epoch 156/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0220 - acc: 0.9944\n",
      "Epoch 157/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0215 - acc: 0.9945\n",
      "Epoch 158/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0211 - acc: 0.9947\n",
      "Epoch 159/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0206 - acc: 0.9949\n",
      "Epoch 160/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0202 - acc: 0.9950\n",
      "Epoch 161/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0198 - acc: 0.9952\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0193 - acc: 0.9952\n",
      "Epoch 163/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0189 - acc: 0.9954\n",
      "Epoch 164/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0185 - acc: 0.9956\n",
      "Epoch 165/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0181 - acc: 0.9957\n",
      "Epoch 166/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0177 - acc: 0.9959\n",
      "Epoch 167/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0173 - acc: 0.9959\n",
      "Epoch 168/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0169 - acc: 0.9960\n",
      "Epoch 169/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0165 - acc: 0.9961\n",
      "Epoch 170/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0162 - acc: 0.9962\n",
      "Epoch 171/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0158 - acc: 0.9962\n",
      "Epoch 172/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0154 - acc: 0.9965\n",
      "Epoch 173/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0151 - acc: 0.9966\n",
      "Epoch 174/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0147 - acc: 0.9967\n",
      "Epoch 175/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0144 - acc: 0.9967\n",
      "Epoch 176/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0141 - acc: 0.9968\n",
      "Epoch 177/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0137 - acc: 0.9969\n",
      "Epoch 178/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0134 - acc: 0.9970\n",
      "Epoch 179/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0131 - acc: 0.9971\n",
      "Epoch 180/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0128 - acc: 0.9972\n",
      "Epoch 181/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0125 - acc: 0.9972\n",
      "Epoch 182/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0122 - acc: 0.9974\n",
      "Epoch 183/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0119 - acc: 0.9975\n",
      "Epoch 184/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0116 - acc: 0.9976\n",
      "Epoch 185/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0113 - acc: 0.9977\n",
      "Epoch 186/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0110 - acc: 0.9978\n",
      "Epoch 187/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0108 - acc: 0.9979\n",
      "Epoch 188/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0105 - acc: 0.9980\n",
      "Epoch 189/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0102 - acc: 0.9981\n",
      "Epoch 190/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0100 - acc: 0.9982\n",
      "Epoch 191/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0098 - acc: 0.9983\n",
      "Epoch 192/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0095 - acc: 0.9984\n",
      "Epoch 193/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0093 - acc: 0.9984\n",
      "Epoch 194/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0091 - acc: 0.9985\n",
      "Epoch 195/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0088 - acc: 0.9985\n",
      "Epoch 196/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0086 - acc: 0.9986\n",
      "Epoch 197/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0084 - acc: 0.9986\n",
      "Epoch 198/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0082 - acc: 0.9986\n",
      "Epoch 199/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0080 - acc: 0.9987\n",
      "Epoch 200/200\n",
      "10000/10000 [==============================] - 0s 7us/step - loss: 0.0078 - acc: 0.9988\n",
      "1000/1000 [==============================] - 2s 2ms/step\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-8c4cfbe5351b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplotinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot_acc_prob_curve_10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-95f4ddfb3c19>\u001b[0m in \u001b[0;36mplot_acc_prob_curve_10000\u001b[0;34m(prob)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# change loss from possion to binary it works well :P\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                     \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;31m# hack for list_devices() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plotinfo = list(map(plot_acc_prob_curve_10000,np.arange(0,1,.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12d8dbe10>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8VOW9+PHPNzuBLEBCAlkIyJqEPSCC1gUrwQUEtWKvVry21rq0vXbD9trF2lZve7W3V23Lr1q19oqURRAVtIpVECGTsAaIRpiQhC0hCyErkzy/PzLRGBMySWbmzEy+79eL12vmzHPO+c55hXxzzvN9nkeMMSillFJBVgeglFLKN2hCUEopBWhCUEop5aQJQSmlFKAJQSmllJMmBKWUUoAmBKWUUk6aEJRSSgGaEJRSSjmFWB1AT8TFxZm0tDSrw1BKKb+Sm5tbboyJ766dXyWEtLQ0bDab1WEopZRfEZEiV9rpIyOllFKAJgSllFJOmhCUUkoBmhCUUko5aUJQSikFaEJQSinlpAlBKaUUoAlBWaS67hyrcorRJVyV8h2aEJQl/vzeJ/xwzV52HqmwOhSllJMmBOV1LS2GdbtKAdicf9LiaJRSbTQhKK/78PBpjlc3EB0Rwub8E/rYSCkfoQlBed2avFKiwkP4YfYESqvq2VdabXVISik0ISgvq2tysGn/ca6eNJxrJw8nOEjYtP+E1WEppdCEoLzszfyT1DY1s2R6ErGRYVw0eiib9utjI6V8gSYE5VVr8kpIHjyAmWlDAJifmcjh8loKT521ODKllCYE5TUnqhvYVljOkmlJBAUJAPPTExBBHxsp5QM0ISivWb+7lBYDi6cnf7ptWHQE01MHsylfE4JSVtOEoLzCGMPavFKmp8YyKm7g5z7Lzkgk/9gZiivqLIpOKQUuJgQRyRaRAhEpFJHlnXweLiIvOz/fISJpzu1DRWSLiJwVkSe7OPYGEdnfly+hfN+B42coOFnzubuDNvMzEgF9bKSU1bpNCCISDDwFLADSgVtEJL1DszuBSmPMGOAJ4DHn9gbgIeD7XRx7CaC9if3A2rxSQoOF6yYP/8JnqUMjSR8erY+NlLKYK3cIs4BCY8xhY0wTsBJY1KHNIuB55+vVwDwREWNMrTFmK62J4XNEZBDwAPBIr6P3M/nHqqlrclgdhtc5mltYv7uUeRMSiI0M67RNdmYiuUWVnDrzhR8VpZSXuJIQkoDidu9LnNs6bWOMcQDVwNBujvtL4L+B8z44FpG7RMQmIraysjIXwvVN1fXnuP6pbfzm9UNWh+J1739cTvnZJpZM7/hj85kFma2PjTYf0LmNlLKKKwlBOtnWcRSRK20+aywyFRhjjFnX3cmNMSuMMVnGmKz4+PjumvusvKOVnGs2rMkrobr+nNXheNWavBIGR4Zy2fhhXbYZM2wQo+MHsln7EZSyjCsJoQRIafc+GTjWVRsRCQFigPPNa3wRMENE7MBWYJyIvOtayP7JZq9ABOqamvmHrbj7HQJEdf053jxwkoVTRhAW0vWPm4iQnZHI9sOnqapr8mKESqk2riSEHGCsiIwSkTBgKbChQ5sNwO3O1zcC75jzzEVgjPmjMWaEMSYNuBj4yBhzWU+D9yc59komJ8eSNXIwL2wvormlf0zV8Ma+4zQ5WljSSXVRR9mZiTS3GP558JQXIlNKddRtQnD2CdwHbAYOAquMMfki8rCILHQ2ewYYKiKFtHYUf1qa6rwLeBxYJiIlnVQoBbwmRwt7iquYOXIwt89J42hFHe8W9I9femt3lTI6fiCTk2O6bTspKYYRMRFafqqURUJcaWSMeR14vcO2n7Z73QDc1MW+ad0c2w5kuhKHv9p/rJpGRwtZaYOZNzGBhOhwnvvAzryJCVaH5lHFFXXsPFLBD+aPR6SzbqbPExHmZyby9x1HqW10MDDcpR9PpZSb6EhlL7DZW7tTZowcQmhwELdeOJL3Py4P+And2lZFu35a19VFHWVnJNLkaGFLP7mDUsqXaELwghx7JaPiBhIfFQ7ALRemEhYcxAvb7ZbG5UmtU1WUcNHooSTFDnB5v6y0IcQNCtPHRkpZQBOChxljyC2qJGvk4E+3xQ0K59opw1mdW8KZhsAsQc07WoX9dN15xx50JjhI+HJ6IlsOnaLhXLOHolNKdUYTgocdLq+loraJrLTBn9t+x5xR1DU1s9pWYlFknrU2r4SI0CAWTPriVBXdyc5MpLapmW2F5R6ITCnVFU0IHtbWf5DlXBCmzaTkGKanxvLCdjstAVaC2uhoZuPe48zPSGRQLzqGLxo9lKiIEH1spJSXaULwsBx7JUMGhjG6w5TPAMvmjsJ+uo5/feS/U3J0ZsuhU1TXn3Np7EFnwkKCuHJiAm8dPImjucXN0SmluqIJwcNyiyqZMXJwp2WXCzITGRYVzl8/sHs/MA9ak1fKsKhw5l7Q3XRWXZufkUhV3Tl2HjnfgHellDtpQvCgsppGjpTXMrND/0Gb0OAgbp09kvc+KuOTssAoQa2obWLLoVNcPy2JkODe/3hdOi6eiNAgnRJbKS/ShOBBuUWd9x+0d8ssZwlqgNwlvLrnGI4W0+Pqoo4GhAVz2bhhbNp/IuD6WJTyVZoQPCjHXkl4SBCZI7qetiE+KpxrJ7eWoNYEQAnq2rwS0odHMyExus/Hys5M5FRNI7uKq9wQmVKqO5oQPMhWVMmUlNjzzvIJcPucNGqbmlmd698lqIWnathTUt3nu4M2V0wcRmiwsFkfGynlFZoQPKSuyUF+aXWX/QftTUmJZVpqLC9sL/LrxyNr80oJElg4dYRbjhcdEcrcMXFs2n+C80yeq5RyE00IHrK7uApHizlv/0F7y+akcaS8ln997J8lqC0thld2lfKlcfEMi4pw23GzMxI5WlHHweM1bjumUqpzmhA8JNdeiQhMT+3+DgFgQeZw4qPCed5PO5c/PHKaY9UNvR570JUr0xMIErTaSCkv0ITgITlFlYxPiCJmQKhL7cNCgvi3C1N5t6CMw35Ygro2r5So8BCuSnfvlN5xg8KZmTZEl9ZUygs0IXhAc4shr6jyC/MXdeerF6YSGiy8sL3IQ5F5Rl2Tgzf2HefqScOJCA12+/GzMxMpOFnjl4lSKX+iCcEDDp04w9lGBzNd7D9oMywqgmsmtZagnm10eCg693sz/yS1Tc1uqy7qaH5GIqCPjZTyNJcSgohki0iBiBSKyPJOPg8XkZedn+8QkTTn9qEiskVEzorIk+3aR4rIayJySETyReRRd30hX5BbVAnAjJE9u0OA1hLUs40O1vhRCeqavBKSYgf0OAG6akTsAKYkx+hjI6U8rNuEICLBwFPAAiAduKWTdZHvBCqNMWOAJ4DHnNsbgIeA73dy6N8ZYyYA04C5IrKgd1/B9+TYKxkeE9GjhWHaTEsdzJSUWJ7/wD9mQT15poFtheUsmZ5EUFD3y2T2VnbmcPaUVFNaVe+xcyjV37lyhzALKDTGHDbGNAErgUUd2iwCnne+Xg3MExExxtQaY7bSmhg+ZYypM8Zscb5uAvIA95anWMQYQ86RCrLShri0jnBnls0ZyeHyWt73g/UA1u8upcXA4h4sk9kb8zNaO6vf1MdGSnmMKwkhCShu977Eua3TNsYYB1ANuDTVpYjEAtcBb7vS3teVVtVz4kyDSwPSunL1pOHEDQrnuW1H3BiZ+xljWJNbyrTUWEbHD/LouUbHD2J8QpSukaCUB7mSEDr7M7fjswxX2nzxwCIhwEvAH4wxh7toc5eI2ETEVlbm+4O2+tJ/0CY8JJivXpjKloIyjpTXuis0tztw/AwFJ2vcPvagK/MzE8mxV1B+ttEr51Oqv3ElIZQAKe3eJwPHumrj/CUfA7gykf0K4GNjzO+7amCMWWGMyTLGZMXHx7twSGvl2CsYFB7S58ndbr0wlZAg4YXtdrfE5Qlr80oJDRaum9zzZTJ7IzsjkRYD/zxw0ivnU6q/cSUh5ABjRWSUiIQBS4ENHdpsAG53vr4ReMd0M/mMiDxCa+L4bs9C9m02eyXTRw4muI8drMOiI7h60nBW20qo9cESVEdzC+t3lzJvQgKxkWFeOefE4VGkDonU8lOlPKTbhODsE7gP2AwcBFYZY/JF5GERWehs9gwwVEQKgQeAT0tTRcQOPA4sE5ESEUkXkWTgJ7RWLeWJyG4R+bo7v5gVquvPUXCyhqw+PC5qb9ncNGoaHazN870S1Pc/Lqf8bBOLPTT2oDMiQnZmItsKyzkTAFOFK+VrXFoB3RjzOvB6h20/bfe6Abipi33Tujis52oULZJ3tBJj6PEI5a5MS4llcnIMz31g59bZI3tdteQJa3eVEhsZyuXjh3n1vPMzElnx3mHeOdi6KptSyn10pLIb2ewVhAQJU1Ni3XI8EWHZnDQ+Katlqw+VoJ5pOMeb+SdYOGVEt2s9uNu0lFgSosO12kgpD9CE4EY59koykmKIDHPpxssl10weTtygMJ7bZnfbMfvqjX3HaXS0eK26qL2gIGF+RiLvfnSK+qZmr59fqUCmCcFNmhwt7Cmuclv/QZvwkGC+OiuVdwpOUXTaN0pQ1+SVMjp+IFOSu14a1JOyMxJpONfCvz7y/TJkpfyJJgQ32X+smkZHS58GpHXl32aPJFh8YxbU4oo6dh6p4IbpyZb1acwaNYTYyFBdWlMpN9OE4CY2e+uwixkj3T/BW0J0BAsmDWeVrdjyEtR1u0oBLO3QDQkO4ssTE/jnwZM0OVosi0OpQKMJwU1y7JWMihtIfFS4R46/bM5IahocrHX+QraCMYa1eSXMHj2kVxP3uVN2ZiI1DQ62Hz5taRxKBRJNCG5gjCG3qLJP01V0Z3rqYCYlxfD8B3bLFpzfVVyF/XSdJZ3JHc0dE8fAsGCtNlLKjTQhuMHh8loqaps80n/QRkS4fU4ahafOsq3Qmr+K1+aVEBEaxILMREvO315EaDCXTxjGWwdO0OwH04Qr5Q80IbhBW/9BlocWiGlz7eThDB0YxnMf2D16ns40Opp5dc9x5mckEhXh2jrRnpadmUj52aZPr79Sqm80IbiBzV7JkIFhjI4b6NHzRIQGc8usVN4+dJKjp+s8eq6Othw6RXX9OZ94XNTm8vHDCAsJ0rmNlHITTQhuYHP2H3ijDPPW2SMJEuFvH9o9fq721uSVMiwqnLkXuLTMhVcMDA/hS2Pj2bz/hGX9KkoFEk0IfVRW08iR8lqP9h+0lxgTQXZmIi/nFFPX5J0S1IraJrYcOsWiqSMICfatH5nszESOVTewr7Ta6lCU8nu+9b/bD+UWeaf/oL075qRxpsHx6ZgAT3t1zzEcLcanHhe1uXLiMIKDRKuNlHIDTQh9ZLNXEh4SROYI703jMGPkYDJGRHutBHXtrlImDo9m4vC+LfrjCbGRYVw0eiib9LGRUn2mCaGPcooqmZIS69VZP9tmQf3o5Fm2f+LZEtTCU2fZU1zFDV5c96Cn5mcmcri8lsJTZ60ORSm/pgmhD+qaHOSXVnut/6C966aMYMjAMP7q4RLUdbtKCBJYOHWER8/TF/PTExCBN/SxkVJ9ogmhD3YXV+FoMV7tP2jTWoKawtsHT1Jc4ZkS1JYWw7q8Ur40Lp5hUREeOYc7DIuOYHrqYO1HUKqPXEoIIpItIgUiUigiyzv5PFxEXnZ+vkNE0pzbh4rIFhE5KyJPdthnhojsc+7zB/Gl5cBclGuvRKR1WgkrtK2i9rcPPTML6odHTnOsusEnO5M7WpCZyIHjZ7w+PkOpQNJtQhCRYOApYAGtayDfIiLpHZrdCVQaY8YATwCPObc3AA8B3+/k0H8E7gLGOv9l9+YLWCmnqJLxCVHEDLBm5O7wmAFkZySycudRj5Sgrs0rZVB4CFelJ7j92O42P6N1Og2dElup3nPlDmEWUGiMOWyMaQJWAos6tFkEPO98vRqYJyJijKk1xmylNTF8SkSGA9HGmO2mtTTkBeD6vnwRb2tuMeQVVbpt/eTeut1ZgvrKrmNuPW59UzNv7DvO1ZMSiQgNduuxPSFlSCQZI6J11LJSfeBKQkgCitu9L3Fu67SNMcYBVAPnG9Ka5DzO+Y7p0wpO1HC20UGWB9Y/6ImZaYNJH+7+EtQ3D5ygtqnZLx4XtcnOSCS3qJJTZxq6b6yU+gJXEkJnz/Y7/uZxpU2v2ovIXSJiExFbWZnvLJlo+3RAmrV3CG0lqAUna9y6NsCavFKSYgcwy4IO897Kds7CuvnASYsjUco/uZIQSoCUdu+TgY7PJz5tIyIhQAxwvikoS5zHOd8xATDGrDDGZBljsuLj410I1zty7JUMj4mwfKEYaC0JHRwZyvNuKkE9eaaBrR+XsWR6EkFB/tPXP2bYIEbHD2SzVhsp1SuuJIQcYKyIjBKRMGApsKFDmw3A7c7XNwLvmPM8vzDGHAdqRGS2s7roa8D6HkdvIZu9gqy0IZatK9xeRGgwS2el8taBk5RU9r3KZv3uUloMLLZwmczeEBGyMxLZfvg0VXVNVoejlN/pNiE4+wTuAzYDB4FVxph8EXlYRBY6mz0DDBWRQuAB4NPSVBGxA48Dy0SkpF2F0reAvwCFwCfAG+75Sp5XWlXP8eoGsjy4QlpPuasE1RjDmtxSpqbEMjp+kJui857szESaWwxv6WMjpXosxJVGxpjXgdc7bPtpu9cNwE1d7JvWxXYbkOlqoL7kswVxfCchJMUO4Kr0BFbuLOa788YxIKx3lUEHjp+h4GQNv1yU4eYIvWNSUgxJsQPYnH+Cm7JSut9BKfUpHancCzn2CgaFhzAh0bcme1s2J43q+nOs3937WVDX5ZUSGixcO9l3p6o4HxFhfkYi731cztlG70wPrlSg0ITQCzZ7JdNSYwn2sQ7XWaOGMCExiud6WYLqaG7hld3HuGLCMAYPDPNAhN6RnZlIk6OFdwtOWR2KUn5FE0IPVdefo+BkDTN9sBxTRLhjbhqHTtSw40jP1xl+v7Cc8rONfjX2oDMzRg4mblCYzm2kVA9pQuihvKOVGONb/QftLZqaRGxkKM9ts/d437V5pcRGhnL5+GHuD8yLgoOEL6cnsuXQKRrONVsdjlJ+QxNCD9nsFYQECVNTYq0OpVMRocEsnZnKmwdOUFpV7/J+ZxrO8Wb+CRZOGeHVtR08JTszkdqmZrYVllsdilJ+w///53uZzV5JxohoIsNcKtCyxK2zUwH423bXS1Df2HecRkeL34096MpFo4cSFRGij42U6gFNCD3Q5Ghhd3GVJesf9ETy4EiuSk9kZc5Rlx+ZrMkrZXTcQJ+98+mpsJAgrpyYwFsHT3KuucXqcJTyC5oQemD/sWoaHS2WrJDWU7fPSaOqzrUS1OKKOnYeqWDJ9CSfGHntLvMzEqmqO8fOXnSwK9UfaULogbYBaTMsnuHUFbNHt5WgFnVbgvrKrtakcX2APC5qc+m4eAaEButjI6VcpAmhB2z2StKGRhIfFW51KN0SEW6fk8bB42fO+xeyMYa1u0qZPXoIyYMjvRih5w0IC+ay8fFszj9BS4v7pgZXKlBpQnCRMQZbUaXP9x+0d/3UJGIGhPL8dnuXbXYVV3GkvNbvxx50JTszkVM1jewqrrI6FKV8niYEFx0ur6Witskv+g/aDAgLZunMFDbnn+RYFyWoa/NKCA8JYoFzLYFAc/mEYYQGiy6tqZQLNCG4yJ/6D9q7dfZIjDG82MksqI2OZl7dc5z5GYlERVizLrSnRUeEMndMHJv2n3DrinJKBSJNCC6y2SsZHBnKBfEDrQ6lR1KGRHLlxARe2vnFEtQth8qorj/HkumB1ZncUXZGIkcr6jh4vMbqUJTyaZoQXNTWf+CPZZnL5qZRWXeODXs+vyjd2rwS4qPCuXhMnEWReceV6QkECWzaf9zqUJTyaZoQXFBW08iR8lq/6j9o76LRQxmfEMVz2z6bBbWitoktBae4fuoIQoID+8cgblA4M9OGsEn7EZQ6r8D+TeAmuUX+2X/Qpq0E9cDxM9iKKgHYuPcY55pNwFYXdbQgM5GPTp7lk7KzVoeilM/ShOACm72S8JAgMpN8a0Gcnrh+2giiI0J47gM70DpVxcTh0Uwc7r/fqSeuymitotJqI6W65lJCEJFsESkQkUIRWd7J5+Ei8rLz8x0iktbuswed2wtEZH677f8hIvkisl9EXhKRCHd8IU/IKapkSkos4SG9W5bSF0SGhbB0Viqb9p9gW2E5e4qrWBJgI5PPZ0TsAKakxLJZRy0r1aVuE4KIBANPAQuAdOAWEUnv0OxOoNIYMwZ4AnjMuW86sBTIALKBp0UkWESSgG8DWcaYTCDY2c7n1Dc1k19aTdZI/+w/aO+22SNpMYb7X9pFkMCiqf65TGZvZWcksqekukfTgivVn7hyhzALKDTGHDbGNAErgUUd2iwCnne+Xg3Mk9ZynEXASmNMozHmCFDoPB5ACDBAREKASOAYPmh3cRWOFuOTK6T1VFsJakVtE5eMjWdYtM/elHnE/IwEAN70wcdGx6rqKT/baHUYqp9zJSEkAcXt3pc4t3XaxhjjAKqBoV3ta4wpBX4HHAWOA9XGmDc7O7mI3CUiNhGxlZWVuRCue9nsFYjA9FT/v0MA+Pe5owD4SlaKxZF43+j4QYxPiPKZye5aBwYe47ZndjD3sXdY/PQ26pocVoel+jFXEkJnhfcdh3x21abT7SIymNa7h1HACGCgiNza2cmNMSuMMVnGmKz4+HgXwnWvnKJKxidEERMZGCN5L7pgKO9+/zKunhSYU1V0Z35mIjn2Ckv/Gi84UcPDrx5g9q/f5v6XdnG4rJbbZo+kuKKeJ976yLK4lHJl2a8SoP2fk8l88fFOW5sS5yOgGKDiPPteCRwxxpQBiMhaYA7wYi++g8c0txjyiioD7ll7Wpx/jbZ2p+yMRP7w9se8deAkt8xK9dp5zzY62LjnGCtzitldXEVosHBVeiI3z0xh7pg4goMER4vhma1HuG7KCCYnB8ZCRcq/uJIQcoCxIjIKKKW18/erHdpsAG4HtgM3Au8YY4yIbAD+T0Qep/VOYCywE2gBZotIJFAPzANsbvg+blVwooazjY6A6D9QrSYOj2Lk0Eg27T/h8YRgjCHvaBWrcop5de8x6pqaGTtsEP95zUQWT0ti6KDPT6O+fMEE/nngJD9as48N980lNMAHDCrf021CMMY4ROQ+YDOt1UDPGmPyReRhwGaM2QA8A/xNRAppvTNY6tw3X0RWAQcAB3CvMaYZ2CEiq4E85/ZdwAr3f72+sTkHpGX56Qhl9UUiQnZGIs9uO0J1/TliBrj/UWBFbRNr80p4OaeYj0+dJTIsmOsmj+ArM1OYnhrb5fQn0RGh/PL6TL75t1xWvHeYey8f4/bYlDof8acZILOysozN5r0biftf2oXNXsEHy6/wyzmMVOfyjlay5OkP+P3NU922SlxLi2FrYTkv5xTz5oETnGs2TE2JZenMFK6dMoJB4a7cjLf61ou5vH3oFJu+cwmj4we5JT7Vv4lIrjEmq7t2rv+U9kM2ewUzRg7WZBBgpibHkhAdzqb9J/qcEI5V1fMPWwmrbMWUVtUTGxnKbbPTuHlmCuMTo3p1zF8symBbYTkPrt3HS9+YTVCQ/vwp79CE0IXSqnqOVzdo/0EACgoS5mcksspWTH1TMwPCejYCvcnRwtsHT7Iyp5j3Pi7DGLhkbBzLF0zgqoyEPo9oHxYVwU+umciP1uzjZVuxVzu/Vf+mCaELbQviaP9BYMrOSOSF7UX866Mysl1cLa7wVA0v5xSzNq+U07VNJEZHcP/lY7gpK4WUIe5dj/orWSm8susYv379IFdMGEZCPxtEqKyhCaELOfYKBoWHMCGxf0z+1t/MGjWE2MhQNuefOG9CqGtysHHvcV7OKSa3qJKQIOHKiQncPCuFL42NJ9hDj3NEhN8smcT837/HT9fv58+3dfv4V6k+04TQBZu9kmmpsR77D6+sFRIcxJcnJrAp/wRNjhbCQj4r8TTGsKekmpdzinl1zzHONjoYHT+QBxdMYMn0ZOKjws9zZPdJixvIf3x5HI++cYhN+4+TnTncK+dV/ZcmhE5U15+j4GQNV0/S/4CBLDszkX/klvDBJ+VcNn4YVXVNrNtVyss5xRw6UUNEaBDXTBrB0lkpZFlUXPD1i0fx6p5jPLQ+n4suiPNImaxSbTQhdCLvaCXGaP9BoJs7Jo5B4SE8u83O2rzST+8WJifH8KvFmVw3ZQTREdb+Ag4JDuKxGyaz6Klt/Ob1gzx6w2RL41GBTRNCJ2z2CoKDhKkpOn1AIIsIDebyCcN4dc8xoiNCuGVmCl+ZmULGiBirQ/uczKQYvn7xKP783mEWTh3BnAsCew1sZR1NCJ2w2SvJHBFNZJhenkD3n9dM5LrJw/nSuHgiQn13AaTvXjmOTfkn+PHafWz67pd8Olblv3SylA6aHC3sLq4iS8cf9AsJ0RFclZHo879gB4QF85vFk7CfruN/3v7Y6nBUgNKE0MH+Y9U0OlqYqf0HysfMGRPHV7KSWfHeYfKPVVsdjgpAmhA6aBuQNmOk3iEo3/PjqycyODKMH63Zi6O5xepwVIDRhNCBzV5J2tBIr9WaK9UTsZFh/GJhBvtLz/DstiNWh6MCjCaEdowx2Ioqtf9A+bSrJyXy5fQEHn/rI4pO11odjgogmhDaOVxeS0VtE1kjtf9A+S4R4ZeLMgkNCuLH6/bhT1PYK9+mCaGdXHslgN4hKJ+XGBPBjxZMYFvhaf6RW2J1OCpAaEJoJ8deweDIUC6I779rDiv/8dVZqcxKG8KvXjvIqZoGq8NRAcClhCAi2SJSICKFIrK8k8/DReRl5+c7RCSt3WcPOrcXiMj8dttjRWS1iBwSkYMicpE7vlBftPUf6II4yh8EBQm/uWES9U3N/OLVA1aHowJAtwlBRIKBp4AFQDpwi4ikd2h2J1BpjBkDPAE85tw3ndb1lTOAbOBp5/EA/gfYZIyZAEwBDvb96/ReWU0jR8prtf9A+ZUL4gfx7XljeG3vcd46cNLqcJSfc+UOYRZQaIw5bIxpAlYCizq0WQQ873y9GpgnrX9mLwJWGmMajTFHgEJglohEA18CngEwxjQZY6r6/nV6L7dI+w+uSKmWAAAXCklEQVSUf7rrSxcwITGKh17Zz5mGc1aHo/yYKwkhCShu977Eua3TNsYYB1ANDD3PvqOBMuCvIrJLRP4iIpY+uLfZKwgPCSIzSRfEUf4lLCSIR2+YzMmaBv5r0yGrw1F+zJWE0NkD9Y51bl216Wp7CDAd+KMxZhpQC3yhbwJARO4SEZuI2MrKylwIt3dyiiqZkhzb5/VwlbLC1JRY7pgzihc/PEqOc7S9Uj3lSkIoAVLavU8GjnXVRkRCgBig4jz7lgAlxpgdzu2raU0QX2CMWWGMyTLGZMXHx7sQbs/VNzWTX1qt6x8ov/b9+eNIHjyA5Wv20nCu2epwlB9yJSHkAGNFZJSIhNHaSbyhQ5sNwO3O1zcC75jW0TIbgKXOKqRRwFhgpzHmBFAsIuOd+8wDLCuT2F1chaPFMFP7D5QfiwwL4VeLJ/FJWS1PbSm0Ohzlh7qd8N8Y4xCR+4DNQDDwrDEmX0QeBmzGmA20dg7/TUQKab0zWOrcN19EVtH6y94B3GuMafvT5X7g784kcxi4w83fzWU2ewUiMD1V7xCUf7t0XDxLpiXxx3c/4ZrJw5mQqH1iynXiT8Pes7KyjM1mc/txv/bsTk5WN7D5P77k9mMr5W0VtU1c+fi/SBkSydpvzSE4SMfV9HcikmuMyequXb8fqdzcYthVVKn9BypgDBkYxs+uS2dPcRXPf2C3OhzlR/p9Qig4UUNNo0P7D1RAWThlBJePj+e3mwsorqizOhzlJ/p9QrAVtZbo6R2CCiQiwiOLJyECP3llv86IqlzS7xNCjr2SxOgIkmIHWB2KUm6VFDuAH84fz3sflfHK7lKrw1F+oN8nhFx7BVlpg3VCOxWQbrsojempsTz86gFOn220Ohzl4/p1QiitqudYdYP2H6iAFRwkPHrDZM42Onh4o86Iqs6vXycEm3OI/wyd4VQFsHEJUdxz2RjW7z7GlkOnrA5H+bB+nRBy7BUMCg9hQmKU1aEo5VH3XH4BY4YN4ifr9nG20WF1OMpH9euEYLNXMi01lpDgfn0ZVD8QHhLMYzdM4viZBn63ucDqcJSP6re/Cavrz1Fwskb7D1S/MWPkEL42eyTPb7d/uv6HUu3124SQd7QSY9AV0lS/8oPsCSRGR7B8zV6aHC1Wh6N8TL9NCDZ7BcFBwtTUWKtDUcprBoWH8Mj1mXx86ix/fPcTq8NRPqYfJ4RKMkdEExnW7YSvSgWUeRMTuG7KCJ7c8jEfn6yxOhzlQ/plQmhytLC7uErXT1b91s+uS2dgeAjL1+6jpUWntVCt+mVC2H+smkZHi/YfqH4rblA4D12TTm5RJS/uKLI6HOUj+mVCyLW3VljM0AntVD+2ZHoSl4yN47E3DnGsqt7qcJQP6JcJIcdeQdrQSIZFRVgdilKWERF+vXgSLQYe0hlRFf0wIRhjsBVVMmOk9h8olTIkku9dNY63D51i497jVoejLOZSQhCRbBEpEJFCEVneyefhIvKy8/MdIpLW7rMHndsLRGR+h/2CRWSXiGzs6xdx1eHyWipqm5ipj4uUAuCOuaOYkhzDzzfkU1nbZHU4ykLdJgQRCQaeAhYA6cAtIpLeodmdQKUxZgzwBPCYc990YCmQAWQDTzuP1+Y7wMG+fomeaOs/0AojpVoFBwm/WTKZ6vpzPPKaV/87Kh/jyh3CLKDQGHPYGNMErAQWdWizCHje+Xo1ME9aFxhYBKw0xjQaY44Ahc7jISLJwDXAX/r+NVyXY69gcGQoF8QP9OZplfJp6SOi+ealo1mTV8I/D5y0OhyfcOpMA89sPULDuWarQ/EaVxJCElDc7n2Jc1unbYwxDqAaGNrNvr8Hfgicd/y8iNwlIjYRsZWVlbkQ7vm19R/ogjhKfd79V4xlQmIU33wxl7+8f7hfdzLvPFLBNf+7lV9uPMDjb31kdThe40pC6Ow3Z8eflK7adLpdRK4FThljcrs7uTFmhTEmyxiTFR8f332051FW08iR8lrtP1CqExGhwfzj7ou4cuIwHnntIPf9365+N1W2MYa/vH+YW/7fhwwKD+Gq9AT+8v5h9pVUWx2aV7iSEEqAlHbvk4FjXbURkRAgBqg4z75zgYUiYqf1EdQVIvJiL+LvkbYZHrX/QKnORUWE8qdbZ/Dgggm8sf84i57cSuGp/jG9RW2jg/te2sUjrx3kyonDWH/fXH570xTiBoXzwzV7Odcc+JMBupIQcoCxIjJKRMJo7STe0KHNBuB25+sbgXdM6/3mBmCpswppFDAW2GmMedAYk2yMSXMe7x1jzK1u+D7nZbNXEBYSRGZStKdPpZTfEhG+eekFvHjnhVTVnWPRk9t4fV9gl6QWnjrLoqe28ca+4yxfMIE/3TqD6IhQYgaE8svrMzl4/Az/7/3DVofpcd0mBGefwH3AZlorglYZY/JF5GERWehs9gwwVEQKgQeA5c5984FVwAFgE3CvMcayHpqcokqmJscSHhLcfWOl+rk5Y+LY+O2LGZcYxT1/z+NXrx3AEYB/Jb+xr/VOqLK2iRfvvJC7L73gc32M8zMSWZCZyO//+TFHymstjNTzxJ86jrKysozNZuvVvvVNzUz6+Wbu+tJofpg9wc2RKRW4mhwtPPLaAV7YXsSsUUN48qvTAmKUv6O5hf/aXMCK9w4zNSWWP946neExAzpte+pMA/Me/xfpw6N56RuzCQryr6IUEck1xmR1167fjFTeXVyFo8XoCmlK9VBYSBAPL8rkiZunsLekimv/sBWbvcLqsPqkrKaRf/vLDla8d5jbZo/k5W/O7jIZAAyLjuAnV09kx5EKVtmKu2zn7/pNQmj7AZ6eqhVGSvXG4mnJrLtnLpFhwSxd8SHPbj3il6WpuUUVXPu/77OnpIrHvzKFX16f6dJj5JtnpjB79BB+9fpBTp5p8EKk3tdvEkJOUSXjE6KIiQy1OhSl/NbE4dGsv+9iLhs/jIc3HuDbK3dT6yelqcYYntt2hJv//CERocGsu2cuS6Ynu7y/SOuI7iZHCz9bn+/BSK3TLxJCc4thV1ElWTr+QKk+ixkQyorbZvCD+eN5be8xFj+9jcNlZ60O67zqmhx8Z+Vufv7qAS4bH8+G+y5m4vCeVxuOihvId68cx6b8E2zaH3iVV/0iIRScqKGm0aH9B0q5SVCQcO/lY3jh3y+k/GwTC5/cxqb9J6wOq1OHy86y+KkP2Lj3GD+YP54Vt2URM6D3Twq+fsko0odH89P1+VTXn3NjpNbrFwnBVtTafzBDV0hTyq0uHhvHq/dfzAXxA7n7xVwefeOQT5Wmbtp/gkVPbuNUTQPP//ss7r18TJ8rhEKDg3jshsmUn23k0TcCazLAfpEQcuyVJEZHkDy46yoCpVTvJMUOYNXdF/HVC1P5078+4bZndlJ+ttHSmBzNLTz6xiHufjGX0fED2fjtS7hkbN+mvmlvUnIM37hkNC/tLGb7J6fddlyr9YuEkOfsP9AJ7ZTyjPCQYH69eBK/vXEyeUcrufYPW8k7WmlJLOVnG/naszv5078+4asXprLq7otIinX/H4PfvXIcqUMi+fG6fQEzI2q/SAivf+cSHrx6otVhKBXwbspKYe09cwgNEW7+83Ze2G73amlqWzLKLarktzdO5teLJ3lsZoIBYcH8ZskkjpTX8oe3P/bIObytXySEmAGhHvkLQSn1RRkjYth4X+sjmp+uz+eBVXuob/LsX9DGGP623c7Nf95OaIiw9p453JSV0u1+fTV3TBw3zUjmz+8d5sCxMx4/n6f1i4SglPKumMhQ/vK1LB748jhe2V3K4qe3YffQPED1Tc18b9UeHlqfz8Vj4th43yVkjIjxyLk685NrJjI4MowfrdnrUx3qvaEJQSnlEUFBwrfnjeW5O2Zx4kwD1z25lbfcvBqbvbyWxU9vY93uUh748jieuX2m1wefxkaG8YuFGewrreav2+xePbe7aUJQSnnUpePiefW+i0kbOpBvvGDjt5sP0dzS936Ffx44yXVPbuXEmQb+umwm35431rJJ566elMiVExP477cKOHq6zpIY3EETglLK41KGRPKPuy9i6cwUntryCbc/u5OK2qZeHau5xfC7zQV8/QUbI4dG8qpzKg0riQi/vD6DkKAgfrxun1/O8QSaEJRSXhIRGsyjN0zmsRsmsdNewbV/eJ/dxVU9OkZFbRPL/rqTJ7cUcnNWCqvvnkPKkEgPRdwzw2MG8KMFE9haWM7q3BKrw+kVTQhKKa+6eWYqa+6eg4jwlT9t5+87ilz6i3pPcRXX/e9Wdhyp4NElk3jsxslEhPrWYlf/NiuVmWmDeeS1g5TVWDs4rzc0ISilvG5Scgwb77+Yiy4Yyk/W7ecHq/d2ObjLGMP/7TjKTX/aDsCau+ewdFaqN8N1WVBQ64yo9U3N/OJV/5sR1aWEICLZIlIgIoUisryTz8NF5GXn5ztEJK3dZw86txeIyHznthQR2SIiB0UkX0S+464vpJTyD4MHhvHsspl8Z95YVueWsOTpD77QIdtwrpkfrN7Lj9ftY/YFQ9l4/8VMSvZeSWlvjBk2iPuvGMPGvcf5p5urqjyt24QgIsHAU8ACIB24RUTSOzS7E6g0xowBngAec+6bDiwFMoBs4Gnn8RzA94wxE4HZwL2dHFMpFeCCg4T/+PI4/rpsJiWVdVz7v++z5dApAI6ermPJ0x+wOreEb88by1+XzWTwwDCLI3bNNy+9gPEJUTy0fj81Df4zI6ordwizgEJjzGFjTBOwEljUoc0i4Hnn69XAPGmdOGgRsNIY02iMOQIUArOMMceNMXkAxpga4CCQ1Pevo5TyR5dPGMbG+y8heXAkdzyXw/I1e7nuya2UVNbx7LLWAW7BfrSOcVhIEI/dOJkTZxr4r00FVofjMlcSQhLQfhHREr74y/vTNsYYB1ANDHVlX+fjpWnADtfDVkoFmtShkay9Zw43TE9mZU4xI2IHsPH+S7hiQoLVofXK1JRY7pgzir99WOQ3a1C7khA6S8sdSwK6anPefUVkELAG+K4xptOJQETkLhGxiYitrKzMhXCVUv4qIjSY3900mVfuncu6e+aQOtQ3Skp763tXjSMpdgA/WrOXRofvz4jqSkIoAdrPEpUMHOuqjYiEADFAxfn2FZFQWpPB340xa7s6uTFmhTEmyxiTFR/vvvnMlVK+SUSYmhLrcyWlvTEwPIRfLc7kk7JantryidXhdMuVhJADjBWRUSISRmsn8YYObTYAtztf3wi8Y1oLizcAS51VSKOAscBOZ//CM8BBY8zj7vgiSinliy4bP4zF05L447uFFJyosTqc8+o2ITj7BO4DNtPa+bvKGJMvIg+LyEJns2eAoSJSCDwALHfumw+sAg4Am4B7jTHNwFzgNuAKEdnt/He1m7+bUkr5hIeuTScqIpQfrdnrlnmcPEX8ac6NrKwsY7PZrA5DKaV6bP3uUr6zcjc/uy6dO+aO8uq5RSTXGJPVXTsdqayUUl6wcMoILhsfz283F1BS6ZszompCUEopLxARHrk+E4D/fGW/T86IqglBKaW8JHlwJD+cP553C8pYv7tjsab1NCEopZQX3XZRGtNSY/nFq/mcPutbM6JqQlBKKS8KDhIeu2EyZxsdPPLaQavD+RxNCEop5WXjEqL41mVjWLerlHcLTlkdzqc0ISillAXuvfwCxgwbxE/W7ae20WF1OIAmBKWUskR4SDCP3TCJY9X1/O5N35gRVROCUkpZZMbIIdw2eyTPfWBn19FKq8PRhKCUUlb6wfzxJEZHsHzNPpocLZbGoglBKaUsFBURyiPXZ1JwsoY//cvaGVE1ISillMXmTUzguikjePKdQgpPWTcjqiYEpZTyAT+7Lp3I8GCWr9lHi0UzompCUEopHxA3KJz/vCYdW1Elf9951JIYNCEopZSPuGF6EpeMjeOxNw5xvLre6+fXhKCUUj5CRPj14kk0txgesmBGVE0ISinlQ1KGRPK9q8bxz4OneG3fca+e26WEICLZIlIgIoUisryTz8NF5GXn5ztEJK3dZw86txeIyHxXj6mUUv3VsjlpTE6O4ecb8qmqa/LaebtNCCISDDwFLADSgVtEJL1DszuBSmPMGOAJ4DHnvunAUiADyAaeFpFgF4+plFL9UkhwEI8umUxV3Tl+5cUZUV25Q5gFFBpjDhtjmoCVwKIObRYBzztfrwbmiYg4t680xjQaY44Ahc7juXJMpZTqt9JHRPPNS0fzj9wStn5c7pVzupIQkoDidu9LnNs6bWOMcQDVwNDz7OvKMZVSql+7/4qxjI4byIPr9lLf1Ozx87mSEKSTbR27vrtq09PtXzy5yF0iYhMRW1lZ2XkDVUqpQBIRGsyvl0xiclIsDec8nxBCXGhTAqS0e58MdFwMtK1NiYiEADFARTf7dndMAIwxK4AVAFlZWb63KrVSSnnQ7NFDmT16qFfO5codQg4wVkRGiUgYrZ3EGzq02QDc7nx9I/COaS2g3QAsdVYhjQLGAjtdPKZSSikv6vYOwRjjEJH7gM1AMPCsMSZfRB4GbMaYDcAzwN9EpJDWO4Olzn3zRWQVcABwAPcaY5oBOjum+7+eUkopV4m3R8L1RVZWlrHZbFaHoZRSfkVEco0xWd2105HKSimlAE0ISimlnDQhKKWUAjQhKKWUctKEoJRSCvCzKiMRKQOKerl7HOCdCUH8g16Pz+i1+Dy9Hp8JlGsx0hgT310jv0oIfSEiNlfKrvoLvR6f0WvxeXo9PtPfroU+MlJKKQVoQlBKKeXUnxLCCqsD8DF6PT6j1+Lz9Hp8pl9di37Th6CUUur8+tMdglJKqfMIuIQgItkiUiAihSKyvJPPw0XkZefnO0QkzftReocL1+IBETkgIntF5G0RGWlFnN7S3fVo1+5GETEiEtDVJa5cDxH5ivNnJF9E/s/bMXqLC/9XUkVki4jscv5/udqKOD3OGBMw/2idSvsTYDQQBuwB0ju0uQf4k/P1UuBlq+O28FpcDkQ6X38rUK+Fq9fD2S4KeA/4EMiyOm6Lfz7GAruAwc73w6yO28JrsQL4lvN1OmC3Om5P/Au0O4RZQKEx5rAxpglYCSzq0GYR8Lzz9Wpgnoh0tqSnv+v2Whhjthhj6pxvP6R15bpA5crPBsAvgf8CGrwZnAVcuR7fAJ4yxlQCGGNOeTlGb3HlWhgg2vk6hi5WePR3gZYQkoDidu9LnNs6bWOMcQDVgHfWp/MuV65Fe3cCb3g0Imt1ez1EZBqQYozZ6M3ALOLKz8c4YJyIbBORD0Uk22vReZcr1+LnwK0iUgK8DtzvndC8y5U1lf1JZ3/pdyyjcqVNIHD5e4rIrUAWcKlHI7LWea+HiAQBTwDLvBWQxVz5+Qih9bHRZbTePb4vIpnGmCoPx+ZtrlyLW4DnjDH/LSIX0bpCZKYxpsXz4XlPoN0hlAAp7d4n88Vbu0/biEgIrbd/FV6JzrtcuRaIyJXAT4CFxphGL8Vmhe6uRxSQCbwrInZgNrAhgDuWXf2/st4Yc84YcwQooDVBBBpXrsWdwCoAY8x2IILWeY4CSqAlhBxgrIiMEpEwWjuNN3RoswG43fn6RuAd4+wpCjDdXgvnI5I/05oMAvX5cJvzXg9jTLUxJs4Yk2aMSaO1T2WhMSZQ12x15f/KK7QWHiAicbQ+Qjrs1Si9w5VrcRSYByAiE2lNCGVejdILAiohOPsE7gM2AweBVcaYfBF5WEQWOps9AwwVkULgAaDL8kN/5uK1+C0wCPiHiOwWkY7/CQKGi9ej33DxemwGTovIAWAL8ANjzGlrIvYcF6/F94BviMge4CVgWSD+IakjlZVSSgEBdoeglFKq9zQhKKWUAjQhKKWUctKEoJRSCtCEoJRSykkTglJKKUATglJKKSdNCEoppQD4/zOl0H84DzuMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.arange(0,1,.1),plotinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./data/noise_size_10000.pickle', 'wb') as f:\n",
    "    pickle.dump(plotinfo, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
